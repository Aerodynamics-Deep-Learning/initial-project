{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c7b2b3b",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1367f38",
   "metadata": {},
   "source": [
    "MLP & Pairwise NIF, Size Analysis\n",
    "\n",
    "    - MLPs with sizes reflecting that of NeuralFoil:\n",
    "        - xxsmall\n",
    "        - xsmall\n",
    "        - small\n",
    "        - medium\n",
    "        - large\n",
    "        - xlarge\n",
    "        - xxlarge\n",
    "        - xxxlarge\n",
    "        - sub-enormous\n",
    "        - enormous\n",
    "\n",
    "    - Pairwise NIF models with the following sizes (paramnet+shapenet):\n",
    "        - xxsmall + xsmall\n",
    "        - xsmall + small\n",
    "        - small + medium\n",
    "        - medium +large\n",
    "        - large + xlarge\n",
    "        - xlarge + xxlarge\n",
    "        - xxlarge + xxxlarge\n",
    "        - xxxlarge + sub-enormous\n",
    "        - sub-enormous + enormous\n",
    "\n",
    "    - these sizes are:\n",
    "        - xxsmall = 2 layers, 32 neurons\n",
    "        - x small =  3 layers, 32 neurons\n",
    "        - small = 3 layers, 48 neurons \n",
    "        - medium = 4 layers, 64 neurons\n",
    "        - large = 4 layers, 128 neurons \n",
    "        - xlarge = 4 layers, 256 neurons\n",
    "        - xxlarge = 5 layers, 256 neurons\n",
    "        - xxxlarge = 5 layers, 512 neurons     \n",
    "        - sub-enormous = 5 layers, 1024 neurons\n",
    "        - enormous = 5 layers, 2048 neurons\n",
    "\n",
    "    - with the following hyperparams/datasets:\n",
    "\n",
    "        - AdamW optimizer:\n",
    "            - constant lr = 1e-4\n",
    "            - constant wd = 1e-2\n",
    "\n",
    "        - airfoil_dataset_8bern.csv\n",
    "            - 9 bernstein coefficients * 2 + trailing edge * 2\n",
    "            - set mach number = 0.1\n",
    "            - reynolds: [3m, 6m, 9m]\n",
    "            - aoa: [-4, -3, -2, -1, 0 , 1, ..., 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af14ec",
   "metadata": {},
   "source": [
    "# Imports, Important Definitions, Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b505bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Define autroreload so that it doesn't cause pain in the ass when we change the functions and run this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84f3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\SenkDosya\\Projects\\AeroML\n",
      "C:\\SenkDosya\\Projects\\AeroML\\initial-project\n"
     ]
    }
   ],
   "source": [
    "# Import all the models, training functions, manipulators here\n",
    "\n",
    "# Define the relative paths, append it to the system path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path.cwd().resolve().parents[2]\n",
    "github_root = Path.cwd().resolve().parents[1]\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(github_root))\n",
    "\n",
    "print(project_root)\n",
    "print(github_root)\n",
    "\n",
    "# Import shenanigans\n",
    "from defs.helper_functions.training_functions import *\n",
    "from defs.helper_functions.data_loaders import *\n",
    "from defs.models.MLP import *\n",
    "from defs.models.NIF import *\n",
    "\n",
    "# Time, to precisely: time\n",
    "import time\n",
    "\n",
    "# Garbage collector\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe9bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")  # Force CPU for testing purposes\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac3e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the entire exploration, define the dictionaries to store stuff\n",
    "\n",
    "exploration_dict = {\n",
    "    'cfg_models':  {\n",
    "        'cfg_mlps': {},\n",
    "        'cfg_pairnifs': {}\n",
    "    },\n",
    "    'perf_models': {\n",
    "        'r2s': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'maes': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'preds': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'flops': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'params': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14781451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model sizes, append them to the exploration dictionary\n",
    "\n",
    "model_size_list = [\n",
    "    [32,32],\n",
    "    [32,32,32],\n",
    "    [48,48,48],\n",
    "    [64,64,64,64],\n",
    "    [128,128,128,128],\n",
    "    [256,256,256,256],\n",
    "    [256,256,256,256,256],\n",
    "    [512,512,512,512,512],\n",
    "    [1024,1024,1024,1024,1024],\n",
    "    [2048,2048,2048,2048,2048]\n",
    "]\n",
    "\n",
    "model_name_list = [\n",
    "    'xxsmall',\n",
    "    'xsmall',\n",
    "    'small',\n",
    "    'medium',\n",
    "    'large',\n",
    "    'xlarge',\n",
    "    'xxlarge',\n",
    "    'xxxlarge',\n",
    "    'sub_enormous',\n",
    "    'enormous'\n",
    "]\n",
    "\n",
    "for i in range(len(model_name_list)):\n",
    "    cfg_name = model_name_list[i]\n",
    "    exploration_dict['cfg_models']['cfg_mlps'][cfg_name] = {\n",
    "            'input_dim': 23,\n",
    "            'output_dim': 3,\n",
    "            'hidden_units': model_size_list[i],\n",
    "            'activation': nn.GELU\n",
    "        }\n",
    "\n",
    "for i in range(len(model_name_list)-1):\n",
    "    cfg_name = f'{model_name_list[i]}_{model_name_list[i+1]}'\n",
    "    exploration_dict['cfg_models']['cfg_pairnifs'][cfg_name] = {\n",
    "        'cfg_shape_net': {\n",
    "            'input_dim': 20,\n",
    "            'output_dim': 3,\n",
    "            'hidden_units': model_size_list[i+1],\n",
    "            'shape_activation': nn.GELU\n",
    "        },\n",
    "        'cfg_param_net': {\n",
    "            'input_dim': 3,\n",
    "            'hidden_units': model_size_list[i],\n",
    "            'param_activation': nn.GELU\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ecdf1",
   "metadata": {},
   "source": [
    "# Data Pre-Processing, Imports for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a19d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         airfoil_name      Bu_0      Bu_1      Bu_2      Bu_3      Bu_4  \\\n",
      "0       airfoil_0001  0.273636 -0.200785  1.229138 -1.445725  1.790261   \n",
      "1       airfoil_0001  0.273636 -0.200785  1.229138 -1.445725  1.790261   \n",
      "2       airfoil_0001  0.273636 -0.200785  1.229138 -1.445725  1.790261   \n",
      "3       airfoil_0001  0.273636 -0.200785  1.229138 -1.445725  1.790261   \n",
      "4       airfoil_0001  0.273636 -0.200785  1.229138 -1.445725  1.790261   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "137245  airfoil_1830  0.327410  1.499799 -0.780311  3.149627 -1.415475   \n",
      "137246  airfoil_1830  0.327410  1.499799 -0.780311  3.149627 -1.415475   \n",
      "137247  airfoil_1830  0.327410  1.499799 -0.780311  3.149627 -1.415475   \n",
      "137248  airfoil_1830  0.327410  1.499799 -0.780311  3.149627 -1.415475   \n",
      "137249  airfoil_1830  0.327410  1.499799 -0.780311  3.149627 -1.415475   \n",
      "\n",
      "            Bu_5      Bu_6      Bu_7     Bu_8  ...      Bl_7      Bl_8  \\\n",
      "0      -0.843033  0.539314  0.097329  0.15940  ...  0.077214  0.073847   \n",
      "1      -0.843033  0.539314  0.097329  0.15940  ...  0.077214  0.073847   \n",
      "2      -0.843033  0.539314  0.097329  0.15940  ...  0.077214  0.073847   \n",
      "3      -0.843033  0.539314  0.097329  0.15940  ...  0.077214  0.073847   \n",
      "4      -0.843033  0.539314  0.097329  0.15940  ...  0.077214  0.073847   \n",
      "...          ...       ...       ...      ...  ...       ...       ...   \n",
      "137245  2.534748  0.138996  1.169421  0.52533  ...  0.389470 -0.257985   \n",
      "137246  2.534748  0.138996  1.169421  0.52533  ...  0.389470 -0.257985   \n",
      "137247  2.534748  0.138996  1.169421  0.52533  ...  0.389470 -0.257985   \n",
      "137248  2.534748  0.138996  1.169421  0.52533  ...  0.389470 -0.257985   \n",
      "137249  2.534748  0.138996  1.169421  0.52533  ...  0.389470 -0.257985   \n",
      "\n",
      "        te_y_upper  te_y_lower  Reynolds  Mach  AoA        Cd        Cl  \\\n",
      "0         0.001419   -0.001419       3.0   0.1   -4  0.006682 -0.050106   \n",
      "1         0.001419   -0.001419       3.0   0.1   -3  0.006512  0.065783   \n",
      "2         0.001419   -0.001419       3.0   0.1   -2  0.006433  0.181631   \n",
      "3         0.001419   -0.001419       3.0   0.1   -1  0.006404  0.297443   \n",
      "4         0.001419   -0.001419       3.0   0.1    0  0.006480  0.413217   \n",
      "...            ...         ...       ...   ...  ...       ...       ...   \n",
      "137245    0.000454   -0.000454       9.0   0.1   16  0.119998  1.415771   \n",
      "137246    0.000454   -0.000454       9.0   0.1   17  0.127955  1.450842   \n",
      "137247    0.000454   -0.000454       9.0   0.1   18  0.136104  1.485818   \n",
      "137248    0.000454   -0.000454       9.0   0.1   19  0.144152  1.521043   \n",
      "137249    0.000454   -0.000454       9.0   0.1   20  0.152689  1.551263   \n",
      "\n",
      "              Cm  \n",
      "0      -0.088654  \n",
      "1      -0.088899  \n",
      "2      -0.089121  \n",
      "3      -0.089344  \n",
      "4      -0.089582  \n",
      "...          ...  \n",
      "137245 -0.138972  \n",
      "137246 -0.140147  \n",
      "137247 -0.141682  \n",
      "137248 -0.143752  \n",
      "137249 -0.145036  \n",
      "\n",
      "[137250 rows x 27 columns]>\n",
      "(137250, 27)\n",
      "torch.Size([137250, 20])\n",
      "torch.Size([137250, 3])\n",
      "torch.Size([137250, 3])\n",
      "137250\n"
     ]
    }
   ],
   "source": [
    "# Figure out the data\n",
    "df = pd.read_csv(rf\"{str(project_root)}\\airfoil_data\\airfoil_dataset_8bern.csv\")\n",
    "\n",
    "df = df.drop(['N1', 'N2'], axis=1) # Remove N1 and N2 since all the airfoils are subsonic\n",
    "\n",
    "df['Reynolds'] = df['Reynolds'] / 1000000 # Normalize the Reynolds feature, too dominant\n",
    "\n",
    "print(df.head)\n",
    "\n",
    "geom, cond, perf, names= get_dataset(df, loc_geometry=[1,20], loc_cond=[21,23], loc_perf_coeffs=[24,26], loc_names=0) # Get the necessary stuff for the dataset\n",
    "print(df.shape); print(geom.shape); print(cond.shape); print(perf.shape); print(len(names))\n",
    "\n",
    "ds = AirfoilDataset(geom, cond, perf, names)\n",
    "\n",
    "del df, geom, cond, perf, names # Delete these to preserve memory\n",
    "\n",
    "cfg_loader = {\n",
    "    'n_epoch': 100,\n",
    "    'n_train': 1000,\n",
    "    'n_test': 17250,\n",
    "    'train_batch': 1\n",
    "}\n",
    "\n",
    "dl_train, dl_val, dl_test = get_dataloaders(ds=ds, cfg_loader=cfg_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10489854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configs of train, get dataloaders\n",
    "cfg_train = {\n",
    "    'cfg_loader': cfg_loader,\n",
    "    'dtype': torch.float32,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "dataloaders = [dl_train, dl_val, dl_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9d3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for analysis\n",
    "\n",
    "# Metrics import\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "from defs.helper_functions.auxiliary import get_n_params\n",
    "\n",
    "# Import plots and set up indices\n",
    "from defs.helper_functions.plot import all_plots_ex_analysis\n",
    "index= np.arange(cfg_train['cfg_loader']['n_epoch'] * cfg_train['cfg_loader']['n_train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e829f30",
   "metadata": {},
   "source": [
    "# MLP Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e163cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: xxsmall | Started loop\n",
      "Model: xxsmall | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 224.30980947127682\n",
      "Epoch 1 | Val loss: 0.06863674521446228\n",
      "Epoch 2 | Total train loss: 56.61697225809621\n",
      "Epoch 2 | Val loss: 0.061338700354099274\n",
      "Epoch 3 | Total train loss: 51.131289272918366\n",
      "Epoch 3 | Val loss: 0.048057183623313904\n",
      "Epoch 4 | Total train loss: 44.65498405406106\n",
      "Epoch 4 | Val loss: 0.04417048394680023\n",
      "Epoch 5 | Total train loss: 40.90106576779726\n",
      "Epoch 5 | Val loss: 0.03214333578944206\n",
      "Epoch 6 | Total train loss: 39.71340433509249\n",
      "Epoch 6 | Val loss: 0.037278931587934494\n",
      "Epoch 7 | Total train loss: 35.507425093503116\n",
      "Epoch 7 | Val loss: 0.031549446284770966\n",
      "Epoch 8 | Total train loss: 29.49805541930982\n",
      "Epoch 8 | Val loss: 0.036712922155857086\n",
      "Epoch 9 | Total train loss: 27.06246025377004\n",
      "Epoch 9 | Val loss: 0.02603023685514927\n",
      "Epoch 10 | Total train loss: 25.534248096603278\n",
      "Epoch 10 | Val loss: 0.02814541570842266\n",
      "Epoch 11 | Total train loss: 23.86817579351009\n",
      "Epoch 11 | Val loss: 0.02073231153190136\n",
      "Epoch 12 | Total train loss: 22.130447889521747\n",
      "Epoch 12 | Val loss: 0.017200635746121407\n",
      "Epoch 13 | Total train loss: 25.460512088641735\n",
      "Epoch 13 | Val loss: 0.022218137979507446\n",
      "Epoch 14 | Total train loss: 21.179672176347594\n",
      "Epoch 14 | Val loss: 0.025090361014008522\n",
      "Epoch 15 | Total train loss: 20.671176715057186\n",
      "Epoch 15 | Val loss: 0.018536193296313286\n",
      "Epoch 16 | Total train loss: 20.538889313127584\n",
      "Epoch 16 | Val loss: 0.023423783481121063\n",
      "Epoch 17 | Total train loss: 18.294336027011923\n",
      "Epoch 17 | Val loss: 0.02089039795100689\n",
      "Epoch 18 | Total train loss: 20.847443565864864\n",
      "Epoch 18 | Val loss: 0.020416514948010445\n",
      "Epoch 19 | Total train loss: 20.715761576862405\n",
      "Epoch 19 | Val loss: 0.025973161682486534\n",
      "Epoch 20 | Total train loss: 18.95989161598618\n",
      "Epoch 20 | Val loss: 0.01874292828142643\n",
      "Epoch 21 | Total train loss: 17.302088860272306\n",
      "Epoch 21 | Val loss: 0.015618890523910522\n",
      "Epoch 22 | Total train loss: 16.495896222833096\n",
      "Epoch 22 | Val loss: 0.010413974523544312\n",
      "Epoch 23 | Total train loss: 15.801132334590875\n",
      "Epoch 23 | Val loss: 0.0148399006575346\n",
      "Epoch 24 | Total train loss: 16.047409428304945\n",
      "Epoch 24 | Val loss: 0.017925575375556946\n",
      "Epoch 25 | Total train loss: 18.99738230079265\n",
      "Epoch 25 | Val loss: 0.017623404040932655\n",
      "Epoch 26 | Total train loss: 16.424710582496573\n",
      "Epoch 26 | Val loss: 0.012982601299881935\n",
      "Epoch 27 | Total train loss: 16.180949542889607\n",
      "Epoch 27 | Val loss: 0.014032065868377686\n",
      "Epoch 28 | Total train loss: 16.087694668887934\n",
      "Epoch 28 | Val loss: 0.012896202504634857\n",
      "Epoch 29 | Total train loss: 16.943190227799278\n",
      "Epoch 29 | Val loss: 0.02196543663740158\n",
      "Epoch 30 | Total train loss: 16.268702380618834\n",
      "Epoch 30 | Val loss: 0.01842498779296875\n",
      "Epoch 31 | Total train loss: 16.431027559066024\n",
      "Epoch 31 | Val loss: 0.017793582752346992\n",
      "Epoch 32 | Total train loss: 15.243294434922063\n",
      "Epoch 32 | Val loss: 0.012231388129293919\n",
      "Epoch 33 | Total train loss: 15.636448659140342\n",
      "Epoch 33 | Val loss: 0.011370398104190826\n",
      "Epoch 34 | Total train loss: 13.802713545470624\n",
      "Epoch 34 | Val loss: 0.009651419706642628\n",
      "Epoch 35 | Total train loss: 14.013316213190137\n",
      "Epoch 35 | Val loss: 0.016757160425186157\n",
      "Epoch 36 | Total train loss: 14.256693436937667\n",
      "Epoch 36 | Val loss: 0.012357707135379314\n",
      "Epoch 37 | Total train loss: 12.082457563898515\n",
      "Epoch 37 | Val loss: 0.01105393748730421\n",
      "Epoch 38 | Total train loss: 10.861630917213915\n",
      "Epoch 38 | Val loss: 0.014772653579711914\n",
      "Epoch 39 | Total train loss: 12.571216858398657\n",
      "Epoch 39 | Val loss: 0.012671628035604954\n",
      "Epoch 40 | Total train loss: 11.488817287436177\n",
      "Epoch 40 | Val loss: 0.013022465631365776\n",
      "Epoch 41 | Total train loss: 10.358395320377895\n",
      "Epoch 41 | Val loss: 0.01402285322546959\n",
      "Epoch 42 | Total train loss: 10.582771929486626\n",
      "Epoch 42 | Val loss: 0.009953583590686321\n",
      "Epoch 43 | Total train loss: 10.618124745865316\n",
      "Epoch 43 | Val loss: 0.012891877442598343\n",
      "Epoch 44 | Total train loss: 10.749182251015554\n",
      "Epoch 44 | Val loss: 0.008460013195872307\n",
      "Epoch 45 | Total train loss: 11.08020935744662\n",
      "Epoch 45 | Val loss: 0.010412944480776787\n",
      "Epoch 46 | Total train loss: 10.489552827324133\n",
      "Epoch 46 | Val loss: 0.009131357073783875\n",
      "Epoch 47 | Total train loss: 9.399017758303216\n",
      "Epoch 47 | Val loss: 0.01315102819353342\n",
      "Epoch 48 | Total train loss: 9.600195736272326\n",
      "Epoch 48 | Val loss: 0.009594849310815334\n",
      "Epoch 49 | Total train loss: 8.817468733745955\n",
      "Epoch 49 | Val loss: 0.011094516143202782\n",
      "Epoch 50 | Total train loss: 11.150052764470843\n",
      "Epoch 50 | Val loss: 0.008593671023845673\n",
      "Epoch 51 | Total train loss: 9.928584336303402\n",
      "Epoch 51 | Val loss: 0.011048484593629837\n",
      "Epoch 52 | Total train loss: 9.069289472333367\n",
      "Epoch 52 | Val loss: 0.008456694893538952\n",
      "Epoch 53 | Total train loss: 8.296174391807654\n",
      "Epoch 53 | Val loss: 0.010312827304005623\n",
      "Epoch 54 | Total train loss: 9.622993749008856\n",
      "Epoch 54 | Val loss: 0.0064795417711138725\n",
      "Epoch 55 | Total train loss: 10.257712836458722\n",
      "Epoch 55 | Val loss: 0.010562513954937458\n",
      "Epoch 56 | Total train loss: 9.295503042181963\n",
      "Epoch 56 | Val loss: 0.008057430386543274\n",
      "Epoch 57 | Total train loss: 10.592030692233948\n",
      "Epoch 57 | Val loss: 0.007760722190141678\n",
      "Epoch 58 | Total train loss: 8.695854209029676\n",
      "Epoch 58 | Val loss: 0.005724115297198296\n",
      "Epoch 59 | Total train loss: 8.032979655435383\n",
      "Epoch 59 | Val loss: 0.0067323120310902596\n",
      "Epoch 60 | Total train loss: 7.798055669384212\n",
      "Epoch 60 | Val loss: 0.012824245728552341\n",
      "Epoch 61 | Total train loss: 9.479016403211972\n",
      "Epoch 61 | Val loss: 0.009713227860629559\n",
      "Epoch 62 | Total train loss: 10.911567716666013\n",
      "Epoch 62 | Val loss: 0.00951414741575718\n",
      "Epoch 63 | Total train loss: 8.880013797598622\n",
      "Epoch 63 | Val loss: 0.006350059062242508\n",
      "Epoch 64 | Total train loss: 7.697472177319014\n",
      "Epoch 64 | Val loss: 0.012143522500991821\n",
      "Epoch 65 | Total train loss: 7.455514820064764\n",
      "Epoch 65 | Val loss: 0.008786091580986977\n",
      "Epoch 66 | Total train loss: 8.786786612296282\n",
      "Epoch 66 | Val loss: 0.008066571317613125\n",
      "Epoch 67 | Total train loss: 9.958688254562844\n",
      "Epoch 67 | Val loss: 0.00851327646523714\n",
      "Epoch 68 | Total train loss: 7.9278981475413275\n",
      "Epoch 68 | Val loss: 0.008546408265829086\n",
      "Epoch 69 | Total train loss: 8.171184394593183\n",
      "Epoch 69 | Val loss: 0.008707531727850437\n",
      "Epoch 70 | Total train loss: 8.731073579028589\n",
      "Epoch 70 | Val loss: 0.007767133880406618\n",
      "Epoch 71 | Total train loss: 7.467352621115424\n",
      "Epoch 71 | Val loss: 0.007630138657987118\n",
      "Epoch 72 | Total train loss: 9.241562914076837\n",
      "Epoch 72 | Val loss: 0.005785829853266478\n",
      "Epoch 73 | Total train loss: 9.699897295026176\n",
      "Epoch 73 | Val loss: 0.007539849262684584\n",
      "Epoch 74 | Total train loss: 8.721914193523162\n",
      "Epoch 74 | Val loss: 0.00955885834991932\n",
      "Epoch 75 | Total train loss: 8.73879258307511\n",
      "Epoch 75 | Val loss: 0.00984267145395279\n",
      "Epoch 76 | Total train loss: 8.566271914493882\n",
      "Epoch 76 | Val loss: 0.00967896357178688\n",
      "Epoch 77 | Total train loss: 10.678932771697873\n",
      "Epoch 77 | Val loss: 0.00804938655346632\n",
      "Epoch 78 | Total train loss: 8.782432856047649\n",
      "Epoch 78 | Val loss: 0.008040093816816807\n",
      "Epoch 79 | Total train loss: 7.4744123344116815\n",
      "Epoch 79 | Val loss: 0.009547429159283638\n",
      "Epoch 80 | Total train loss: 8.041805939926007\n",
      "Epoch 80 | Val loss: 0.010685782879590988\n",
      "Epoch 81 | Total train loss: 7.830173557741659\n",
      "Epoch 81 | Val loss: 0.006185309030115604\n",
      "Epoch 82 | Total train loss: 8.752102474358253\n",
      "Epoch 82 | Val loss: 0.008079647086560726\n",
      "Epoch 83 | Total train loss: 8.118552007069184\n",
      "Epoch 83 | Val loss: 0.008937615901231766\n",
      "Epoch 84 | Total train loss: 7.909163914626788\n",
      "Epoch 84 | Val loss: 0.01008203998208046\n",
      "Epoch 85 | Total train loss: 7.730772687692934\n",
      "Epoch 85 | Val loss: 0.008715692907571793\n",
      "Epoch 86 | Total train loss: 6.741933583788978\n",
      "Epoch 86 | Val loss: 0.008216438814997673\n",
      "Epoch 87 | Total train loss: 7.602341293742484\n",
      "Epoch 87 | Val loss: 0.005797937978059053\n",
      "Epoch 88 | Total train loss: 9.53234481064942\n",
      "Epoch 88 | Val loss: 0.005898235831409693\n",
      "Epoch 89 | Total train loss: 8.684286474664077\n",
      "Epoch 89 | Val loss: 0.009441190399229527\n",
      "Epoch 90 | Total train loss: 6.590010065326169\n",
      "Epoch 90 | Val loss: 0.007542141247540712\n",
      "Epoch 91 | Total train loss: 7.632975411744837\n",
      "Epoch 91 | Val loss: 0.006711157504469156\n",
      "Epoch 92 | Total train loss: 7.090946741725247\n",
      "Epoch 92 | Val loss: 0.006568791810423136\n",
      "Epoch 93 | Total train loss: 7.792676902572111\n",
      "Epoch 93 | Val loss: 0.00702100433409214\n",
      "Epoch 94 | Total train loss: 6.53954284777933\n",
      "Epoch 94 | Val loss: 0.007498073857277632\n",
      "Epoch 95 | Total train loss: 7.461110765676267\n",
      "Epoch 95 | Val loss: 0.006240015849471092\n",
      "Epoch 96 | Total train loss: 7.325103689095158\n",
      "Epoch 96 | Val loss: 0.007320032455027103\n",
      "Epoch 97 | Total train loss: 7.335057447865893\n",
      "Epoch 97 | Val loss: 0.004881992936134338\n",
      "Epoch 98 | Total train loss: 7.348859004871656\n",
      "Epoch 98 | Val loss: 0.010007288306951523\n",
      "Epoch 99 | Total train loss: 7.787641611076197\n",
      "Epoch 99 | Val loss: 0.008189167827367783\n",
      "Epoch 100 | Total train loss: 7.430031447763213\n",
      "Epoch 100 | Val loss: 0.006131212692707777\n",
      "Test loss: 0.0076082246378064156\n",
      "Model: xxsmall | Training complete\n",
      "Model: xxsmall | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxsmall | Plots made and saved\n",
      "Model: xxsmall | Predictions saved\n",
      "Model: xxsmall | Model saved\n",
      "Model: xxsmall | Garbage collected, states deleted\n",
      "Model: xsmall | Started loop\n",
      "Model: xsmall | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 168.48155972150562\n",
      "Epoch 1 | Val loss: 0.06670723110437393\n",
      "Epoch 2 | Total train loss: 53.85728008965816\n",
      "Epoch 2 | Val loss: 0.04767365753650665\n",
      "Epoch 3 | Total train loss: 45.78882264764434\n",
      "Epoch 3 | Val loss: 0.03674500808119774\n",
      "Epoch 4 | Total train loss: 41.10934217465501\n",
      "Epoch 4 | Val loss: 0.04399010166525841\n",
      "Epoch 5 | Total train loss: 41.12919133189462\n",
      "Epoch 5 | Val loss: 0.04105314239859581\n",
      "Epoch 6 | Total train loss: 35.067930995550114\n",
      "Epoch 6 | Val loss: 0.03018980100750923\n",
      "Epoch 7 | Total train loss: 28.558150768056294\n",
      "Epoch 7 | Val loss: 0.031119467690587044\n",
      "Epoch 8 | Total train loss: 26.737168129142447\n",
      "Epoch 8 | Val loss: 0.02973305806517601\n",
      "Epoch 9 | Total train loss: 24.09476173251278\n",
      "Epoch 9 | Val loss: 0.02731696516275406\n",
      "Epoch 10 | Total train loss: 23.38082338050117\n",
      "Epoch 10 | Val loss: 0.023951657116413116\n",
      "Epoch 11 | Total train loss: 23.235023606429422\n",
      "Epoch 11 | Val loss: 0.022151509299874306\n",
      "Epoch 12 | Total train loss: 20.998596963956516\n",
      "Epoch 12 | Val loss: 0.017781123518943787\n",
      "Epoch 13 | Total train loss: 21.146927009249453\n",
      "Epoch 13 | Val loss: 0.026471208781003952\n",
      "Epoch 14 | Total train loss: 18.818845578944092\n",
      "Epoch 14 | Val loss: 0.013497426174581051\n",
      "Epoch 15 | Total train loss: 18.346631593180064\n",
      "Epoch 15 | Val loss: 0.024499250575900078\n",
      "Epoch 16 | Total train loss: 20.891007929653824\n",
      "Epoch 16 | Val loss: 0.018798327073454857\n",
      "Epoch 17 | Total train loss: 16.877471867403074\n",
      "Epoch 17 | Val loss: 0.016811514273285866\n",
      "Epoch 18 | Total train loss: 20.060242076401664\n",
      "Epoch 18 | Val loss: 0.01599665731191635\n",
      "Epoch 19 | Total train loss: 20.001104166190203\n",
      "Epoch 19 | Val loss: 0.019596366211771965\n",
      "Epoch 20 | Total train loss: 19.246626445321\n",
      "Epoch 20 | Val loss: 0.015780789777636528\n",
      "Epoch 21 | Total train loss: 22.81138307115043\n",
      "Epoch 21 | Val loss: 0.02261577732861042\n",
      "Epoch 22 | Total train loss: 15.887089182860564\n",
      "Epoch 22 | Val loss: 0.016585586592555046\n",
      "Epoch 23 | Total train loss: 16.15584854512963\n",
      "Epoch 23 | Val loss: 0.019747566431760788\n",
      "Epoch 24 | Total train loss: 14.58342702639402\n",
      "Epoch 24 | Val loss: 0.013812960125505924\n",
      "Epoch 25 | Total train loss: 12.588238958546754\n",
      "Epoch 25 | Val loss: 0.013438435271382332\n",
      "Epoch 26 | Total train loss: 17.25538450549766\n",
      "Epoch 26 | Val loss: 0.01634136401116848\n",
      "Epoch 27 | Total train loss: 16.453720620131662\n",
      "Epoch 27 | Val loss: 0.020966343581676483\n",
      "Epoch 28 | Total train loss: 16.377235281257526\n",
      "Epoch 28 | Val loss: 0.011636272072792053\n",
      "Epoch 29 | Total train loss: 15.635182890797296\n",
      "Epoch 29 | Val loss: 0.012848989106714725\n",
      "Epoch 30 | Total train loss: 12.403626732300438\n",
      "Epoch 30 | Val loss: 0.012859001755714417\n",
      "Epoch 31 | Total train loss: 13.952884649505904\n",
      "Epoch 31 | Val loss: 0.01722181960940361\n",
      "Epoch 32 | Total train loss: 12.715194274526539\n",
      "Epoch 32 | Val loss: 0.011929221451282501\n",
      "Epoch 33 | Total train loss: 12.48479710740321\n",
      "Epoch 33 | Val loss: 0.010819229297339916\n",
      "Epoch 34 | Total train loss: 12.861882986828164\n",
      "Epoch 34 | Val loss: 0.013909310102462769\n",
      "Epoch 35 | Total train loss: 11.250785509224443\n",
      "Epoch 35 | Val loss: 0.0092618428170681\n",
      "Epoch 36 | Total train loss: 12.651318263517851\n",
      "Epoch 36 | Val loss: 0.008218382485210896\n",
      "Epoch 37 | Total train loss: 10.456618751916722\n",
      "Epoch 37 | Val loss: 0.01521003246307373\n",
      "Epoch 38 | Total train loss: 8.795235310797693\n",
      "Epoch 38 | Val loss: 0.010202223435044289\n",
      "Epoch 39 | Total train loss: 11.48371435236777\n",
      "Epoch 39 | Val loss: 0.009609140455722809\n",
      "Epoch 40 | Total train loss: 11.403159004656118\n",
      "Epoch 40 | Val loss: 0.009614512324333191\n",
      "Epoch 41 | Total train loss: 11.131703772144121\n",
      "Epoch 41 | Val loss: 0.014052585698664188\n",
      "Epoch 42 | Total train loss: 9.691141220790996\n",
      "Epoch 42 | Val loss: 0.009409607388079166\n",
      "Epoch 43 | Total train loss: 10.867028230115466\n",
      "Epoch 43 | Val loss: 0.010572570376098156\n",
      "Epoch 44 | Total train loss: 9.781829163787222\n",
      "Epoch 44 | Val loss: 0.011899017728865147\n",
      "Epoch 45 | Total train loss: 9.843477801709923\n",
      "Epoch 45 | Val loss: 0.009874029085040092\n",
      "Epoch 46 | Total train loss: 10.387837180660654\n",
      "Epoch 46 | Val loss: 0.007909613661468029\n",
      "Epoch 47 | Total train loss: 9.81813952966877\n",
      "Epoch 47 | Val loss: 0.010403139516711235\n",
      "Epoch 48 | Total train loss: 9.302886134860728\n",
      "Epoch 48 | Val loss: 0.005173714365810156\n",
      "Epoch 49 | Total train loss: 11.090163168189065\n",
      "Epoch 49 | Val loss: 0.015021469444036484\n",
      "Epoch 50 | Total train loss: 9.564008772743136\n",
      "Epoch 50 | Val loss: 0.012010617181658745\n",
      "Epoch 51 | Total train loss: 8.50529864166515\n",
      "Epoch 51 | Val loss: 0.007903393357992172\n",
      "Epoch 52 | Total train loss: 12.453072592126091\n",
      "Epoch 52 | Val loss: 0.009123943746089935\n",
      "Epoch 53 | Total train loss: 9.168102025092821\n",
      "Epoch 53 | Val loss: 0.009563314728438854\n",
      "Epoch 54 | Total train loss: 8.953666934868124\n",
      "Epoch 54 | Val loss: 0.00918815191835165\n",
      "Epoch 55 | Total train loss: 8.851230824984896\n",
      "Epoch 55 | Val loss: 0.008101928047835827\n",
      "Epoch 56 | Total train loss: 9.012379885547489\n",
      "Epoch 56 | Val loss: 0.008950206451117992\n",
      "Epoch 57 | Total train loss: 8.805754457308922\n",
      "Epoch 57 | Val loss: 0.00867155846208334\n",
      "Epoch 58 | Total train loss: 9.857809078941386\n",
      "Epoch 58 | Val loss: 0.009305797517299652\n",
      "Epoch 59 | Total train loss: 8.745747097277672\n",
      "Epoch 59 | Val loss: 0.006705904379487038\n",
      "Epoch 60 | Total train loss: 8.967286218465233\n",
      "Epoch 60 | Val loss: 0.00940990261733532\n",
      "Epoch 61 | Total train loss: 7.9985652837799535\n",
      "Epoch 61 | Val loss: 0.007208675611764193\n",
      "Epoch 62 | Total train loss: 9.755457109376039\n",
      "Epoch 62 | Val loss: 0.008016041480004787\n",
      "Epoch 63 | Total train loss: 8.386000375653452\n",
      "Epoch 63 | Val loss: 0.008855813182890415\n",
      "Epoch 64 | Total train loss: 8.201222172760481\n",
      "Epoch 64 | Val loss: 0.009532185271382332\n",
      "Epoch 65 | Total train loss: 8.351766638030256\n",
      "Epoch 65 | Val loss: 0.01050054095685482\n",
      "Epoch 66 | Total train loss: 8.676942910055686\n",
      "Epoch 66 | Val loss: 0.00810663215816021\n",
      "Epoch 67 | Total train loss: 8.063482233081231\n",
      "Epoch 67 | Val loss: 0.008422551676630974\n",
      "Epoch 68 | Total train loss: 8.669769980358979\n",
      "Epoch 68 | Val loss: 0.011057347990572453\n",
      "Epoch 69 | Total train loss: 8.241568701065262\n",
      "Epoch 69 | Val loss: 0.006707454100251198\n",
      "Epoch 70 | Total train loss: 8.806847307296266\n",
      "Epoch 70 | Val loss: 0.010976300574839115\n",
      "Epoch 71 | Total train loss: 8.069269810482183\n",
      "Epoch 71 | Val loss: 0.0066305045038461685\n",
      "Epoch 72 | Total train loss: 7.295300967856463\n",
      "Epoch 72 | Val loss: 0.008254984393715858\n",
      "Epoch 73 | Total train loss: 8.665812780826286\n",
      "Epoch 73 | Val loss: 0.0086961155757308\n",
      "Epoch 74 | Total train loss: 7.87411201482746\n",
      "Epoch 74 | Val loss: 0.006285078823566437\n",
      "Epoch 75 | Total train loss: 6.179367011099885\n",
      "Epoch 75 | Val loss: 0.008262241259217262\n",
      "Epoch 76 | Total train loss: 7.987164669421304\n",
      "Epoch 76 | Val loss: 0.007561408914625645\n",
      "Epoch 77 | Total train loss: 8.875336068409752\n",
      "Epoch 77 | Val loss: 0.009803535416722298\n",
      "Epoch 78 | Total train loss: 7.187780192853097\n",
      "Epoch 78 | Val loss: 0.00940363947302103\n",
      "Epoch 79 | Total train loss: 8.478601067204897\n",
      "Epoch 79 | Val loss: 0.007637490052729845\n",
      "Epoch 80 | Total train loss: 6.849523044028274\n",
      "Epoch 80 | Val loss: 0.006037943530827761\n",
      "Epoch 81 | Total train loss: 8.135484693602848\n",
      "Epoch 81 | Val loss: 0.005974684841930866\n",
      "Epoch 82 | Total train loss: 7.533711873879838\n",
      "Epoch 82 | Val loss: 0.007031684275716543\n",
      "Epoch 83 | Total train loss: 7.266740463856763\n",
      "Epoch 83 | Val loss: 0.007232799660414457\n",
      "Epoch 84 | Total train loss: 8.190363781778842\n",
      "Epoch 84 | Val loss: 0.008846865966916084\n",
      "Epoch 85 | Total train loss: 7.89454187790443\n",
      "Epoch 85 | Val loss: 0.005547398701310158\n",
      "Epoch 86 | Total train loss: 6.631774457377105\n",
      "Epoch 86 | Val loss: 0.005835721269249916\n",
      "Epoch 87 | Total train loss: 6.782053778030047\n",
      "Epoch 87 | Val loss: 0.009176528081297874\n",
      "Epoch 88 | Total train loss: 7.5160913803071026\n",
      "Epoch 88 | Val loss: 0.005180349573493004\n",
      "Epoch 89 | Total train loss: 7.774264997308137\n",
      "Epoch 89 | Val loss: 0.007773745339363813\n",
      "Epoch 90 | Total train loss: 7.217416011066234\n",
      "Epoch 90 | Val loss: 0.008113086223602295\n",
      "Epoch 91 | Total train loss: 7.420509822625377\n",
      "Epoch 91 | Val loss: 0.005143034737557173\n",
      "Epoch 92 | Total train loss: 7.631507099964892\n",
      "Epoch 92 | Val loss: 0.011190525256097317\n",
      "Epoch 93 | Total train loss: 6.036793344070588\n",
      "Epoch 93 | Val loss: 0.008841639384627342\n",
      "Epoch 94 | Total train loss: 7.225219024401213\n",
      "Epoch 94 | Val loss: 0.006037011276930571\n",
      "Epoch 95 | Total train loss: 6.864600465087051\n",
      "Epoch 95 | Val loss: 0.00633283331990242\n",
      "Epoch 96 | Total train loss: 6.388411121617878\n",
      "Epoch 96 | Val loss: 0.007446022238582373\n",
      "Epoch 97 | Total train loss: 4.835749479317997\n",
      "Epoch 97 | Val loss: 0.008739208802580833\n",
      "Epoch 98 | Total train loss: 7.02462292752557\n",
      "Epoch 98 | Val loss: 0.007386977784335613\n",
      "Epoch 99 | Total train loss: 7.217870715232493\n",
      "Epoch 99 | Val loss: 0.006850088480859995\n",
      "Epoch 100 | Total train loss: 6.368045125382764\n",
      "Epoch 100 | Val loss: 0.006367214489728212\n",
      "Test loss: 0.008159615099430084\n",
      "Model: xsmall | Training complete\n",
      "Model: xsmall | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xsmall | Plots made and saved\n",
      "Model: xsmall | Predictions saved\n",
      "Model: xsmall | Model saved\n",
      "Model: xsmall | Garbage collected, states deleted\n",
      "Model: small | Started loop\n",
      "Model: small | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 115.52238716211286\n",
      "Epoch 1 | Val loss: 0.0511864610016346\n",
      "Epoch 2 | Total train loss: 48.8188499824646\n",
      "Epoch 2 | Val loss: 0.037525128573179245\n",
      "Epoch 3 | Total train loss: 41.95088343946918\n",
      "Epoch 3 | Val loss: 0.051473651081323624\n",
      "Epoch 4 | Total train loss: 44.723830946020826\n",
      "Epoch 4 | Val loss: 0.04231753945350647\n",
      "Epoch 5 | Total train loss: 33.470249924783275\n",
      "Epoch 5 | Val loss: 0.03513085097074509\n",
      "Epoch 6 | Total train loss: 32.827818654117436\n",
      "Epoch 6 | Val loss: 0.034078143537044525\n",
      "Epoch 7 | Total train loss: 31.320162477166377\n",
      "Epoch 7 | Val loss: 0.026165101677179337\n",
      "Epoch 8 | Total train loss: 24.99197718358073\n",
      "Epoch 8 | Val loss: 0.023198911920189857\n",
      "Epoch 9 | Total train loss: 26.296105008206723\n",
      "Epoch 9 | Val loss: 0.026357706636190414\n",
      "Epoch 10 | Total train loss: 21.095708942665624\n",
      "Epoch 10 | Val loss: 0.022693129256367683\n",
      "Epoch 11 | Total train loss: 24.207324844672257\n",
      "Epoch 11 | Val loss: 0.02205076441168785\n",
      "Epoch 12 | Total train loss: 20.103540811567655\n",
      "Epoch 12 | Val loss: 0.023219194263219833\n",
      "Epoch 13 | Total train loss: 17.95029169889449\n",
      "Epoch 13 | Val loss: 0.023204196244478226\n",
      "Epoch 14 | Total train loss: 19.238502910966417\n",
      "Epoch 14 | Val loss: 0.01890880987048149\n",
      "Epoch 15 | Total train loss: 17.474730262400044\n",
      "Epoch 15 | Val loss: 0.017754994332790375\n",
      "Epoch 16 | Total train loss: 17.03107431252738\n",
      "Epoch 16 | Val loss: 0.022178031504154205\n",
      "Epoch 17 | Total train loss: 14.824602639744626\n",
      "Epoch 17 | Val loss: 0.014184987172484398\n",
      "Epoch 18 | Total train loss: 13.053118534425266\n",
      "Epoch 18 | Val loss: 0.014178095385432243\n",
      "Epoch 19 | Total train loss: 16.663634321981135\n",
      "Epoch 19 | Val loss: 0.018682586029171944\n",
      "Epoch 20 | Total train loss: 14.716839392729526\n",
      "Epoch 20 | Val loss: 0.02318301424384117\n",
      "Epoch 21 | Total train loss: 17.55183010152996\n",
      "Epoch 21 | Val loss: 0.015743359923362732\n",
      "Epoch 22 | Total train loss: 15.826719302540823\n",
      "Epoch 22 | Val loss: 0.013432695530354977\n",
      "Epoch 23 | Total train loss: 11.677661335808807\n",
      "Epoch 23 | Val loss: 0.017202485352754593\n",
      "Epoch 24 | Total train loss: 12.207721533207405\n",
      "Epoch 24 | Val loss: 0.013578488491475582\n",
      "Epoch 25 | Total train loss: 15.618437661186363\n",
      "Epoch 25 | Val loss: 0.009531643241643906\n",
      "Epoch 26 | Total train loss: 12.635182436911236\n",
      "Epoch 26 | Val loss: 0.014720872975885868\n",
      "Epoch 27 | Total train loss: 13.06374021794204\n",
      "Epoch 27 | Val loss: 0.014952133409678936\n",
      "Epoch 28 | Total train loss: 11.581976535479043\n",
      "Epoch 28 | Val loss: 0.01686660759150982\n",
      "Epoch 29 | Total train loss: 12.382360946427525\n",
      "Epoch 29 | Val loss: 0.01249566301703453\n",
      "Epoch 30 | Total train loss: 10.802665955565772\n",
      "Epoch 30 | Val loss: 0.012290670536458492\n",
      "Epoch 31 | Total train loss: 12.016300876899095\n",
      "Epoch 31 | Val loss: 0.011114290915429592\n",
      "Epoch 32 | Total train loss: 9.755773578974186\n",
      "Epoch 32 | Val loss: 0.010960531421005726\n",
      "Epoch 33 | Total train loss: 12.656582180266014\n",
      "Epoch 33 | Val loss: 0.012632330879569054\n",
      "Epoch 34 | Total train loss: 8.94341647737292\n",
      "Epoch 34 | Val loss: 0.01602504961192608\n",
      "Epoch 35 | Total train loss: 10.566246081763438\n",
      "Epoch 35 | Val loss: 0.012511280365288258\n",
      "Epoch 36 | Total train loss: 9.927346379269238\n",
      "Epoch 36 | Val loss: 0.00914060976356268\n",
      "Epoch 37 | Total train loss: 10.688469573652355\n",
      "Epoch 37 | Val loss: 0.012789926491677761\n",
      "Epoch 38 | Total train loss: 8.067300450382845\n",
      "Epoch 38 | Val loss: 0.010919470340013504\n",
      "Epoch 39 | Total train loss: 9.408275320528674\n",
      "Epoch 39 | Val loss: 0.00823174323886633\n",
      "Epoch 40 | Total train loss: 8.617772427627187\n",
      "Epoch 40 | Val loss: 0.010050946846604347\n",
      "Epoch 41 | Total train loss: 7.884074133691229\n",
      "Epoch 41 | Val loss: 0.006822153925895691\n",
      "Epoch 42 | Total train loss: 9.200885654210879\n",
      "Epoch 42 | Val loss: 0.008015522733330727\n",
      "Epoch 43 | Total train loss: 10.691316688024017\n",
      "Epoch 43 | Val loss: 0.01031388808041811\n",
      "Epoch 44 | Total train loss: 9.149635467211283\n",
      "Epoch 44 | Val loss: 0.008132432587444782\n",
      "Epoch 45 | Total train loss: 9.272081963988057\n",
      "Epoch 45 | Val loss: 0.006362684071063995\n",
      "Epoch 46 | Total train loss: 9.781542092802056\n",
      "Epoch 46 | Val loss: 0.008170750923454762\n",
      "Epoch 47 | Total train loss: 8.384731180144627\n",
      "Epoch 47 | Val loss: 0.007413503713905811\n",
      "Epoch 48 | Total train loss: 10.318268266848463\n",
      "Epoch 48 | Val loss: 0.008215381763875484\n",
      "Epoch 49 | Total train loss: 8.228346572462556\n",
      "Epoch 49 | Val loss: 0.01016735378652811\n",
      "Epoch 50 | Total train loss: 9.136504249717746\n",
      "Epoch 50 | Val loss: 0.009491965174674988\n",
      "Epoch 51 | Total train loss: 8.461553514529896\n",
      "Epoch 51 | Val loss: 0.007737892679870129\n",
      "Epoch 52 | Total train loss: 8.104531334813828\n",
      "Epoch 52 | Val loss: 0.009722677059471607\n",
      "Epoch 53 | Total train loss: 8.430834632416236\n",
      "Epoch 53 | Val loss: 0.006903660483658314\n",
      "Epoch 54 | Total train loss: 8.111301427379999\n",
      "Epoch 54 | Val loss: 0.00886853039264679\n",
      "Epoch 55 | Total train loss: 8.478044056243561\n",
      "Epoch 55 | Val loss: 0.008117367513477802\n",
      "Epoch 56 | Total train loss: 9.138637473193626\n",
      "Epoch 56 | Val loss: 0.00838304590433836\n",
      "Epoch 57 | Total train loss: 8.310756020955523\n",
      "Epoch 57 | Val loss: 0.007558617740869522\n",
      "Epoch 58 | Total train loss: 8.503423620305853\n",
      "Epoch 58 | Val loss: 0.013001685030758381\n",
      "Epoch 59 | Total train loss: 8.173358599136009\n",
      "Epoch 59 | Val loss: 0.011610830202698708\n",
      "Epoch 60 | Total train loss: 7.845204414804357\n",
      "Epoch 60 | Val loss: 0.009298814460635185\n",
      "Epoch 61 | Total train loss: 6.767987939266277\n",
      "Epoch 61 | Val loss: 0.008704052306711674\n",
      "Epoch 62 | Total train loss: 8.024941581409166\n",
      "Epoch 62 | Val loss: 0.0045343926176428795\n",
      "Epoch 63 | Total train loss: 7.289433550475394\n",
      "Epoch 63 | Val loss: 0.006390822120010853\n",
      "Epoch 64 | Total train loss: 7.867143509344942\n",
      "Epoch 64 | Val loss: 0.006993061862885952\n",
      "Epoch 65 | Total train loss: 7.411537039106861\n",
      "Epoch 65 | Val loss: 0.005319911055266857\n",
      "Epoch 66 | Total train loss: 7.738534813361184\n",
      "Epoch 66 | Val loss: 0.007969049736857414\n",
      "Epoch 67 | Total train loss: 6.294263662549156\n",
      "Epoch 67 | Val loss: 0.010178949683904648\n",
      "Epoch 68 | Total train loss: 6.879453623400309\n",
      "Epoch 68 | Val loss: 0.0053468323312699795\n",
      "Epoch 69 | Total train loss: 6.807294204084428\n",
      "Epoch 69 | Val loss: 0.005772950127720833\n",
      "Epoch 70 | Total train loss: 6.616792290942897\n",
      "Epoch 70 | Val loss: 0.008762072771787643\n",
      "Epoch 71 | Total train loss: 8.502470956914635\n",
      "Epoch 71 | Val loss: 0.005816135089844465\n",
      "Epoch 72 | Total train loss: 7.107409158298651\n",
      "Epoch 72 | Val loss: 0.005780097562819719\n",
      "Epoch 73 | Total train loss: 6.659176890447725\n",
      "Epoch 73 | Val loss: 0.005300038959830999\n",
      "Epoch 74 | Total train loss: 5.663038587345454\n",
      "Epoch 74 | Val loss: 0.0053506591357290745\n",
      "Epoch 75 | Total train loss: 6.419342309743115\n",
      "Epoch 75 | Val loss: 0.006321178749203682\n",
      "Epoch 76 | Total train loss: 7.092935014953127\n",
      "Epoch 76 | Val loss: 0.005354054272174835\n",
      "Epoch 77 | Total train loss: 6.8224353003770375\n",
      "Epoch 77 | Val loss: 0.007395025342702866\n",
      "Epoch 78 | Total train loss: 6.472944254256618\n",
      "Epoch 78 | Val loss: 0.0059678624384105206\n",
      "Epoch 79 | Total train loss: 5.907709618226363\n",
      "Epoch 79 | Val loss: 0.006968813017010689\n",
      "Epoch 80 | Total train loss: 6.835868178466342\n",
      "Epoch 80 | Val loss: 0.005400305613875389\n",
      "Epoch 81 | Total train loss: 6.479733817700435\n",
      "Epoch 81 | Val loss: 0.0070328000001609325\n",
      "Epoch 82 | Total train loss: 7.362592795698447\n",
      "Epoch 82 | Val loss: 0.007155005354434252\n",
      "Epoch 83 | Total train loss: 6.6394975332577815\n",
      "Epoch 83 | Val loss: 0.006144625600427389\n",
      "Epoch 84 | Total train loss: 7.153734473742588\n",
      "Epoch 84 | Val loss: 0.005474423058331013\n",
      "Epoch 85 | Total train loss: 7.345571096513417\n",
      "Epoch 85 | Val loss: 0.006608685944229364\n",
      "Epoch 86 | Total train loss: 7.523991594630388\n",
      "Epoch 86 | Val loss: 0.0054271831177175045\n",
      "Epoch 87 | Total train loss: 6.545429874672209\n",
      "Epoch 87 | Val loss: 0.006321575026959181\n",
      "Epoch 88 | Total train loss: 5.402812131193741\n",
      "Epoch 88 | Val loss: 0.005974284373223782\n",
      "Epoch 89 | Total train loss: 5.531110076187815\n",
      "Epoch 89 | Val loss: 0.005246855318546295\n",
      "Epoch 90 | Total train loss: 5.267434011021123\n",
      "Epoch 90 | Val loss: 0.005178169347345829\n",
      "Epoch 91 | Total train loss: 5.62579379409749\n",
      "Epoch 91 | Val loss: 0.006279497407376766\n",
      "Epoch 92 | Total train loss: 5.512219287143125\n",
      "Epoch 92 | Val loss: 0.005862020887434483\n",
      "Epoch 93 | Total train loss: 6.6629185359956224\n",
      "Epoch 93 | Val loss: 0.006876444909721613\n",
      "Epoch 94 | Total train loss: 5.109347167908709\n",
      "Epoch 94 | Val loss: 0.005174216348677874\n",
      "Epoch 95 | Total train loss: 5.835919105117284\n",
      "Epoch 95 | Val loss: 0.005927025806158781\n",
      "Epoch 96 | Total train loss: 7.3649139346208585\n",
      "Epoch 96 | Val loss: 0.0068557774648070335\n",
      "Epoch 97 | Total train loss: 5.771576712996762\n",
      "Epoch 97 | Val loss: 0.008803521282970905\n",
      "Epoch 98 | Total train loss: 5.538944212968545\n",
      "Epoch 98 | Val loss: 0.006821418181061745\n",
      "Epoch 99 | Total train loss: 4.77628064699968\n",
      "Epoch 99 | Val loss: 0.004998400341719389\n",
      "Epoch 100 | Total train loss: 6.357800544009081\n",
      "Epoch 100 | Val loss: 0.00533266318961978\n",
      "Test loss: 0.005689084529876709\n",
      "Model: small | Training complete\n",
      "Model: small | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: small | Plots made and saved\n",
      "Model: small | Predictions saved\n",
      "Model: small | Model saved\n",
      "Model: small | Garbage collected, states deleted\n",
      "Model: medium | Started loop\n",
      "Model: medium | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 121.48098203638801\n",
      "Epoch 1 | Val loss: 0.05880791321396828\n",
      "Epoch 2 | Total train loss: 48.624570964295344\n",
      "Epoch 2 | Val loss: 0.038501620292663574\n",
      "Epoch 3 | Total train loss: 40.92954271871349\n",
      "Epoch 3 | Val loss: 0.03484298661351204\n",
      "Epoch 4 | Total train loss: 36.91730908728823\n",
      "Epoch 4 | Val loss: 0.036957498639822006\n",
      "Epoch 5 | Total train loss: 33.51699703201757\n",
      "Epoch 5 | Val loss: 0.023318875581026077\n",
      "Epoch 6 | Total train loss: 24.184140664875486\n",
      "Epoch 6 | Val loss: 0.029538238421082497\n",
      "Epoch 7 | Total train loss: 25.776797703461398\n",
      "Epoch 7 | Val loss: 0.023403450846672058\n",
      "Epoch 8 | Total train loss: 20.452682005612587\n",
      "Epoch 8 | Val loss: 0.020475037395954132\n",
      "Epoch 9 | Total train loss: 20.578793968282298\n",
      "Epoch 9 | Val loss: 0.01725350320339203\n",
      "Epoch 10 | Total train loss: 19.176718143874268\n",
      "Epoch 10 | Val loss: 0.01880725473165512\n",
      "Epoch 11 | Total train loss: 19.54152647364299\n",
      "Epoch 11 | Val loss: 0.024229632690548897\n",
      "Epoch 12 | Total train loss: 18.876035207748828\n",
      "Epoch 12 | Val loss: 0.017416279762983322\n",
      "Epoch 13 | Total train loss: 20.0124769450249\n",
      "Epoch 13 | Val loss: 0.017775800079107285\n",
      "Epoch 14 | Total train loss: 18.195205771216024\n",
      "Epoch 14 | Val loss: 0.018716489896178246\n",
      "Epoch 15 | Total train loss: 16.73543044066696\n",
      "Epoch 15 | Val loss: 0.016974424943327904\n",
      "Epoch 16 | Total train loss: 17.93407986039128\n",
      "Epoch 16 | Val loss: 0.013897719793021679\n",
      "Epoch 17 | Total train loss: 17.774083644437894\n",
      "Epoch 17 | Val loss: 0.01393487025052309\n",
      "Epoch 18 | Total train loss: 13.65336394449514\n",
      "Epoch 18 | Val loss: 0.021143855527043343\n",
      "Epoch 19 | Total train loss: 14.613270486130432\n",
      "Epoch 19 | Val loss: 0.017191773280501366\n",
      "Epoch 20 | Total train loss: 13.323332151773116\n",
      "Epoch 20 | Val loss: 0.014834594912827015\n",
      "Epoch 21 | Total train loss: 14.206016010270332\n",
      "Epoch 21 | Val loss: 0.013336566276848316\n",
      "Epoch 22 | Total train loss: 14.540692642170143\n",
      "Epoch 22 | Val loss: 0.011182009242475033\n",
      "Epoch 23 | Total train loss: 11.852183166197847\n",
      "Epoch 23 | Val loss: 0.014039218425750732\n",
      "Epoch 24 | Total train loss: 12.005562787777421\n",
      "Epoch 24 | Val loss: 0.014412641525268555\n",
      "Epoch 25 | Total train loss: 13.41542903699792\n",
      "Epoch 25 | Val loss: 0.015190038830041885\n",
      "Epoch 26 | Total train loss: 12.486744685456415\n",
      "Epoch 26 | Val loss: 0.019586632028222084\n",
      "Epoch 27 | Total train loss: 11.185481861431981\n",
      "Epoch 27 | Val loss: 0.010139621794223785\n",
      "Epoch 28 | Total train loss: 10.27600304688167\n",
      "Epoch 28 | Val loss: 0.013722293078899384\n",
      "Epoch 29 | Total train loss: 12.272782819622535\n",
      "Epoch 29 | Val loss: 0.009786187671124935\n",
      "Epoch 30 | Total train loss: 9.57022679815418\n",
      "Epoch 30 | Val loss: 0.017802560701966286\n",
      "Epoch 31 | Total train loss: 9.634035046716917\n",
      "Epoch 31 | Val loss: 0.009489522315561771\n",
      "Epoch 32 | Total train loss: 9.636369665597158\n",
      "Epoch 32 | Val loss: 0.013750440441071987\n",
      "Epoch 33 | Total train loss: 11.11383097872519\n",
      "Epoch 33 | Val loss: 0.009898150339722633\n",
      "Epoch 34 | Total train loss: 8.795451437858901\n",
      "Epoch 34 | Val loss: 0.011026359163224697\n",
      "Epoch 35 | Total train loss: 10.464608335863886\n",
      "Epoch 35 | Val loss: 0.011607500724494457\n",
      "Epoch 36 | Total train loss: 9.74251425258035\n",
      "Epoch 36 | Val loss: 0.011265651322901249\n",
      "Epoch 37 | Total train loss: 8.699722957046106\n",
      "Epoch 37 | Val loss: 0.011449574492871761\n",
      "Epoch 38 | Total train loss: 10.75634010289832\n",
      "Epoch 38 | Val loss: 0.014059146866202354\n",
      "Epoch 39 | Total train loss: 9.315035291295317\n",
      "Epoch 39 | Val loss: 0.008598077110946178\n",
      "Epoch 40 | Total train loss: 9.017111662784941\n",
      "Epoch 40 | Val loss: 0.007168072275817394\n",
      "Epoch 41 | Total train loss: 8.82171469847708\n",
      "Epoch 41 | Val loss: 0.0076456693932414055\n",
      "Epoch 42 | Total train loss: 8.959551814087718\n",
      "Epoch 42 | Val loss: 0.00924358144402504\n",
      "Epoch 43 | Total train loss: 9.034601071090492\n",
      "Epoch 43 | Val loss: 0.006326994393020868\n",
      "Epoch 44 | Total train loss: 8.652762025752509\n",
      "Epoch 44 | Val loss: 0.009115925058722496\n",
      "Epoch 45 | Total train loss: 8.546711914962884\n",
      "Epoch 45 | Val loss: 0.006960233207792044\n",
      "Epoch 46 | Total train loss: 7.405877669257734\n",
      "Epoch 46 | Val loss: 0.010855667293071747\n",
      "Epoch 47 | Total train loss: 7.8531153993099\n",
      "Epoch 47 | Val loss: 0.009695918299257755\n",
      "Epoch 48 | Total train loss: 7.915689590629313\n",
      "Epoch 48 | Val loss: 0.00937431026250124\n",
      "Epoch 49 | Total train loss: 8.645074467796803\n",
      "Epoch 49 | Val loss: 0.005366365425288677\n",
      "Epoch 50 | Total train loss: 8.295460724448276\n",
      "Epoch 50 | Val loss: 0.0063628824427723885\n",
      "Epoch 51 | Total train loss: 6.669931745323538\n",
      "Epoch 51 | Val loss: 0.008535000495612621\n",
      "Epoch 52 | Total train loss: 7.075019519973466\n",
      "Epoch 52 | Val loss: 0.01271007675677538\n",
      "Epoch 53 | Total train loss: 7.873468599517764\n",
      "Epoch 53 | Val loss: 0.00472622225061059\n",
      "Epoch 54 | Total train loss: 8.878703826989295\n",
      "Epoch 54 | Val loss: 0.007606856059283018\n",
      "Epoch 55 | Total train loss: 7.524946119920891\n",
      "Epoch 55 | Val loss: 0.010317942127585411\n",
      "Epoch 56 | Total train loss: 8.793847670725654\n",
      "Epoch 56 | Val loss: 0.007252899929881096\n",
      "Epoch 57 | Total train loss: 6.954752679634112\n",
      "Epoch 57 | Val loss: 0.009965536184608936\n",
      "Epoch 58 | Total train loss: 8.262216705986361\n",
      "Epoch 58 | Val loss: 0.005681042093783617\n",
      "Epoch 59 | Total train loss: 7.085202167666694\n",
      "Epoch 59 | Val loss: 0.0054071079939603806\n",
      "Epoch 60 | Total train loss: 8.22718973132163\n",
      "Epoch 60 | Val loss: 0.007779235951602459\n",
      "Epoch 61 | Total train loss: 5.948667904176091\n",
      "Epoch 61 | Val loss: 0.005919779185205698\n",
      "Epoch 62 | Total train loss: 7.154585959484336\n",
      "Epoch 62 | Val loss: 0.006716195959597826\n",
      "Epoch 63 | Total train loss: 7.51847175672151\n",
      "Epoch 63 | Val loss: 0.007493423763662577\n",
      "Epoch 64 | Total train loss: 6.138725733465435\n",
      "Epoch 64 | Val loss: 0.006154518108814955\n",
      "Epoch 65 | Total train loss: 5.89303182161666\n",
      "Epoch 65 | Val loss: 0.007097368594259024\n",
      "Epoch 66 | Total train loss: 6.635524222727383\n",
      "Epoch 66 | Val loss: 0.006742684170603752\n",
      "Epoch 67 | Total train loss: 7.744910078014755\n",
      "Epoch 67 | Val loss: 0.0081042954698205\n",
      "Epoch 68 | Total train loss: 5.9736622630380225\n",
      "Epoch 68 | Val loss: 0.005590367596596479\n",
      "Epoch 69 | Total train loss: 6.487659249093326\n",
      "Epoch 69 | Val loss: 0.0054032206535339355\n",
      "Epoch 70 | Total train loss: 7.363351027688509\n",
      "Epoch 70 | Val loss: 0.007751949597150087\n",
      "Epoch 71 | Total train loss: 5.351123003234079\n",
      "Epoch 71 | Val loss: 0.006137387827038765\n",
      "Epoch 72 | Total train loss: 7.005475689821424\n",
      "Epoch 72 | Val loss: 0.008291080594062805\n",
      "Epoch 73 | Total train loss: 7.067692496283286\n",
      "Epoch 73 | Val loss: 0.013949940912425518\n",
      "Epoch 74 | Total train loss: 5.533282512928054\n",
      "Epoch 74 | Val loss: 0.005876863840967417\n",
      "Epoch 75 | Total train loss: 6.3978635898636185\n",
      "Epoch 75 | Val loss: 0.004837688524276018\n",
      "Epoch 76 | Total train loss: 6.986469755590861\n",
      "Epoch 76 | Val loss: 0.005929569248110056\n",
      "Epoch 77 | Total train loss: 5.237022358307058\n",
      "Epoch 77 | Val loss: 0.0064876205287873745\n",
      "Epoch 78 | Total train loss: 6.050526509197823\n",
      "Epoch 78 | Val loss: 0.007842576131224632\n",
      "Epoch 79 | Total train loss: 5.691901612250604\n",
      "Epoch 79 | Val loss: 0.005176599137485027\n",
      "Epoch 80 | Total train loss: 6.355758101890501\n",
      "Epoch 80 | Val loss: 0.006653001066297293\n",
      "Epoch 81 | Total train loss: 5.629920522787415\n",
      "Epoch 81 | Val loss: 0.004380600526928902\n",
      "Epoch 82 | Total train loss: 6.430432830132986\n",
      "Epoch 82 | Val loss: 0.004114242270588875\n",
      "Epoch 83 | Total train loss: 5.268077301702306\n",
      "Epoch 83 | Val loss: 0.0071387640200555325\n",
      "Epoch 84 | Total train loss: 5.426873572398165\n",
      "Epoch 84 | Val loss: 0.006222152151167393\n",
      "Epoch 85 | Total train loss: 5.31744266080841\n",
      "Epoch 85 | Val loss: 0.004896425176411867\n",
      "Epoch 86 | Total train loss: 5.733237262034095\n",
      "Epoch 86 | Val loss: 0.005250698886811733\n",
      "Epoch 87 | Total train loss: 7.258043707567197\n",
      "Epoch 87 | Val loss: 0.007953302003443241\n",
      "Epoch 88 | Total train loss: 4.464808010843569\n",
      "Epoch 88 | Val loss: 0.007191473618149757\n",
      "Epoch 89 | Total train loss: 6.032837283996685\n",
      "Epoch 89 | Val loss: 0.006488049402832985\n",
      "Epoch 90 | Total train loss: 6.39190342431084\n",
      "Epoch 90 | Val loss: 0.005765203852206469\n",
      "Epoch 91 | Total train loss: 5.986992070746965\n",
      "Epoch 91 | Val loss: 0.005316185764968395\n",
      "Epoch 92 | Total train loss: 5.661279910479436\n",
      "Epoch 92 | Val loss: 0.004793108440935612\n",
      "Epoch 93 | Total train loss: 6.062213724499486\n",
      "Epoch 93 | Val loss: 0.008644902147352695\n",
      "Epoch 94 | Total train loss: 5.377791075888979\n",
      "Epoch 94 | Val loss: 0.0072417547926306725\n",
      "Epoch 95 | Total train loss: 6.252314758133707\n",
      "Epoch 95 | Val loss: 0.004582340829074383\n",
      "Epoch 96 | Total train loss: 5.769398486627779\n",
      "Epoch 96 | Val loss: 0.005088388454169035\n",
      "Epoch 97 | Total train loss: 5.627386672790379\n",
      "Epoch 97 | Val loss: 0.005350715480744839\n",
      "Epoch 98 | Total train loss: 5.025054025084955\n",
      "Epoch 98 | Val loss: 0.004329644609242678\n",
      "Epoch 99 | Total train loss: 5.222331855813081\n",
      "Epoch 99 | Val loss: 0.00443628104403615\n",
      "Epoch 100 | Total train loss: 5.547597396325955\n",
      "Epoch 100 | Val loss: 0.0052374424412846565\n",
      "Test loss: 0.005283440463244915\n",
      "Model: medium | Training complete\n",
      "Model: medium | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: medium | Plots made and saved\n",
      "Model: medium | Predictions saved\n",
      "Model: medium | Model saved\n",
      "Model: medium | Garbage collected, states deleted\n",
      "Model: large | Started loop\n",
      "Model: large | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 83.05330041534762\n",
      "Epoch 1 | Val loss: 0.03615459054708481\n",
      "Epoch 2 | Total train loss: 41.41665101284889\n",
      "Epoch 2 | Val loss: 0.03321573883295059\n",
      "Epoch 3 | Total train loss: 37.00694963278329\n",
      "Epoch 3 | Val loss: 0.02778015471994877\n",
      "Epoch 4 | Total train loss: 25.665604713231005\n",
      "Epoch 4 | Val loss: 0.026947345584630966\n",
      "Epoch 5 | Total train loss: 23.464429431009194\n",
      "Epoch 5 | Val loss: 0.024819741025567055\n",
      "Epoch 6 | Total train loss: 21.18709816050523\n",
      "Epoch 6 | Val loss: 0.016682825982570648\n",
      "Epoch 7 | Total train loss: 17.52369174166097\n",
      "Epoch 7 | Val loss: 0.02182166837155819\n",
      "Epoch 8 | Total train loss: 17.61294099103634\n",
      "Epoch 8 | Val loss: 0.02106817625463009\n",
      "Epoch 9 | Total train loss: 21.055643629129463\n",
      "Epoch 9 | Val loss: 0.016654949635267258\n",
      "Epoch 10 | Total train loss: 17.524685748018783\n",
      "Epoch 10 | Val loss: 0.014635778963565826\n",
      "Epoch 11 | Total train loss: 19.930980972524594\n",
      "Epoch 11 | Val loss: 0.009988860227167606\n",
      "Epoch 12 | Total train loss: 14.834531077362044\n",
      "Epoch 12 | Val loss: 0.016452843323349953\n",
      "Epoch 13 | Total train loss: 16.744542890242883\n",
      "Epoch 13 | Val loss: 0.02399020828306675\n",
      "Epoch 14 | Total train loss: 15.043576377774457\n",
      "Epoch 14 | Val loss: 0.013535582460463047\n",
      "Epoch 15 | Total train loss: 15.210731773445787\n",
      "Epoch 15 | Val loss: 0.009469952434301376\n",
      "Epoch 16 | Total train loss: 11.916161481812196\n",
      "Epoch 16 | Val loss: 0.02041744254529476\n",
      "Epoch 17 | Total train loss: 10.872271285263196\n",
      "Epoch 17 | Val loss: 0.017414970323443413\n",
      "Epoch 18 | Total train loss: 10.926496606303772\n",
      "Epoch 18 | Val loss: 0.008821511641144753\n",
      "Epoch 19 | Total train loss: 12.117436920387604\n",
      "Epoch 19 | Val loss: 0.022516874596476555\n",
      "Epoch 20 | Total train loss: 10.260700242266921\n",
      "Epoch 20 | Val loss: 0.009712738916277885\n",
      "Epoch 21 | Total train loss: 13.324140361707578\n",
      "Epoch 21 | Val loss: 0.005495119374245405\n",
      "Epoch 22 | Total train loss: 10.320032058982179\n",
      "Epoch 22 | Val loss: 0.012261915020644665\n",
      "Epoch 23 | Total train loss: 10.888563716484441\n",
      "Epoch 23 | Val loss: 0.012764422222971916\n",
      "Epoch 24 | Total train loss: 9.05041042036595\n",
      "Epoch 24 | Val loss: 0.013242249377071857\n",
      "Epoch 25 | Total train loss: 10.923635051689416\n",
      "Epoch 25 | Val loss: 0.0076125371269881725\n",
      "Epoch 26 | Total train loss: 8.675362465991839\n",
      "Epoch 26 | Val loss: 0.013264468871057034\n",
      "Epoch 27 | Total train loss: 9.51936251592565\n",
      "Epoch 27 | Val loss: 0.00894396286457777\n",
      "Epoch 28 | Total train loss: 10.403486612471283\n",
      "Epoch 28 | Val loss: 0.009870773181319237\n",
      "Epoch 29 | Total train loss: 9.270779998631951\n",
      "Epoch 29 | Val loss: 0.01123170368373394\n",
      "Epoch 30 | Total train loss: 7.902080185624982\n",
      "Epoch 30 | Val loss: 0.013756164349615574\n",
      "Epoch 31 | Total train loss: 9.33206021156775\n",
      "Epoch 31 | Val loss: 0.0063615827821195126\n",
      "Epoch 32 | Total train loss: 9.883599497335695\n",
      "Epoch 32 | Val loss: 0.009333152323961258\n",
      "Epoch 33 | Total train loss: 10.979520093118936\n",
      "Epoch 33 | Val loss: 0.009841918013989925\n",
      "Epoch 34 | Total train loss: 7.0807157997330705\n",
      "Epoch 34 | Val loss: 0.00892177876085043\n",
      "Epoch 35 | Total train loss: 7.654431381679842\n",
      "Epoch 35 | Val loss: 0.008750361390411854\n",
      "Epoch 36 | Total train loss: 9.220353188206445\n",
      "Epoch 36 | Val loss: 0.00778259476646781\n",
      "Epoch 37 | Total train loss: 7.924897567441235\n",
      "Epoch 37 | Val loss: 0.00882826466113329\n",
      "Epoch 38 | Total train loss: 6.843502627063344\n",
      "Epoch 38 | Val loss: 0.014743833802640438\n",
      "Epoch 39 | Total train loss: 8.417315485730569\n",
      "Epoch 39 | Val loss: 0.007847446016967297\n",
      "Epoch 40 | Total train loss: 6.272173614371582\n",
      "Epoch 40 | Val loss: 0.008396838791668415\n",
      "Epoch 41 | Total train loss: 7.474406925085532\n",
      "Epoch 41 | Val loss: 0.00866714958101511\n",
      "Epoch 42 | Total train loss: 6.952926105322376\n",
      "Epoch 42 | Val loss: 0.009884852916002274\n",
      "Epoch 43 | Total train loss: 8.057609642900161\n",
      "Epoch 43 | Val loss: 0.007969689555466175\n",
      "Epoch 44 | Total train loss: 8.622430532319527\n",
      "Epoch 44 | Val loss: 0.006605392321944237\n",
      "Epoch 45 | Total train loss: 7.592540932653492\n",
      "Epoch 45 | Val loss: 0.006646185182034969\n",
      "Epoch 46 | Total train loss: 6.792975268328291\n",
      "Epoch 46 | Val loss: 0.00798527430742979\n",
      "Epoch 47 | Total train loss: 8.327205668072338\n",
      "Epoch 47 | Val loss: 0.005689013749361038\n",
      "Epoch 48 | Total train loss: 7.9020995472860704\n",
      "Epoch 48 | Val loss: 0.0060576507821679115\n",
      "Epoch 49 | Total train loss: 6.287190466809193\n",
      "Epoch 49 | Val loss: 0.01474337000399828\n",
      "Epoch 50 | Total train loss: 6.690288836516402\n",
      "Epoch 50 | Val loss: 0.006959784775972366\n",
      "Epoch 51 | Total train loss: 6.687186224128311\n",
      "Epoch 51 | Val loss: 0.007312641944736242\n",
      "Epoch 52 | Total train loss: 7.510968987096476\n",
      "Epoch 52 | Val loss: 0.006403584033250809\n",
      "Epoch 53 | Total train loss: 5.924166915352714\n",
      "Epoch 53 | Val loss: 0.010527527891099453\n",
      "Epoch 54 | Total train loss: 6.540702613047415\n",
      "Epoch 54 | Val loss: 0.00529378280043602\n",
      "Epoch 55 | Total train loss: 7.939798124241179\n",
      "Epoch 55 | Val loss: 0.008326786570250988\n",
      "Epoch 56 | Total train loss: 6.742157760346117\n",
      "Epoch 56 | Val loss: 0.006004342343658209\n",
      "Epoch 57 | Total train loss: 7.564937992282125\n",
      "Epoch 57 | Val loss: 0.005980692803859711\n",
      "Epoch 58 | Total train loss: 7.405970577440826\n",
      "Epoch 58 | Val loss: 0.00929013080894947\n",
      "Epoch 59 | Total train loss: 8.346801794092613\n",
      "Epoch 59 | Val loss: 0.004647531546652317\n",
      "Epoch 60 | Total train loss: 6.314978290983845\n",
      "Epoch 60 | Val loss: 0.003762743901461363\n",
      "Epoch 61 | Total train loss: 7.06950238917409\n",
      "Epoch 61 | Val loss: 0.006680918857455254\n",
      "Epoch 62 | Total train loss: 6.683442736630411\n",
      "Epoch 62 | Val loss: 0.00547192245721817\n",
      "Epoch 63 | Total train loss: 6.026086735761055\n",
      "Epoch 63 | Val loss: 0.003796379314735532\n",
      "Epoch 64 | Total train loss: 6.050784314556381\n",
      "Epoch 64 | Val loss: 0.0070092384703457355\n",
      "Epoch 65 | Total train loss: 6.395393099491002\n",
      "Epoch 65 | Val loss: 0.00387065508402884\n",
      "Epoch 66 | Total train loss: 6.839142157348988\n",
      "Epoch 66 | Val loss: 0.006069221533834934\n",
      "Epoch 67 | Total train loss: 5.979593529423596\n",
      "Epoch 67 | Val loss: 0.009051626548171043\n",
      "Epoch 68 | Total train loss: 6.256793840896535\n",
      "Epoch 68 | Val loss: 0.0069481017999351025\n",
      "Epoch 69 | Total train loss: 5.834609494216352\n",
      "Epoch 69 | Val loss: 0.009956550784409046\n",
      "Epoch 70 | Total train loss: 7.607854935646628\n",
      "Epoch 70 | Val loss: 0.009897269308567047\n",
      "Epoch 71 | Total train loss: 5.567671106257308\n",
      "Epoch 71 | Val loss: 0.007721327245235443\n",
      "Epoch 72 | Total train loss: 4.593404971401128\n",
      "Epoch 72 | Val loss: 0.0058829584158957005\n",
      "Epoch 73 | Total train loss: 6.395263684752081\n",
      "Epoch 73 | Val loss: 0.006362684071063995\n",
      "Epoch 74 | Total train loss: 5.477351560049328\n",
      "Epoch 74 | Val loss: 0.005833029747009277\n",
      "Epoch 75 | Total train loss: 7.160834801040551\n",
      "Epoch 75 | Val loss: 0.003973666112869978\n",
      "Epoch 76 | Total train loss: 6.941389899722026\n",
      "Epoch 76 | Val loss: 0.0069906204007565975\n",
      "Epoch 77 | Total train loss: 4.9019270925835485\n",
      "Epoch 77 | Val loss: 0.005462356377393007\n",
      "Epoch 78 | Total train loss: 5.706947934532991\n",
      "Epoch 78 | Val loss: 0.006960137281566858\n",
      "Epoch 79 | Total train loss: 5.7894007559916645\n",
      "Epoch 79 | Val loss: 0.003952807746827602\n",
      "Epoch 80 | Total train loss: 6.591769201519014\n",
      "Epoch 80 | Val loss: 0.004861020017415285\n",
      "Epoch 81 | Total train loss: 6.247176939455244\n",
      "Epoch 81 | Val loss: 0.004546594340354204\n",
      "Epoch 82 | Total train loss: 4.503214545647097\n",
      "Epoch 82 | Val loss: 0.0036918220575898886\n",
      "Epoch 83 | Total train loss: 5.8915860959965585\n",
      "Epoch 83 | Val loss: 0.005672343075275421\n",
      "Epoch 84 | Total train loss: 5.52534565169276\n",
      "Epoch 84 | Val loss: 0.007234042044728994\n",
      "Epoch 85 | Total train loss: 4.852877494957397\n",
      "Epoch 85 | Val loss: 0.0125017324462533\n",
      "Epoch 86 | Total train loss: 5.064436814572218\n",
      "Epoch 86 | Val loss: 0.007163342088460922\n",
      "Epoch 87 | Total train loss: 5.079210184668227\n",
      "Epoch 87 | Val loss: 0.0070400722324848175\n",
      "Epoch 88 | Total train loss: 4.695940538521086\n",
      "Epoch 88 | Val loss: 0.003631700063124299\n",
      "Epoch 89 | Total train loss: 4.289366184018263\n",
      "Epoch 89 | Val loss: 0.006372485775500536\n",
      "Epoch 90 | Total train loss: 5.162470854925914\n",
      "Epoch 90 | Val loss: 0.004648153204470873\n",
      "Epoch 91 | Total train loss: 5.829668264765303\n",
      "Epoch 91 | Val loss: 0.003474263474345207\n",
      "Epoch 92 | Total train loss: 5.373118106313996\n",
      "Epoch 92 | Val loss: 0.002286059083417058\n",
      "Epoch 93 | Total train loss: 5.013025510786804\n",
      "Epoch 93 | Val loss: 0.005016578361392021\n",
      "Epoch 94 | Total train loss: 5.032559937799476\n",
      "Epoch 94 | Val loss: 0.004045702517032623\n",
      "Epoch 95 | Total train loss: 5.7834924268715895\n",
      "Epoch 95 | Val loss: 0.005408440716564655\n",
      "Epoch 96 | Total train loss: 5.22044220779344\n",
      "Epoch 96 | Val loss: 0.005159997846931219\n",
      "Epoch 97 | Total train loss: 4.8387513275147285\n",
      "Epoch 97 | Val loss: 0.004528337623924017\n",
      "Epoch 98 | Total train loss: 4.208322575791868\n",
      "Epoch 98 | Val loss: 0.004421400371938944\n",
      "Epoch 99 | Total train loss: 4.14586357163239\n",
      "Epoch 99 | Val loss: 0.005744346883147955\n",
      "Epoch 100 | Total train loss: 6.115306433182354\n",
      "Epoch 100 | Val loss: 0.006973349489271641\n",
      "Test loss: 0.007502581924200058\n",
      "Model: large | Training complete\n",
      "Model: large | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: large | Plots made and saved\n",
      "Model: large | Predictions saved\n",
      "Model: large | Model saved\n",
      "Model: large | Garbage collected, states deleted\n",
      "Model: xlarge | Started loop\n",
      "Model: xlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 61.281908401302644\n",
      "Epoch 1 | Val loss: 0.04244794324040413\n",
      "Epoch 2 | Total train loss: 33.883135233239955\n",
      "Epoch 2 | Val loss: 0.027266357094049454\n",
      "Epoch 3 | Total train loss: 26.104256960997873\n",
      "Epoch 3 | Val loss: 0.027247026562690735\n",
      "Epoch 4 | Total train loss: 22.545681075008815\n",
      "Epoch 4 | Val loss: 0.02415064349770546\n",
      "Epoch 5 | Total train loss: 21.337654997366826\n",
      "Epoch 5 | Val loss: 0.02309902012348175\n",
      "Epoch 6 | Total train loss: 19.786133376833277\n",
      "Epoch 6 | Val loss: 0.016758505254983902\n",
      "Epoch 7 | Total train loss: 18.98838409396558\n",
      "Epoch 7 | Val loss: 0.019860999658703804\n",
      "Epoch 8 | Total train loss: 17.323813071440327\n",
      "Epoch 8 | Val loss: 0.014673075638711452\n",
      "Epoch 9 | Total train loss: 16.199641478687226\n",
      "Epoch 9 | Val loss: 0.017579680308699608\n",
      "Epoch 10 | Total train loss: 16.7286431845057\n",
      "Epoch 10 | Val loss: 0.012758816592395306\n",
      "Epoch 11 | Total train loss: 15.094661049400202\n",
      "Epoch 11 | Val loss: 0.013735143467783928\n",
      "Epoch 12 | Total train loss: 14.168429338526039\n",
      "Epoch 12 | Val loss: 0.014384042471647263\n",
      "Epoch 13 | Total train loss: 11.908836103963495\n",
      "Epoch 13 | Val loss: 0.014274976216256618\n",
      "Epoch 14 | Total train loss: 11.868802709102965\n",
      "Epoch 14 | Val loss: 0.01554148830473423\n",
      "Epoch 15 | Total train loss: 9.603620454756765\n",
      "Epoch 15 | Val loss: 0.00584123469889164\n",
      "Epoch 16 | Total train loss: 12.505004368390018\n",
      "Epoch 16 | Val loss: 0.011644064448773861\n",
      "Epoch 17 | Total train loss: 11.064790739831437\n",
      "Epoch 17 | Val loss: 0.012227996252477169\n",
      "Epoch 18 | Total train loss: 10.917740683096781\n",
      "Epoch 18 | Val loss: 0.016116822138428688\n",
      "Epoch 19 | Total train loss: 9.938654050507921\n",
      "Epoch 19 | Val loss: 0.007482230197638273\n",
      "Epoch 20 | Total train loss: 11.493217269933211\n",
      "Epoch 20 | Val loss: 0.006903454661369324\n",
      "Epoch 21 | Total train loss: 9.142841562507101\n",
      "Epoch 21 | Val loss: 0.006684379652142525\n",
      "Epoch 22 | Total train loss: 8.7359299326381\n",
      "Epoch 22 | Val loss: 0.011040251702070236\n",
      "Epoch 23 | Total train loss: 9.489001797269339\n",
      "Epoch 23 | Val loss: 0.009329588152468204\n",
      "Epoch 24 | Total train loss: 11.028997996944781\n",
      "Epoch 24 | Val loss: 0.008219720795750618\n",
      "Epoch 25 | Total train loss: 9.384545902269679\n",
      "Epoch 25 | Val loss: 0.005853906273841858\n",
      "Epoch 26 | Total train loss: 10.44987395419139\n",
      "Epoch 26 | Val loss: 0.008442871272563934\n",
      "Epoch 27 | Total train loss: 8.164729534756816\n",
      "Epoch 27 | Val loss: 0.009692619554698467\n",
      "Epoch 28 | Total train loss: 9.569890383711936\n",
      "Epoch 28 | Val loss: 0.013397382572293282\n",
      "Epoch 29 | Total train loss: 9.511955254202462\n",
      "Epoch 29 | Val loss: 0.008167414925992489\n",
      "Epoch 30 | Total train loss: 10.546168669230383\n",
      "Epoch 30 | Val loss: 0.009491987526416779\n",
      "Epoch 31 | Total train loss: 8.405905221506288\n",
      "Epoch 31 | Val loss: 0.008922536857426167\n",
      "Epoch 32 | Total train loss: 10.550776580341108\n",
      "Epoch 32 | Val loss: 0.005672847386449575\n",
      "Epoch 33 | Total train loss: 8.446942835499158\n",
      "Epoch 33 | Val loss: 0.005934551823884249\n",
      "Epoch 34 | Total train loss: 8.902964478877038\n",
      "Epoch 34 | Val loss: 0.005466065835207701\n",
      "Epoch 35 | Total train loss: 7.451995259900457\n",
      "Epoch 35 | Val loss: 0.01061970368027687\n",
      "Epoch 36 | Total train loss: 9.834734320692405\n",
      "Epoch 36 | Val loss: 0.008759298361837864\n",
      "Epoch 37 | Total train loss: 8.15122989126553\n",
      "Epoch 37 | Val loss: 0.010004095733165741\n",
      "Epoch 38 | Total train loss: 8.284305859433516\n",
      "Epoch 38 | Val loss: 0.0075005292892456055\n",
      "Epoch 39 | Total train loss: 9.770029647863112\n",
      "Epoch 39 | Val loss: 0.007257021497935057\n",
      "Epoch 40 | Total train loss: 5.832922486407369\n",
      "Epoch 40 | Val loss: 0.0049689109437167645\n",
      "Epoch 41 | Total train loss: 7.37648467079282\n",
      "Epoch 41 | Val loss: 0.010692395269870758\n",
      "Epoch 42 | Total train loss: 7.952866777091913\n",
      "Epoch 42 | Val loss: 0.005648909602314234\n",
      "Epoch 43 | Total train loss: 8.187044098678484\n",
      "Epoch 43 | Val loss: 0.0046078856103122234\n",
      "Epoch 44 | Total train loss: 6.84616820329893\n",
      "Epoch 44 | Val loss: 0.010904205031692982\n",
      "Epoch 45 | Total train loss: 7.416392081709773\n",
      "Epoch 45 | Val loss: 0.011951131746172905\n",
      "Epoch 46 | Total train loss: 6.094343466801661\n",
      "Epoch 46 | Val loss: 0.005514306947588921\n",
      "Epoch 47 | Total train loss: 8.534792262196675\n",
      "Epoch 47 | Val loss: 0.006428256165236235\n",
      "Epoch 48 | Total train loss: 5.7762759752440616\n",
      "Epoch 48 | Val loss: 0.005002507008612156\n",
      "Epoch 49 | Total train loss: 7.234743727711475\n",
      "Epoch 49 | Val loss: 0.008563052862882614\n",
      "Epoch 50 | Total train loss: 8.321485061919475\n",
      "Epoch 50 | Val loss: 0.0063509284518659115\n",
      "Epoch 51 | Total train loss: 6.969576319301268\n",
      "Epoch 51 | Val loss: 0.007037724833935499\n",
      "Epoch 52 | Total train loss: 6.526646234296777\n",
      "Epoch 52 | Val loss: 0.006059482228010893\n",
      "Epoch 53 | Total train loss: 7.354164960180242\n",
      "Epoch 53 | Val loss: 0.0043203337118029594\n",
      "Epoch 54 | Total train loss: 5.890039496661814\n",
      "Epoch 54 | Val loss: 0.009049126878380775\n",
      "Epoch 55 | Total train loss: 6.874349512347408\n",
      "Epoch 55 | Val loss: 0.009308883920311928\n",
      "Epoch 56 | Total train loss: 5.508457156589429\n",
      "Epoch 56 | Val loss: 0.006083254236727953\n",
      "Epoch 57 | Total train loss: 6.65240433879967\n",
      "Epoch 57 | Val loss: 0.006905767135322094\n",
      "Epoch 58 | Total train loss: 7.019053312822223\n",
      "Epoch 58 | Val loss: 0.004233879968523979\n",
      "Epoch 59 | Total train loss: 6.210230350147356\n",
      "Epoch 59 | Val loss: 0.007509338203817606\n",
      "Epoch 60 | Total train loss: 5.440919434771217\n",
      "Epoch 60 | Val loss: 0.005123430863022804\n",
      "Epoch 61 | Total train loss: 6.013430586047093\n",
      "Epoch 61 | Val loss: 0.004823740106076002\n",
      "Epoch 62 | Total train loss: 6.112047985676668\n",
      "Epoch 62 | Val loss: 0.0048177847638726234\n",
      "Epoch 63 | Total train loss: 6.042179059703358\n",
      "Epoch 63 | Val loss: 0.008031698875129223\n",
      "Epoch 64 | Total train loss: 6.604284683371247\n",
      "Epoch 64 | Val loss: 0.010173660703003407\n",
      "Epoch 65 | Total train loss: 4.316549564273487\n",
      "Epoch 65 | Val loss: 0.005293713416904211\n",
      "Epoch 66 | Total train loss: 6.6224433748679985\n",
      "Epoch 66 | Val loss: 0.006015806458890438\n",
      "Epoch 67 | Total train loss: 5.803635816980723\n",
      "Epoch 67 | Val loss: 0.004642288200557232\n",
      "Epoch 68 | Total train loss: 5.315629262622906\n",
      "Epoch 68 | Val loss: 0.003907955251634121\n",
      "Epoch 69 | Total train loss: 5.01466675651443\n",
      "Epoch 69 | Val loss: 0.004154632333666086\n",
      "Epoch 70 | Total train loss: 5.774564342467556\n",
      "Epoch 70 | Val loss: 0.0042984988540410995\n",
      "Epoch 71 | Total train loss: 5.668048213734892\n",
      "Epoch 71 | Val loss: 0.0027565229684114456\n",
      "Epoch 72 | Total train loss: 5.106855794770581\n",
      "Epoch 72 | Val loss: 0.003936764318495989\n",
      "Epoch 73 | Total train loss: 4.9501016247340885\n",
      "Epoch 73 | Val loss: 0.004823508206754923\n",
      "Epoch 74 | Total train loss: 5.261167221302458\n",
      "Epoch 74 | Val loss: 0.003491101786494255\n",
      "Epoch 75 | Total train loss: 5.3231965270322235\n",
      "Epoch 75 | Val loss: 0.007491028402000666\n",
      "Epoch 76 | Total train loss: 5.463407350264561\n",
      "Epoch 76 | Val loss: 0.010453581809997559\n",
      "Epoch 77 | Total train loss: 4.3943389494078815\n",
      "Epoch 77 | Val loss: 0.006647466216236353\n",
      "Epoch 78 | Total train loss: 5.856239390855535\n",
      "Epoch 78 | Val loss: 0.006027135998010635\n",
      "Epoch 79 | Total train loss: 6.100872859782726\n",
      "Epoch 79 | Val loss: 0.003858775831758976\n",
      "Epoch 80 | Total train loss: 4.7462822931888695\n",
      "Epoch 80 | Val loss: 0.005591591354459524\n",
      "Epoch 81 | Total train loss: 3.8325736201013\n",
      "Epoch 81 | Val loss: 0.00386509345844388\n",
      "Epoch 82 | Total train loss: 6.028761504834847\n",
      "Epoch 82 | Val loss: 0.0031826579943299294\n",
      "Epoch 83 | Total train loss: 5.48413083045449\n",
      "Epoch 83 | Val loss: 0.014014231972396374\n",
      "Epoch 84 | Total train loss: 4.767160363008827\n",
      "Epoch 84 | Val loss: 0.004111336078494787\n",
      "Epoch 85 | Total train loss: 4.854701696728853\n",
      "Epoch 85 | Val loss: 0.005075935274362564\n",
      "Epoch 86 | Total train loss: 4.666159247206764\n",
      "Epoch 86 | Val loss: 0.004403195343911648\n",
      "Epoch 87 | Total train loss: 4.602655572216236\n",
      "Epoch 87 | Val loss: 0.007374571170657873\n",
      "Epoch 88 | Total train loss: 5.288702156643183\n",
      "Epoch 88 | Val loss: 0.003074292792007327\n",
      "Epoch 89 | Total train loss: 4.157168127203249\n",
      "Epoch 89 | Val loss: 0.004882789216935635\n",
      "Epoch 90 | Total train loss: 4.745831783427537\n",
      "Epoch 90 | Val loss: 0.004174130503088236\n",
      "Epoch 91 | Total train loss: 4.1055175655199605\n",
      "Epoch 91 | Val loss: 0.004575910046696663\n",
      "Epoch 92 | Total train loss: 4.616403235242728\n",
      "Epoch 92 | Val loss: 0.0038941320963203907\n",
      "Epoch 93 | Total train loss: 4.044679459870395\n",
      "Epoch 93 | Val loss: 0.0048590730875730515\n",
      "Epoch 94 | Total train loss: 4.116900597345023\n",
      "Epoch 94 | Val loss: 0.004558189772069454\n",
      "Epoch 95 | Total train loss: 4.347135369160583\n",
      "Epoch 95 | Val loss: 0.004246667958796024\n",
      "Epoch 96 | Total train loss: 4.978128632534407\n",
      "Epoch 96 | Val loss: 0.003730837954208255\n",
      "Epoch 97 | Total train loss: 4.487780126140251\n",
      "Epoch 97 | Val loss: 0.003980735316872597\n",
      "Epoch 98 | Total train loss: 4.155660701164493\n",
      "Epoch 98 | Val loss: 0.004354467615485191\n",
      "Epoch 99 | Total train loss: 4.586700640407344\n",
      "Epoch 99 | Val loss: 0.003495373297482729\n",
      "Epoch 100 | Total train loss: 3.3854016118210026\n",
      "Epoch 100 | Val loss: 0.0020788817200809717\n",
      "Test loss: 0.0034772877115756273\n",
      "Model: xlarge | Training complete\n",
      "Model: xlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xlarge | Plots made and saved\n",
      "Model: xlarge | Predictions saved\n",
      "Model: xlarge | Model saved\n",
      "Model: xlarge | Garbage collected, states deleted\n",
      "Model: xxlarge | Started loop\n",
      "Model: xxlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 57.688108947169894\n",
      "Epoch 1 | Val loss: 0.05046998709440231\n",
      "Epoch 2 | Total train loss: 41.47092497995618\n",
      "Epoch 2 | Val loss: 0.03764507547020912\n",
      "Epoch 3 | Total train loss: 26.1585139952972\n",
      "Epoch 3 | Val loss: 0.022239770740270615\n",
      "Epoch 4 | Total train loss: 21.713723961361666\n",
      "Epoch 4 | Val loss: 0.027134273201227188\n",
      "Epoch 5 | Total train loss: 21.894894716524504\n",
      "Epoch 5 | Val loss: 0.014660250395536423\n",
      "Epoch 6 | Total train loss: 19.483923829711102\n",
      "Epoch 6 | Val loss: 0.028850803151726723\n",
      "Epoch 7 | Total train loss: 18.557951125259933\n",
      "Epoch 7 | Val loss: 0.01589772291481495\n",
      "Epoch 8 | Total train loss: 20.588006752610454\n",
      "Epoch 8 | Val loss: 0.01966712437570095\n",
      "Epoch 9 | Total train loss: 19.92353711991018\n",
      "Epoch 9 | Val loss: 0.019014691933989525\n",
      "Epoch 10 | Total train loss: 19.283726438726262\n",
      "Epoch 10 | Val loss: 0.01665651984512806\n",
      "Epoch 11 | Total train loss: 16.25728124708212\n",
      "Epoch 11 | Val loss: 0.010219430550932884\n",
      "Epoch 12 | Total train loss: 16.059527396402245\n",
      "Epoch 12 | Val loss: 0.011038518510758877\n",
      "Epoch 13 | Total train loss: 15.329362428275772\n",
      "Epoch 13 | Val loss: 0.020291022956371307\n",
      "Epoch 14 | Total train loss: 14.58904809163323\n",
      "Epoch 14 | Val loss: 0.012431477196514606\n",
      "Epoch 15 | Total train loss: 13.188524470777338\n",
      "Epoch 15 | Val loss: 0.00941897090524435\n",
      "Epoch 16 | Total train loss: 12.11900645687922\n",
      "Epoch 16 | Val loss: 0.010501876473426819\n",
      "Epoch 17 | Total train loss: 10.721248382915405\n",
      "Epoch 17 | Val loss: 0.013284877873957157\n",
      "Epoch 18 | Total train loss: 11.224679355881108\n",
      "Epoch 18 | Val loss: 0.014810442924499512\n",
      "Epoch 19 | Total train loss: 12.387369009067243\n",
      "Epoch 19 | Val loss: 0.007952123880386353\n",
      "Epoch 20 | Total train loss: 12.410076827367902\n",
      "Epoch 20 | Val loss: 0.007370255887508392\n",
      "Epoch 21 | Total train loss: 11.38933536343211\n",
      "Epoch 21 | Val loss: 0.007698666304349899\n",
      "Epoch 22 | Total train loss: 11.395829959845742\n",
      "Epoch 22 | Val loss: 0.008177748881280422\n",
      "Epoch 23 | Total train loss: 10.75297273716592\n",
      "Epoch 23 | Val loss: 0.011907289735972881\n",
      "Epoch 24 | Total train loss: 9.498328744073888\n",
      "Epoch 24 | Val loss: 0.007625148631632328\n",
      "Epoch 25 | Total train loss: 11.209532617815057\n",
      "Epoch 25 | Val loss: 0.01148312445729971\n",
      "Epoch 26 | Total train loss: 9.88217607078468\n",
      "Epoch 26 | Val loss: 0.008233253844082355\n",
      "Epoch 27 | Total train loss: 9.71053839949127\n",
      "Epoch 27 | Val loss: 0.006027891766279936\n",
      "Epoch 28 | Total train loss: 8.751840575327947\n",
      "Epoch 28 | Val loss: 0.009730911813676357\n",
      "Epoch 29 | Total train loss: 9.318506571250055\n",
      "Epoch 29 | Val loss: 0.008767746388912201\n",
      "Epoch 30 | Total train loss: 10.270431443779671\n",
      "Epoch 30 | Val loss: 0.007126328535377979\n",
      "Epoch 31 | Total train loss: 9.748615292365002\n",
      "Epoch 31 | Val loss: 0.011636859737336636\n",
      "Epoch 32 | Total train loss: 8.257140455904391\n",
      "Epoch 32 | Val loss: 0.006464752834290266\n",
      "Epoch 33 | Total train loss: 10.247025808905619\n",
      "Epoch 33 | Val loss: 0.0075363400392234325\n",
      "Epoch 34 | Total train loss: 10.771024363756482\n",
      "Epoch 34 | Val loss: 0.007882925681769848\n",
      "Epoch 35 | Total train loss: 8.748651494758633\n",
      "Epoch 35 | Val loss: 0.012125764042139053\n",
      "Epoch 36 | Total train loss: 8.029817374445884\n",
      "Epoch 36 | Val loss: 0.009120507165789604\n",
      "Epoch 37 | Total train loss: 8.687608544206796\n",
      "Epoch 37 | Val loss: 0.009302917867898941\n",
      "Epoch 38 | Total train loss: 7.7208906005935205\n",
      "Epoch 38 | Val loss: 0.007100903429090977\n",
      "Epoch 39 | Total train loss: 9.597196406022249\n",
      "Epoch 39 | Val loss: 0.01194328535348177\n",
      "Epoch 40 | Total train loss: 7.155805431937779\n",
      "Epoch 40 | Val loss: 0.010584349744021893\n",
      "Epoch 41 | Total train loss: 8.134469448219534\n",
      "Epoch 41 | Val loss: 0.006438570562750101\n",
      "Epoch 42 | Total train loss: 7.892095293622106\n",
      "Epoch 42 | Val loss: 0.006400479003787041\n",
      "Epoch 43 | Total train loss: 7.323897974636566\n",
      "Epoch 43 | Val loss: 0.008031059987843037\n",
      "Epoch 44 | Total train loss: 8.44715727158291\n",
      "Epoch 44 | Val loss: 0.009706513956189156\n",
      "Epoch 45 | Total train loss: 6.836156779169414\n",
      "Epoch 45 | Val loss: 0.0069831907749176025\n",
      "Epoch 46 | Total train loss: 7.512145470384894\n",
      "Epoch 46 | Val loss: 0.005809958558529615\n",
      "Epoch 47 | Total train loss: 6.500584243097819\n",
      "Epoch 47 | Val loss: 0.010274648666381836\n",
      "Epoch 48 | Total train loss: 7.157466572218482\n",
      "Epoch 48 | Val loss: 0.009076555259525776\n",
      "Epoch 49 | Total train loss: 5.978224335666596\n",
      "Epoch 49 | Val loss: 0.007392082363367081\n",
      "Epoch 50 | Total train loss: 6.770763029272757\n",
      "Epoch 50 | Val loss: 0.007142003625631332\n",
      "Epoch 51 | Total train loss: 6.1456016594117955\n",
      "Epoch 51 | Val loss: 0.004742573015391827\n",
      "Epoch 52 | Total train loss: 5.592593721084427\n",
      "Epoch 52 | Val loss: 0.0056307753548026085\n",
      "Epoch 53 | Total train loss: 6.25708759441153\n",
      "Epoch 53 | Val loss: 0.003545618848875165\n",
      "Epoch 54 | Total train loss: 6.672454566864303\n",
      "Epoch 54 | Val loss: 0.00798447523266077\n",
      "Epoch 55 | Total train loss: 6.523729549071959\n",
      "Epoch 55 | Val loss: 0.006373803596943617\n",
      "Epoch 56 | Total train loss: 6.872356756948193\n",
      "Epoch 56 | Val loss: 0.005791865289211273\n",
      "Epoch 57 | Total train loss: 4.673473249726612\n",
      "Epoch 57 | Val loss: 0.003617446403950453\n",
      "Epoch 58 | Total train loss: 6.291218537491886\n",
      "Epoch 58 | Val loss: 0.009976396337151527\n",
      "Epoch 59 | Total train loss: 6.4560648443581385\n",
      "Epoch 59 | Val loss: 0.0033666491508483887\n",
      "Epoch 60 | Total train loss: 6.8296421263542015\n",
      "Epoch 60 | Val loss: 0.023643380030989647\n",
      "Epoch 61 | Total train loss: 6.368751129544762\n",
      "Epoch 61 | Val loss: 0.0044389087706804276\n",
      "Epoch 62 | Total train loss: 6.331255092740776\n",
      "Epoch 62 | Val loss: 0.006788864266127348\n",
      "Epoch 63 | Total train loss: 5.752619309716394\n",
      "Epoch 63 | Val loss: 0.0056302244774997234\n",
      "Epoch 64 | Total train loss: 5.828589215976393\n",
      "Epoch 64 | Val loss: 0.007248657755553722\n",
      "Epoch 65 | Total train loss: 5.936070546526253\n",
      "Epoch 65 | Val loss: 0.004238661378622055\n",
      "Epoch 66 | Total train loss: 4.756739063813711\n",
      "Epoch 66 | Val loss: 0.0024332376196980476\n",
      "Epoch 67 | Total train loss: 5.881852687085598\n",
      "Epoch 67 | Val loss: 0.00411455100402236\n",
      "Epoch 68 | Total train loss: 6.215853063617885\n",
      "Epoch 68 | Val loss: 0.004471088759601116\n",
      "Epoch 69 | Total train loss: 5.369598144398765\n",
      "Epoch 69 | Val loss: 0.005915908142924309\n",
      "Epoch 70 | Total train loss: 4.686485656766763\n",
      "Epoch 70 | Val loss: 0.006745690014213324\n",
      "Epoch 71 | Total train loss: 4.862938176924217\n",
      "Epoch 71 | Val loss: 0.005236584227532148\n",
      "Epoch 72 | Total train loss: 5.965428516080124\n",
      "Epoch 72 | Val loss: 0.004440446849912405\n",
      "Epoch 73 | Total train loss: 4.915760806359231\n",
      "Epoch 73 | Val loss: 0.007733311969786882\n",
      "Epoch 74 | Total train loss: 5.304468145598889\n",
      "Epoch 74 | Val loss: 0.004022250417619944\n",
      "Epoch 75 | Total train loss: 5.563947246031148\n",
      "Epoch 75 | Val loss: 0.004545505158603191\n",
      "Epoch 76 | Total train loss: 6.724946407281095\n",
      "Epoch 76 | Val loss: 0.005552226677536964\n",
      "Epoch 77 | Total train loss: 4.409234960999129\n",
      "Epoch 77 | Val loss: 0.010925235226750374\n",
      "Epoch 78 | Total train loss: 5.924359611396753\n",
      "Epoch 78 | Val loss: 0.004259421024471521\n",
      "Epoch 79 | Total train loss: 4.900833459593038\n",
      "Epoch 79 | Val loss: 0.0046598175540566444\n",
      "Epoch 80 | Total train loss: 4.513881956773048\n",
      "Epoch 80 | Val loss: 0.004029790870845318\n",
      "Epoch 81 | Total train loss: 4.813788265456424\n",
      "Epoch 81 | Val loss: 0.006430376321077347\n",
      "Epoch 82 | Total train loss: 6.354690164364456\n",
      "Epoch 82 | Val loss: 0.005070331506431103\n",
      "Epoch 83 | Total train loss: 5.095223941555787\n",
      "Epoch 83 | Val loss: 0.004619162995368242\n",
      "Epoch 84 | Total train loss: 4.194011477295021\n",
      "Epoch 84 | Val loss: 0.008843665011227131\n",
      "Epoch 85 | Total train loss: 4.894831234201092\n",
      "Epoch 85 | Val loss: 0.004439254757016897\n",
      "Epoch 86 | Total train loss: 4.501929677541966\n",
      "Epoch 86 | Val loss: 0.0061172484420239925\n",
      "Epoch 87 | Total train loss: 4.580732164838139\n",
      "Epoch 87 | Val loss: 0.0032100349199026823\n",
      "Epoch 88 | Total train loss: 3.987612505979996\n",
      "Epoch 88 | Val loss: 0.0027570032980293036\n",
      "Epoch 89 | Total train loss: 4.745209988126817\n",
      "Epoch 89 | Val loss: 0.004191576037555933\n",
      "Epoch 90 | Total train loss: 4.763815585386169\n",
      "Epoch 90 | Val loss: 0.0034764232113957405\n",
      "Epoch 91 | Total train loss: 3.423937987124191\n",
      "Epoch 91 | Val loss: 0.0043915556743741035\n",
      "Epoch 92 | Total train loss: 4.844294402118749\n",
      "Epoch 92 | Val loss: 0.005825637374073267\n",
      "Epoch 93 | Total train loss: 5.406749076533515\n",
      "Epoch 93 | Val loss: 0.005732137709856033\n",
      "Epoch 94 | Total train loss: 3.253640647445536\n",
      "Epoch 94 | Val loss: 0.003026349935680628\n",
      "Epoch 95 | Total train loss: 4.252312241325626\n",
      "Epoch 95 | Val loss: 0.0034584917593747377\n",
      "Epoch 96 | Total train loss: 5.216972848410705\n",
      "Epoch 96 | Val loss: 0.00404234416782856\n",
      "Epoch 97 | Total train loss: 5.161581783868428\n",
      "Epoch 97 | Val loss: 0.003000869182869792\n",
      "Epoch 98 | Total train loss: 3.5485683467561557\n",
      "Epoch 98 | Val loss: 0.008511743508279324\n",
      "Epoch 99 | Total train loss: 4.297704369114172\n",
      "Epoch 99 | Val loss: 0.0035500654485076666\n",
      "Epoch 100 | Total train loss: 4.766377853861059\n",
      "Epoch 100 | Val loss: 0.0037418485153466463\n",
      "Test loss: 0.0030556428246200085\n",
      "Model: xxlarge | Training complete\n",
      "Model: xxlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxlarge | Plots made and saved\n",
      "Model: xxlarge | Predictions saved\n",
      "Model: xxlarge | Model saved\n",
      "Model: xxlarge | Garbage collected, states deleted\n",
      "Model: xxxlarge | Started loop\n",
      "Model: xxxlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 59.50186982153173\n",
      "Epoch 1 | Val loss: 0.04559757560491562\n",
      "Epoch 2 | Total train loss: 32.37993334823841\n",
      "Epoch 2 | Val loss: 0.027733491733670235\n",
      "Epoch 3 | Total train loss: 28.71880228404325\n",
      "Epoch 3 | Val loss: 0.021427132189273834\n",
      "Epoch 4 | Total train loss: 21.938983365005697\n",
      "Epoch 4 | Val loss: 0.020453883334994316\n",
      "Epoch 5 | Total train loss: 23.307509479159307\n",
      "Epoch 5 | Val loss: 0.02847115322947502\n",
      "Epoch 6 | Total train loss: 21.538090964575076\n",
      "Epoch 6 | Val loss: 0.017677854746580124\n",
      "Epoch 7 | Total train loss: 23.924998797810986\n",
      "Epoch 7 | Val loss: 0.023320075124502182\n",
      "Epoch 8 | Total train loss: 18.958628942636096\n",
      "Epoch 8 | Val loss: 0.020370163023471832\n",
      "Epoch 9 | Total train loss: 17.801238105547327\n",
      "Epoch 9 | Val loss: 0.017314206808805466\n",
      "Epoch 10 | Total train loss: 16.1096127541299\n",
      "Epoch 10 | Val loss: 0.018112970516085625\n",
      "Epoch 11 | Total train loss: 18.288027287169825\n",
      "Epoch 11 | Val loss: 0.01877482421696186\n",
      "Epoch 12 | Total train loss: 14.63753711374784\n",
      "Epoch 12 | Val loss: 0.01188608817756176\n",
      "Epoch 13 | Total train loss: 12.798647604273356\n",
      "Epoch 13 | Val loss: 0.016443604603409767\n",
      "Epoch 14 | Total train loss: 12.70578856236807\n",
      "Epoch 14 | Val loss: 0.02024412900209427\n",
      "Epoch 15 | Total train loss: 13.087508536135374\n",
      "Epoch 15 | Val loss: 0.022548815235495567\n",
      "Epoch 16 | Total train loss: 11.64525317174548\n",
      "Epoch 16 | Val loss: 0.012181692756712437\n",
      "Epoch 17 | Total train loss: 11.213884954169544\n",
      "Epoch 17 | Val loss: 0.007652526255697012\n",
      "Epoch 18 | Total train loss: 10.324664492091301\n",
      "Epoch 18 | Val loss: 0.006319371517747641\n",
      "Epoch 19 | Total train loss: 10.629085343071893\n",
      "Epoch 19 | Val loss: 0.00971544161438942\n",
      "Epoch 20 | Total train loss: 10.20061265482309\n",
      "Epoch 20 | Val loss: 0.012857352383434772\n",
      "Epoch 21 | Total train loss: 9.005035799748839\n",
      "Epoch 21 | Val loss: 0.008652831427752972\n",
      "Epoch 22 | Total train loss: 9.808705218386137\n",
      "Epoch 22 | Val loss: 0.007905606180429459\n",
      "Epoch 23 | Total train loss: 9.7234534106816\n",
      "Epoch 23 | Val loss: 0.010877476073801517\n",
      "Epoch 24 | Total train loss: 10.687782742286345\n",
      "Epoch 24 | Val loss: 0.011496592313051224\n",
      "Epoch 25 | Total train loss: 10.71519454549059\n",
      "Epoch 25 | Val loss: 0.010743226855993271\n",
      "Epoch 26 | Total train loss: 9.131832172951363\n",
      "Epoch 26 | Val loss: 0.010538619942963123\n",
      "Epoch 27 | Total train loss: 8.706842180486547\n",
      "Epoch 27 | Val loss: 0.011885561980307102\n",
      "Epoch 28 | Total train loss: 8.173971367354625\n",
      "Epoch 28 | Val loss: 0.013939902186393738\n",
      "Epoch 29 | Total train loss: 8.274216574181082\n",
      "Epoch 29 | Val loss: 0.004948929883539677\n",
      "Epoch 30 | Total train loss: 8.813581263059405\n",
      "Epoch 30 | Val loss: 0.008960491977632046\n",
      "Epoch 31 | Total train loss: 7.7789893944848245\n",
      "Epoch 31 | Val loss: 0.008137538097798824\n",
      "Epoch 32 | Total train loss: 9.104074187536071\n",
      "Epoch 32 | Val loss: 0.011607819236814976\n",
      "Epoch 33 | Total train loss: 8.309566221315436\n",
      "Epoch 33 | Val loss: 0.007784335874021053\n",
      "Epoch 34 | Total train loss: 8.320953373661723\n",
      "Epoch 34 | Val loss: 0.007241056766360998\n",
      "Epoch 35 | Total train loss: 7.53545608576519\n",
      "Epoch 35 | Val loss: 0.006186887621879578\n",
      "Epoch 36 | Total train loss: 8.506228340021153\n",
      "Epoch 36 | Val loss: 0.006293357349932194\n",
      "Epoch 37 | Total train loss: 8.098368331189135\n",
      "Epoch 37 | Val loss: 0.008399785496294498\n",
      "Epoch 38 | Total train loss: 8.401537846566498\n",
      "Epoch 38 | Val loss: 0.006686571519821882\n",
      "Epoch 39 | Total train loss: 8.598253848178501\n",
      "Epoch 39 | Val loss: 0.0071186781860888\n",
      "Epoch 40 | Total train loss: 7.254970284810497\n",
      "Epoch 40 | Val loss: 0.006500216666609049\n",
      "Epoch 41 | Total train loss: 8.52310495488382\n",
      "Epoch 41 | Val loss: 0.009155010804533958\n",
      "Epoch 42 | Total train loss: 7.796678500561939\n",
      "Epoch 42 | Val loss: 0.008610227145254612\n",
      "Epoch 43 | Total train loss: 6.765985848351079\n",
      "Epoch 43 | Val loss: 0.009130628779530525\n",
      "Epoch 44 | Total train loss: 7.235611797085312\n",
      "Epoch 44 | Val loss: 0.005061410367488861\n",
      "Epoch 45 | Total train loss: 7.756455981317686\n",
      "Epoch 45 | Val loss: 0.006378347985446453\n",
      "Epoch 46 | Total train loss: 6.536659920199611\n",
      "Epoch 46 | Val loss: 0.00728150550276041\n",
      "Epoch 47 | Total train loss: 6.616007704044478\n",
      "Epoch 47 | Val loss: 0.0061704483814537525\n",
      "Epoch 48 | Total train loss: 7.604006881455348\n",
      "Epoch 48 | Val loss: 0.005433462560176849\n",
      "Epoch 49 | Total train loss: 7.283061025354016\n",
      "Epoch 49 | Val loss: 0.007427870761603117\n",
      "Epoch 50 | Total train loss: 6.075286174675966\n",
      "Epoch 50 | Val loss: 0.007422778755426407\n",
      "Epoch 51 | Total train loss: 6.1398864539273745\n",
      "Epoch 51 | Val loss: 0.0034899923484772444\n",
      "Epoch 52 | Total train loss: 6.692476860230727\n",
      "Epoch 52 | Val loss: 0.006047719158232212\n",
      "Epoch 53 | Total train loss: 7.219875427774355\n",
      "Epoch 53 | Val loss: 0.006356774363666773\n",
      "Epoch 54 | Total train loss: 5.735098699257719\n",
      "Epoch 54 | Val loss: 0.0068625882267951965\n",
      "Epoch 55 | Total train loss: 6.263379014528255\n",
      "Epoch 55 | Val loss: 0.006661094259470701\n",
      "Epoch 56 | Total train loss: 5.089055503299619\n",
      "Epoch 56 | Val loss: 0.006079580169171095\n",
      "Epoch 57 | Total train loss: 6.646968090180053\n",
      "Epoch 57 | Val loss: 0.0032026644330471754\n",
      "Epoch 58 | Total train loss: 5.502416269513105\n",
      "Epoch 58 | Val loss: 0.005186401307582855\n",
      "Epoch 59 | Total train loss: 6.679305647195065\n",
      "Epoch 59 | Val loss: 0.006797855254262686\n",
      "Epoch 60 | Total train loss: 5.453259497490876\n",
      "Epoch 60 | Val loss: 0.0046802423894405365\n",
      "Epoch 61 | Total train loss: 5.177748679577007\n",
      "Epoch 61 | Val loss: 0.010776798240840435\n",
      "Epoch 62 | Total train loss: 5.18643220151921\n",
      "Epoch 62 | Val loss: 0.00703930389136076\n",
      "Epoch 63 | Total train loss: 5.154647227832811\n",
      "Epoch 63 | Val loss: 0.004711763001978397\n",
      "Epoch 64 | Total train loss: 5.623601767562491\n",
      "Epoch 64 | Val loss: 0.0027506756596267223\n",
      "Epoch 65 | Total train loss: 5.411834318975707\n",
      "Epoch 65 | Val loss: 0.004991899244487286\n",
      "Epoch 66 | Total train loss: 5.297791168263984\n",
      "Epoch 66 | Val loss: 0.005797511897981167\n",
      "Epoch 67 | Total train loss: 6.232650449197536\n",
      "Epoch 67 | Val loss: 0.00529401795938611\n",
      "Epoch 68 | Total train loss: 5.42489412659819\n",
      "Epoch 68 | Val loss: 0.007622547447681427\n",
      "Epoch 69 | Total train loss: 5.502460697188326\n",
      "Epoch 69 | Val loss: 0.005452725104987621\n",
      "Epoch 70 | Total train loss: 5.123125823024907\n",
      "Epoch 70 | Val loss: 0.005439014174044132\n",
      "Epoch 71 | Total train loss: 5.71573128228556\n",
      "Epoch 71 | Val loss: 0.008615764789283276\n",
      "Epoch 72 | Total train loss: 5.104315115150257\n",
      "Epoch 72 | Val loss: 0.00454670749604702\n",
      "Epoch 73 | Total train loss: 4.8934752181380645\n",
      "Epoch 73 | Val loss: 0.007942620664834976\n",
      "Epoch 74 | Total train loss: 4.698066742113042\n",
      "Epoch 74 | Val loss: 0.00682765431702137\n",
      "Epoch 75 | Total train loss: 4.843361837160387\n",
      "Epoch 75 | Val loss: 0.00392361031845212\n",
      "Epoch 76 | Total train loss: 4.921908847479244\n",
      "Epoch 76 | Val loss: 0.004621039144694805\n",
      "Epoch 77 | Total train loss: 6.174231470428367\n",
      "Epoch 77 | Val loss: 0.00624592462554574\n",
      "Epoch 78 | Total train loss: 4.495441200867845\n",
      "Epoch 78 | Val loss: 0.002786408644169569\n",
      "Epoch 79 | Total train loss: 4.3828328434508705\n",
      "Epoch 79 | Val loss: 0.0029824397061020136\n",
      "Epoch 80 | Total train loss: 5.585727354983671\n",
      "Epoch 80 | Val loss: 0.004006022121757269\n",
      "Epoch 81 | Total train loss: 4.844995103128213\n",
      "Epoch 81 | Val loss: 0.0048704370856285095\n",
      "Epoch 82 | Total train loss: 4.202990207298399\n",
      "Epoch 82 | Val loss: 0.003674356732517481\n",
      "Epoch 83 | Total train loss: 5.168380523730548\n",
      "Epoch 83 | Val loss: 0.005181328859180212\n",
      "Epoch 84 | Total train loss: 3.5624001881851086\n",
      "Epoch 84 | Val loss: 0.0036046099849045277\n",
      "Epoch 85 | Total train loss: 4.025936884852513\n",
      "Epoch 85 | Val loss: 0.0044572544284164906\n",
      "Epoch 86 | Total train loss: 3.9953684759471457\n",
      "Epoch 86 | Val loss: 0.00681851664558053\n",
      "Epoch 87 | Total train loss: 4.106589231685746\n",
      "Epoch 87 | Val loss: 0.004231482278555632\n",
      "Epoch 88 | Total train loss: 4.159869829416834\n",
      "Epoch 88 | Val loss: 0.006639908533543348\n",
      "Epoch 89 | Total train loss: 4.5050752346823355\n",
      "Epoch 89 | Val loss: 0.004561855923384428\n",
      "Epoch 90 | Total train loss: 4.901518343433736\n",
      "Epoch 90 | Val loss: 0.004060197155922651\n",
      "Epoch 91 | Total train loss: 4.5722640832038906\n",
      "Epoch 91 | Val loss: 0.002753189764916897\n",
      "Epoch 92 | Total train loss: 4.196770662985244\n",
      "Epoch 92 | Val loss: 0.003924147225916386\n",
      "Epoch 93 | Total train loss: 5.291418449614838\n",
      "Epoch 93 | Val loss: 0.01127583533525467\n",
      "Epoch 94 | Total train loss: 3.6512919830613555\n",
      "Epoch 94 | Val loss: 0.0026696491986513138\n",
      "Epoch 95 | Total train loss: 5.079335033713164\n",
      "Epoch 95 | Val loss: 0.007102515082806349\n",
      "Epoch 96 | Total train loss: 3.754052860671095\n",
      "Epoch 96 | Val loss: 0.003030566032975912\n",
      "Epoch 97 | Total train loss: 3.900763752821831\n",
      "Epoch 97 | Val loss: 0.004860107786953449\n",
      "Epoch 98 | Total train loss: 3.0691450222708454\n",
      "Epoch 98 | Val loss: 0.0029733770061284304\n",
      "Epoch 99 | Total train loss: 3.6106933472899527\n",
      "Epoch 99 | Val loss: 0.002420442644506693\n",
      "Epoch 100 | Total train loss: 6.070443739514644\n",
      "Epoch 100 | Val loss: 0.003818633733317256\n",
      "Test loss: 0.0039957789704203606\n",
      "Model: xxxlarge | Training complete\n",
      "Model: xxxlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxxlarge | Plots made and saved\n",
      "Model: xxxlarge | Predictions saved\n",
      "Model: xxxlarge | Model saved\n",
      "Model: xxxlarge | Garbage collected, states deleted\n",
      "Model: sub_enormous | Started loop\n",
      "Model: sub_enormous | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 54.97635224891201\n",
      "Epoch 1 | Val loss: 0.041332826018333435\n",
      "Epoch 2 | Total train loss: 39.681318180232665\n",
      "Epoch 2 | Val loss: 0.05026799812912941\n",
      "Epoch 3 | Total train loss: 32.371545310069905\n",
      "Epoch 3 | Val loss: 0.03045998327434063\n",
      "Epoch 4 | Total train loss: 27.990629688652916\n",
      "Epoch 4 | Val loss: 0.03282997012138367\n",
      "Epoch 5 | Total train loss: 25.28084880158167\n",
      "Epoch 5 | Val loss: 0.029982173815369606\n",
      "Epoch 6 | Total train loss: 20.77460457388952\n",
      "Epoch 6 | Val loss: 0.032775480300188065\n",
      "Epoch 7 | Total train loss: 24.38483318505314\n",
      "Epoch 7 | Val loss: 0.030583852902054787\n",
      "Epoch 8 | Total train loss: 23.56270514778771\n",
      "Epoch 8 | Val loss: 0.022927893325686455\n",
      "Epoch 9 | Total train loss: 21.243228979712057\n",
      "Epoch 9 | Val loss: 0.02095155231654644\n",
      "Epoch 10 | Total train loss: 19.67911806945449\n",
      "Epoch 10 | Val loss: 0.027408072724938393\n",
      "Epoch 11 | Total train loss: 18.6794860122709\n",
      "Epoch 11 | Val loss: 0.01758131943643093\n",
      "Epoch 12 | Total train loss: 16.16765631797307\n",
      "Epoch 12 | Val loss: 0.015599477104842663\n",
      "Epoch 13 | Total train loss: 15.884141858246949\n",
      "Epoch 13 | Val loss: 0.01507105864584446\n",
      "Epoch 14 | Total train loss: 16.40881922826793\n",
      "Epoch 14 | Val loss: 0.02280118688941002\n",
      "Epoch 15 | Total train loss: 13.658971748322983\n",
      "Epoch 15 | Val loss: 0.012654312886297703\n",
      "Epoch 16 | Total train loss: 14.507296735347154\n",
      "Epoch 16 | Val loss: 0.013110315427184105\n",
      "Epoch 17 | Total train loss: 12.64391458873331\n",
      "Epoch 17 | Val loss: 0.007966190576553345\n",
      "Epoch 18 | Total train loss: 11.231459274527879\n",
      "Epoch 18 | Val loss: 0.011547963134944439\n",
      "Epoch 19 | Total train loss: 12.055748702309756\n",
      "Epoch 19 | Val loss: 0.007095280569046736\n",
      "Epoch 20 | Total train loss: 11.988393455785285\n",
      "Epoch 20 | Val loss: 0.016277749091386795\n",
      "Epoch 21 | Total train loss: 9.970046585980526\n",
      "Epoch 21 | Val loss: 0.008732707239687443\n",
      "Epoch 22 | Total train loss: 11.01072781910807\n",
      "Epoch 22 | Val loss: 0.006030804477632046\n",
      "Epoch 23 | Total train loss: 10.25380272830455\n",
      "Epoch 23 | Val loss: 0.010543200187385082\n",
      "Epoch 24 | Total train loss: 11.707763193452138\n",
      "Epoch 24 | Val loss: 0.010121176950633526\n",
      "Epoch 25 | Total train loss: 11.062450239127429\n",
      "Epoch 25 | Val loss: 0.009260977618396282\n",
      "Epoch 26 | Total train loss: 9.312580635065387\n",
      "Epoch 26 | Val loss: 0.009122234769165516\n",
      "Epoch 27 | Total train loss: 10.238337767272697\n",
      "Epoch 27 | Val loss: 0.007250196300446987\n",
      "Epoch 28 | Total train loss: 9.375329043342163\n",
      "Epoch 28 | Val loss: 0.00612560985609889\n",
      "Epoch 29 | Total train loss: 6.975693613794647\n",
      "Epoch 29 | Val loss: 0.009228122420608997\n",
      "Epoch 30 | Total train loss: 10.626755062367351\n",
      "Epoch 30 | Val loss: 0.011598042212426662\n",
      "Epoch 31 | Total train loss: 9.132758375765661\n",
      "Epoch 31 | Val loss: 0.011970080435276031\n",
      "Epoch 32 | Total train loss: 8.57036226873447\n",
      "Epoch 32 | Val loss: 0.007784160319715738\n",
      "Epoch 33 | Total train loss: 9.015499574632965\n",
      "Epoch 33 | Val loss: 0.01159531157463789\n",
      "Epoch 34 | Total train loss: 8.013216719746197\n",
      "Epoch 34 | Val loss: 0.009338253177702427\n",
      "Epoch 35 | Total train loss: 9.604637773690229\n",
      "Epoch 35 | Val loss: 0.00731484591960907\n",
      "Epoch 36 | Total train loss: 9.087402931257202\n",
      "Epoch 36 | Val loss: 0.008500348776578903\n",
      "Epoch 37 | Total train loss: 9.720076293103375\n",
      "Epoch 37 | Val loss: 0.007504142355173826\n",
      "Epoch 38 | Total train loss: 8.12836120951124\n",
      "Epoch 38 | Val loss: 0.008167226798832417\n",
      "Epoch 39 | Total train loss: 7.56224570379004\n",
      "Epoch 39 | Val loss: 0.00832271110266447\n",
      "Epoch 40 | Total train loss: 8.20380360561012\n",
      "Epoch 40 | Val loss: 0.009832832030951977\n",
      "Epoch 41 | Total train loss: 8.609114282327937\n",
      "Epoch 41 | Val loss: 0.007405298762023449\n",
      "Epoch 42 | Total train loss: 7.835721948364835\n",
      "Epoch 42 | Val loss: 0.010947064496576786\n",
      "Epoch 43 | Total train loss: 7.244443684224734\n",
      "Epoch 43 | Val loss: 0.009975139982998371\n",
      "Epoch 44 | Total train loss: 6.3431710613933205\n",
      "Epoch 44 | Val loss: 0.0041984813287854195\n",
      "Epoch 45 | Total train loss: 7.64094923566438\n",
      "Epoch 45 | Val loss: 0.007258159574121237\n",
      "Epoch 46 | Total train loss: 9.23888295956749\n",
      "Epoch 46 | Val loss: 0.013674134388566017\n",
      "Epoch 47 | Total train loss: 7.984873764974395\n",
      "Epoch 47 | Val loss: 0.006175253074616194\n",
      "Epoch 48 | Total train loss: 6.282450054320577\n",
      "Epoch 48 | Val loss: 0.008609401993453503\n",
      "Epoch 49 | Total train loss: 6.499126588482341\n",
      "Epoch 49 | Val loss: 0.0061172861605882645\n",
      "Epoch 50 | Total train loss: 6.470260514180154\n",
      "Epoch 50 | Val loss: 0.005647450219839811\n",
      "Epoch 51 | Total train loss: 7.792590489611939\n",
      "Epoch 51 | Val loss: 0.005147943738847971\n",
      "Epoch 52 | Total train loss: 4.877615867703071\n",
      "Epoch 52 | Val loss: 0.0050833625718951225\n",
      "Epoch 53 | Total train loss: 6.218116545714565\n",
      "Epoch 53 | Val loss: 0.0037227433640509844\n",
      "Epoch 54 | Total train loss: 6.201681665480237\n",
      "Epoch 54 | Val loss: 0.004466918297111988\n",
      "Epoch 55 | Total train loss: 8.773527484141596\n",
      "Epoch 55 | Val loss: 0.005566946696490049\n",
      "Epoch 56 | Total train loss: 5.8012979075645035\n",
      "Epoch 56 | Val loss: 0.008862916380167007\n",
      "Epoch 57 | Total train loss: 5.827562470613202\n",
      "Epoch 57 | Val loss: 0.004202624782919884\n",
      "Epoch 58 | Total train loss: 5.021905131042331\n",
      "Epoch 58 | Val loss: 0.005770753137767315\n",
      "Epoch 59 | Total train loss: 5.967008951419075\n",
      "Epoch 59 | Val loss: 0.006462263409048319\n",
      "Epoch 60 | Total train loss: 5.2686566610905174\n",
      "Epoch 60 | Val loss: 0.0044878460466861725\n",
      "Epoch 61 | Total train loss: 5.886002558111159\n",
      "Epoch 61 | Val loss: 0.007912730798125267\n",
      "Epoch 62 | Total train loss: 5.997158687190677\n",
      "Epoch 62 | Val loss: 0.015011339448392391\n",
      "Epoch 63 | Total train loss: 5.3829646201354535\n",
      "Epoch 63 | Val loss: 0.0055999066680669785\n",
      "Epoch 64 | Total train loss: 5.521479228379633\n",
      "Epoch 64 | Val loss: 0.0054917470552027225\n",
      "Epoch 65 | Total train loss: 5.715676262073657\n",
      "Epoch 65 | Val loss: 0.00890318863093853\n",
      "Epoch 66 | Total train loss: 5.867427501092152\n",
      "Epoch 66 | Val loss: 0.005285028833895922\n",
      "Epoch 67 | Total train loss: 5.092111246989134\n",
      "Epoch 67 | Val loss: 0.005628139246255159\n",
      "Epoch 68 | Total train loss: 4.742613947022392\n",
      "Epoch 68 | Val loss: 0.0036605268251150846\n",
      "Epoch 69 | Total train loss: 5.461503573264167\n",
      "Epoch 69 | Val loss: 0.0050119138322770596\n",
      "Epoch 70 | Total train loss: 5.273693242261459\n",
      "Epoch 70 | Val loss: 0.004859864711761475\n",
      "Epoch 71 | Total train loss: 4.583853927899781\n",
      "Epoch 71 | Val loss: 0.0027018229011446238\n",
      "Epoch 72 | Total train loss: 5.065333134838511\n",
      "Epoch 72 | Val loss: 0.0038629898335784674\n",
      "Epoch 73 | Total train loss: 3.885850190438532\n",
      "Epoch 73 | Val loss: 0.0048751989379525185\n",
      "Epoch 74 | Total train loss: 5.931052288945921\n",
      "Epoch 74 | Val loss: 0.0052649821154773235\n",
      "Epoch 75 | Total train loss: 5.693275861868187\n",
      "Epoch 75 | Val loss: 0.00813224446028471\n",
      "Epoch 76 | Total train loss: 5.068163754328225\n",
      "Epoch 76 | Val loss: 0.007525709457695484\n",
      "Epoch 77 | Total train loss: 4.631878928172455\n",
      "Epoch 77 | Val loss: 0.006169766653329134\n",
      "Epoch 78 | Total train loss: 4.2472579581684045\n",
      "Epoch 78 | Val loss: 0.005532391369342804\n",
      "Epoch 79 | Total train loss: 5.162910858231498\n",
      "Epoch 79 | Val loss: 0.004961250815540552\n",
      "Epoch 80 | Total train loss: 4.578038209706847\n",
      "Epoch 80 | Val loss: 0.005181888118386269\n",
      "Epoch 81 | Total train loss: 5.295390145018018\n",
      "Epoch 81 | Val loss: 0.003299740143120289\n",
      "Epoch 82 | Total train loss: 4.840357927421451\n",
      "Epoch 82 | Val loss: 0.005022950936108828\n",
      "Epoch 83 | Total train loss: 4.7427896294775564\n",
      "Epoch 83 | Val loss: 0.003129938617348671\n",
      "Epoch 84 | Total train loss: 4.737732848208793\n",
      "Epoch 84 | Val loss: 0.004076078999787569\n",
      "Epoch 85 | Total train loss: 4.449067031309255\n",
      "Epoch 85 | Val loss: 0.0036832254845649004\n",
      "Epoch 86 | Total train loss: 4.3974230910896495\n",
      "Epoch 86 | Val loss: 0.003379673231393099\n",
      "Epoch 87 | Total train loss: 5.747619434791773\n",
      "Epoch 87 | Val loss: 0.0025141765363514423\n",
      "Epoch 88 | Total train loss: 4.5614752001189345\n",
      "Epoch 88 | Val loss: 0.002756713191047311\n",
      "Epoch 89 | Total train loss: 4.650975875721883\n",
      "Epoch 89 | Val loss: 0.008823331445455551\n",
      "Epoch 90 | Total train loss: 3.1101918932815806\n",
      "Epoch 90 | Val loss: 0.008457168005406857\n",
      "Epoch 91 | Total train loss: 5.083292865718704\n",
      "Epoch 91 | Val loss: 0.006460562814027071\n",
      "Epoch 92 | Total train loss: 5.818211944888617\n",
      "Epoch 92 | Val loss: 0.00458553759381175\n",
      "Epoch 93 | Total train loss: 3.572347412163822\n",
      "Epoch 93 | Val loss: 0.002963858423754573\n",
      "Epoch 94 | Total train loss: 3.8870627460461264\n",
      "Epoch 94 | Val loss: 0.004420910496264696\n",
      "Epoch 95 | Total train loss: 4.569698183305917\n",
      "Epoch 95 | Val loss: 0.0031921204645186663\n",
      "Epoch 96 | Total train loss: 4.043514988862626\n",
      "Epoch 96 | Val loss: 0.0038401393685489893\n",
      "Epoch 97 | Total train loss: 3.7974715282779243\n",
      "Epoch 97 | Val loss: 0.004971977788954973\n",
      "Epoch 98 | Total train loss: 4.537416578235593\n",
      "Epoch 98 | Val loss: 0.002975826384499669\n",
      "Epoch 99 | Total train loss: 3.8214884938711293\n",
      "Epoch 99 | Val loss: 0.0038880810607224703\n",
      "Epoch 100 | Total train loss: 4.168152407046364\n",
      "Epoch 100 | Val loss: 0.002137750154361129\n",
      "Test loss: 0.004709834698587656\n",
      "Model: sub_enormous | Training complete\n",
      "Model: sub_enormous | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: sub_enormous | Plots made and saved\n",
      "Model: sub_enormous | Predictions saved\n",
      "Model: sub_enormous | Model saved\n",
      "Model: sub_enormous | Garbage collected, states deleted\n",
      "Model: enormous | Started loop\n",
      "Model: enormous | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 60.70812140451926\n",
      "Epoch 1 | Val loss: 0.034555159509181976\n",
      "Epoch 2 | Total train loss: 62.11992511581775\n",
      "Epoch 2 | Val loss: 0.03592969849705696\n",
      "Epoch 3 | Total train loss: 38.533387363866495\n",
      "Epoch 3 | Val loss: 0.033464495092630386\n",
      "Epoch 4 | Total train loss: 32.28220675587727\n",
      "Epoch 4 | Val loss: 0.030917854979634285\n",
      "Epoch 5 | Total train loss: 29.281648929096264\n",
      "Epoch 5 | Val loss: 0.024697810411453247\n",
      "Epoch 6 | Total train loss: 26.273337796481428\n",
      "Epoch 6 | Val loss: 0.036389365792274475\n",
      "Epoch 7 | Total train loss: 31.229138292231255\n",
      "Epoch 7 | Val loss: 0.024119963869452477\n",
      "Epoch 8 | Total train loss: 33.2534357459499\n",
      "Epoch 8 | Val loss: 0.03431972116231918\n",
      "Epoch 9 | Total train loss: 30.175631294094273\n",
      "Epoch 9 | Val loss: 0.01698150299489498\n",
      "Epoch 10 | Total train loss: 37.7873599433733\n",
      "Epoch 10 | Val loss: 0.050684429705142975\n",
      "Epoch 11 | Total train loss: 24.937954860382888\n",
      "Epoch 11 | Val loss: 0.020809922367334366\n",
      "Epoch 12 | Total train loss: 25.023456882103346\n",
      "Epoch 12 | Val loss: 0.024986635893583298\n",
      "Epoch 13 | Total train loss: 23.269203822761483\n",
      "Epoch 13 | Val loss: 0.022802751511335373\n",
      "Epoch 14 | Total train loss: 21.90390232831487\n",
      "Epoch 14 | Val loss: 0.032354921102523804\n",
      "Epoch 15 | Total train loss: 23.468960775338928\n",
      "Epoch 15 | Val loss: 0.025240713730454445\n",
      "Epoch 16 | Total train loss: 17.99067717309299\n",
      "Epoch 16 | Val loss: 0.038993075489997864\n",
      "Epoch 17 | Total train loss: 37.160944418992585\n",
      "Epoch 17 | Val loss: 0.015022526495158672\n",
      "Epoch 18 | Total train loss: 14.783130314263417\n",
      "Epoch 18 | Val loss: 0.01641242951154709\n",
      "Epoch 19 | Total train loss: 18.62850749759923\n",
      "Epoch 19 | Val loss: 0.02089916169643402\n",
      "Epoch 20 | Total train loss: 15.080919807083774\n",
      "Epoch 20 | Val loss: 0.012121833860874176\n",
      "Epoch 21 | Total train loss: 14.81502926018311\n",
      "Epoch 21 | Val loss: 0.013611309230327606\n",
      "Epoch 22 | Total train loss: 13.802010431608323\n",
      "Epoch 22 | Val loss: 0.017118146643042564\n",
      "Epoch 23 | Total train loss: 13.008676378435212\n",
      "Epoch 23 | Val loss: 0.009123846888542175\n",
      "Epoch 24 | Total train loss: 14.095332601149494\n",
      "Epoch 24 | Val loss: 0.01447982620447874\n",
      "Epoch 25 | Total train loss: 12.433586495691543\n",
      "Epoch 25 | Val loss: 0.008897739462554455\n",
      "Epoch 26 | Total train loss: 15.690005055859729\n",
      "Epoch 26 | Val loss: 0.013406672514975071\n",
      "Epoch 27 | Total train loss: 12.448154029584657\n",
      "Epoch 27 | Val loss: 0.019550172612071037\n",
      "Epoch 28 | Total train loss: 12.511698130547302\n",
      "Epoch 28 | Val loss: 0.008412272669374943\n",
      "Epoch 29 | Total train loss: 12.340014032427462\n",
      "Epoch 29 | Val loss: 0.018646324053406715\n",
      "Epoch 30 | Total train loss: 11.59999581244574\n",
      "Epoch 30 | Val loss: 0.012340741232037544\n",
      "Epoch 31 | Total train loss: 10.326119881524392\n",
      "Epoch 31 | Val loss: 0.008836734108626842\n",
      "Epoch 32 | Total train loss: 9.238410240826624\n",
      "Epoch 32 | Val loss: 0.014216296374797821\n",
      "Epoch 33 | Total train loss: 10.997773102305473\n",
      "Epoch 33 | Val loss: 0.013364016078412533\n",
      "Epoch 34 | Total train loss: 12.024898205268073\n",
      "Epoch 34 | Val loss: 0.012898461893200874\n",
      "Epoch 35 | Total train loss: 9.352813424322449\n",
      "Epoch 35 | Val loss: 0.012072823941707611\n",
      "Epoch 36 | Total train loss: 10.357939279948823\n",
      "Epoch 36 | Val loss: 0.010364484041929245\n",
      "Epoch 37 | Total train loss: 10.960055336827736\n",
      "Epoch 37 | Val loss: 0.022788802161812782\n",
      "Epoch 38 | Total train loss: 10.09420570542079\n",
      "Epoch 38 | Val loss: 0.007181141525506973\n",
      "Epoch 39 | Total train loss: 9.983254971633869\n",
      "Epoch 39 | Val loss: 0.006783240009099245\n",
      "Epoch 40 | Total train loss: 9.575221268891255\n",
      "Epoch 40 | Val loss: 0.11618561297655106\n",
      "Epoch 41 | Total train loss: 17.257560520994048\n",
      "Epoch 41 | Val loss: 0.005877039860934019\n",
      "Epoch 42 | Total train loss: 43.04617822521186\n",
      "Epoch 42 | Val loss: 0.008896163664758205\n",
      "Epoch 43 | Total train loss: 8.448569510877633\n",
      "Epoch 43 | Val loss: 0.010600319132208824\n",
      "Epoch 44 | Total train loss: 6.644087301926902\n",
      "Epoch 44 | Val loss: 0.006825529970228672\n",
      "Epoch 45 | Total train loss: 6.709873186363211\n",
      "Epoch 45 | Val loss: 0.0066314279101789\n",
      "Epoch 46 | Total train loss: 6.454776907728046\n",
      "Epoch 46 | Val loss: 0.00874366331845522\n",
      "Epoch 47 | Total train loss: 7.682364478249269\n",
      "Epoch 47 | Val loss: 0.007563060149550438\n",
      "Epoch 48 | Total train loss: 8.387308141700373\n",
      "Epoch 48 | Val loss: 0.004587350878864527\n",
      "Epoch 49 | Total train loss: 9.373761796136932\n",
      "Epoch 49 | Val loss: 0.008841120637953281\n",
      "Epoch 50 | Total train loss: 8.19455201118626\n",
      "Epoch 50 | Val loss: 0.006863635499030352\n",
      "Epoch 51 | Total train loss: 8.070614776995171\n",
      "Epoch 51 | Val loss: 0.007555163931101561\n",
      "Epoch 52 | Total train loss: 10.227127422385593\n",
      "Epoch 52 | Val loss: 0.009163939394056797\n",
      "Epoch 53 | Total train loss: 8.287516332235555\n",
      "Epoch 53 | Val loss: 0.008522159419953823\n",
      "Epoch 54 | Total train loss: 7.969686065118502\n",
      "Epoch 54 | Val loss: 0.01368919387459755\n",
      "Epoch 55 | Total train loss: 8.386207133350354\n",
      "Epoch 55 | Val loss: 0.00848759151995182\n",
      "Epoch 56 | Total train loss: 7.934341603204302\n",
      "Epoch 56 | Val loss: 0.01605924777686596\n",
      "Epoch 57 | Total train loss: 8.740872165418011\n",
      "Epoch 57 | Val loss: 0.007107157725840807\n",
      "Epoch 58 | Total train loss: 8.578408979650703\n",
      "Epoch 58 | Val loss: 0.03666650131344795\n",
      "Epoch 59 | Total train loss: 7.768536356765026\n",
      "Epoch 59 | Val loss: 0.012621860951185226\n",
      "Epoch 60 | Total train loss: 7.715985646947956\n",
      "Epoch 60 | Val loss: 0.007468695752322674\n",
      "Epoch 61 | Total train loss: 8.921946010789725\n",
      "Epoch 61 | Val loss: 0.008395243436098099\n",
      "Epoch 62 | Total train loss: 6.573249369246241\n",
      "Epoch 62 | Val loss: 0.006223553325980902\n",
      "Epoch 63 | Total train loss: 6.6118070164104665\n",
      "Epoch 63 | Val loss: 0.008623973466455936\n",
      "Epoch 64 | Total train loss: 7.7400266846423165\n",
      "Epoch 64 | Val loss: 0.007188004907220602\n",
      "Epoch 65 | Total train loss: 7.706144030461019\n",
      "Epoch 65 | Val loss: 0.008852384984493256\n",
      "Epoch 66 | Total train loss: 9.072995840723252\n",
      "Epoch 66 | Val loss: 0.020377179607748985\n",
      "Epoch 67 | Total train loss: 6.741390520518962\n",
      "Epoch 67 | Val loss: 0.010009624063968658\n",
      "Epoch 68 | Total train loss: 7.473090583804151\n",
      "Epoch 68 | Val loss: 0.007168744690716267\n",
      "Epoch 69 | Total train loss: 6.150253345273995\n",
      "Epoch 69 | Val loss: 0.009369978681206703\n",
      "Epoch 70 | Total train loss: 6.576277313929495\n",
      "Epoch 70 | Val loss: 0.008450970984995365\n",
      "Epoch 71 | Total train loss: 8.205054446578288\n",
      "Epoch 71 | Val loss: 0.005772862583398819\n",
      "Epoch 72 | Total train loss: 5.771176613027137\n",
      "Epoch 72 | Val loss: 0.007096559274941683\n",
      "Epoch 73 | Total train loss: 7.640278695736015\n",
      "Epoch 73 | Val loss: 0.0052870092913508415\n",
      "Epoch 74 | Total train loss: 58.1329202245588\n",
      "Epoch 74 | Val loss: 0.008347363211214542\n",
      "Epoch 75 | Total train loss: 5.627796831347041\n",
      "Epoch 75 | Val loss: 0.004391541238874197\n",
      "Epoch 76 | Total train loss: 7.775021613217177\n",
      "Epoch 76 | Val loss: 0.004914573859423399\n",
      "Epoch 77 | Total train loss: 5.384600960878856\n",
      "Epoch 77 | Val loss: 0.006438533775508404\n",
      "Epoch 78 | Total train loss: 5.530703790773771\n",
      "Epoch 78 | Val loss: 0.005686458665877581\n",
      "Epoch 79 | Total train loss: 5.236864032123549\n",
      "Epoch 79 | Val loss: 0.006476413458585739\n",
      "Epoch 80 | Total train loss: 899.6173601840146\n",
      "Epoch 80 | Val loss: 0.0062425932846963406\n",
      "Epoch 81 | Total train loss: 5.540634664455638\n",
      "Epoch 81 | Val loss: 0.00491121644154191\n",
      "Epoch 82 | Total train loss: 4.82404850379443\n",
      "Epoch 82 | Val loss: 0.004455301910638809\n",
      "Epoch 83 | Total train loss: 12825.998497588917\n",
      "Epoch 83 | Val loss: 0.004527747631072998\n",
      "Epoch 84 | Total train loss: 3.5126114794253454\n",
      "Epoch 84 | Val loss: 0.0024221378844231367\n",
      "Epoch 85 | Total train loss: 4.280251055143822\n",
      "Epoch 85 | Val loss: 0.00396697549149394\n",
      "Epoch 86 | Total train loss: 3.5259278119810915\n",
      "Epoch 86 | Val loss: 0.004524874500930309\n",
      "Epoch 87 | Total train loss: 4.504508930692964\n",
      "Epoch 87 | Val loss: 0.002632672432810068\n",
      "Epoch 88 | Total train loss: 5.132231343936837\n",
      "Epoch 88 | Val loss: 0.005244310013949871\n",
      "Epoch 89 | Total train loss: 4.540111330730042\n",
      "Epoch 89 | Val loss: 0.0026926768478006124\n",
      "Epoch 90 | Total train loss: 5.0285355569482135\n",
      "Epoch 90 | Val loss: 0.003086274256929755\n",
      "Epoch 91 | Total train loss: 3.403963514942461\n",
      "Epoch 91 | Val loss: 0.007101739291101694\n",
      "Epoch 92 | Total train loss: 5.286203857870078\n",
      "Epoch 92 | Val loss: 0.004732185043394566\n",
      "Epoch 93 | Total train loss: 4.409973900058162\n",
      "Epoch 93 | Val loss: 0.006872463971376419\n",
      "Epoch 94 | Total train loss: 5.00932873874433\n",
      "Epoch 94 | Val loss: 0.005680210888385773\n",
      "Epoch 95 | Total train loss: 4.588196202030076\n",
      "Epoch 95 | Val loss: 0.003323043929412961\n",
      "Epoch 96 | Total train loss: 5.288328427698161\n",
      "Epoch 96 | Val loss: 0.002950329566374421\n",
      "Epoch 97 | Total train loss: 4.675517553503937\n",
      "Epoch 97 | Val loss: 0.005041263997554779\n",
      "Epoch 98 | Total train loss: 4.049243343061448\n",
      "Epoch 98 | Val loss: 0.004099638666957617\n",
      "Epoch 99 | Total train loss: 5.885909202851735\n",
      "Epoch 99 | Val loss: 0.003271948080509901\n",
      "Epoch 100 | Total train loss: 5.3724348356106475\n",
      "Epoch 100 | Val loss: 0.0032867141999304295\n",
      "Test loss: 0.004158948082476854\n",
      "Model: enormous | Training complete\n",
      "Model: enormous | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: enormous | Plots made and saved\n",
      "Model: enormous | Predictions saved\n",
      "Model: enormous | Model saved\n",
      "Model: enormous | Garbage collected, states deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and store all the MLP models\n",
    "\n",
    "for i in exploration_dict['cfg_models']['cfg_mlps'].keys():\n",
    "    \n",
    "    print(f\"Model: {i} | Started loop\")\n",
    "    model = MLP(cfg_mlp=exploration_dict['cfg_models']['cfg_mlps'][i])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    print(f\"Model: {i} | Initialization done for model and optimizer\")\n",
    "\n",
    "    collector_dict, model = train(cfg_train=cfg_train, model=model, optimizer=optimizer, loss_fn=loss_fn, dataloaders=dataloaders)\n",
    "    print(f\"Model: {i} | Training complete\")\n",
    "\n",
    "    # Calculate and export all the R^2 and MAE results\n",
    "    r2 = r2_score(collector_dict['actual']['test'].cpu(), collector_dict['preds']['test'].cpu(), multioutput='raw_values')\n",
    "    mae = mean_absolute_error(collector_dict['actual']['test'].cpu(), collector_dict['preds']['test'].cpu(), multioutput='raw_values')\n",
    "    exploration_dict['perf_models']['r2s']['mlp'][i] = r2\n",
    "    exploration_dict['perf_models']['maes']['mlp'][i] = mae\n",
    "\n",
    "    #Get the flops of the model, save them\n",
    "    show_tensor_1 = torch.ones(1, 20).to(device)\n",
    "    show_tensor_2 = torch.ones(1, 3).to(device)\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        model(show_tensor_1, show_tensor_2)\n",
    "    flops_per_sample = flop_counter.get_total_flops()\n",
    "    exploration_dict['perf_models']['flops']['mlp'][i] = flops_per_sample\n",
    "\n",
    "    # Save the param amount\n",
    "    exploration_dict['perf_models']['params']['mlp'][i] = get_n_params(model=model)\n",
    "    print(f\"Model: {i} | Metrics saved: R^2, MAE, Flops, Param#\")\n",
    "\n",
    "    # Export the graphs necessary\n",
    "    all_plots_ex_analysis(model_name=i, model_type='mlp', collector_dict=collector_dict, index=index)\n",
    "    print(f\"Model: {i} | Plots made and saved\")\n",
    "\n",
    "    # Export the predictions for future reference\n",
    "    exploration_dict['perf_models']['preds']['mlp'][i] = collector_dict['preds']['test']\n",
    "    print(f\"Model: {i} | Predictions saved\")\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = rf\"trained_models\\mlp\\{i}\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model: {i} | Model saved\")\n",
    "\n",
    "    # Delete unnecessary stuff, free up mem\n",
    "    del collector_dict\n",
    "    del model\n",
    "    del optimizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Model: {i} | Garbage collected, states deleted\")\n",
    "\n",
    "    # Now we are ready for the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849a7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('exploration_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(exploration_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144e2e6",
   "metadata": {},
   "source": [
    "# PointNIF Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb07f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: xxsmall_xsmall | Started loop\n",
      "Model: xxsmall_xsmall | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 166.9293605924886\n",
      "Epoch 1 | Val loss: 0.08314397931098938\n",
      "Epoch 2 | Total train loss: 66.71018017265851\n",
      "Epoch 2 | Val loss: 0.06249658763408661\n",
      "Epoch 3 | Total train loss: 56.05375169841909\n",
      "Epoch 3 | Val loss: 0.05712618678808212\n",
      "Epoch 4 | Total train loss: 43.22379517216905\n",
      "Epoch 4 | Val loss: 0.04093581438064575\n",
      "Epoch 5 | Total train loss: 41.222005982142946\n",
      "Epoch 5 | Val loss: 0.03509506955742836\n",
      "Epoch 6 | Total train loss: 38.696511925663344\n",
      "Epoch 6 | Val loss: 0.039484236389398575\n",
      "Epoch 7 | Total train loss: 40.32471662902208\n",
      "Epoch 7 | Val loss: 0.047474779188632965\n",
      "Epoch 8 | Total train loss: 37.72624534962051\n",
      "Epoch 8 | Val loss: 0.027198340743780136\n",
      "Epoch 9 | Total train loss: 32.07437386317906\n",
      "Epoch 9 | Val loss: 0.03052794188261032\n",
      "Epoch 10 | Total train loss: 32.989973875519354\n",
      "Epoch 10 | Val loss: 0.02601952850818634\n",
      "Epoch 11 | Total train loss: 27.770353065487143\n",
      "Epoch 11 | Val loss: 0.02470252476632595\n",
      "Epoch 12 | Total train loss: 26.927226492125556\n",
      "Epoch 12 | Val loss: 0.02874622493982315\n",
      "Epoch 13 | Total train loss: 28.759272416931708\n",
      "Epoch 13 | Val loss: 0.04195995256304741\n",
      "Epoch 14 | Total train loss: 25.03683754308713\n",
      "Epoch 14 | Val loss: 0.016709808260202408\n",
      "Epoch 15 | Total train loss: 23.33147511575862\n",
      "Epoch 15 | Val loss: 0.016042344272136688\n",
      "Epoch 16 | Total train loss: 21.87580907273059\n",
      "Epoch 16 | Val loss: 0.020408567041158676\n",
      "Epoch 17 | Total train loss: 18.441134840100858\n",
      "Epoch 17 | Val loss: 0.014101711101830006\n",
      "Epoch 18 | Total train loss: 19.015571285486658\n",
      "Epoch 18 | Val loss: 0.02192205749452114\n",
      "Epoch 19 | Total train loss: 19.204583610156988\n",
      "Epoch 19 | Val loss: 0.013401184231042862\n",
      "Epoch 20 | Total train loss: 15.582997031022387\n",
      "Epoch 20 | Val loss: 0.01442445907741785\n",
      "Epoch 21 | Total train loss: 17.982356391634312\n",
      "Epoch 21 | Val loss: 0.013021429069340229\n",
      "Epoch 22 | Total train loss: 14.692393971954516\n",
      "Epoch 22 | Val loss: 0.01770453155040741\n",
      "Epoch 23 | Total train loss: 14.23205006671742\n",
      "Epoch 23 | Val loss: 0.019585471600294113\n",
      "Epoch 24 | Total train loss: 13.740342507879177\n",
      "Epoch 24 | Val loss: 0.016474872827529907\n",
      "Epoch 25 | Total train loss: 13.161497293701359\n",
      "Epoch 25 | Val loss: 0.010652701370418072\n",
      "Epoch 26 | Total train loss: 14.199053359716117\n",
      "Epoch 26 | Val loss: 0.01730436086654663\n",
      "Epoch 27 | Total train loss: 11.369740713893407\n",
      "Epoch 27 | Val loss: 0.017160046845674515\n",
      "Epoch 28 | Total train loss: 11.82389965795619\n",
      "Epoch 28 | Val loss: 0.01175607182085514\n",
      "Epoch 29 | Total train loss: 12.061395026467608\n",
      "Epoch 29 | Val loss: 0.010269124060869217\n",
      "Epoch 30 | Total train loss: 12.33635966237307\n",
      "Epoch 30 | Val loss: 0.008681463077664375\n",
      "Epoch 31 | Total train loss: 9.440269421868834\n",
      "Epoch 31 | Val loss: 0.013593302108347416\n",
      "Epoch 32 | Total train loss: 8.954234477892896\n",
      "Epoch 32 | Val loss: 0.007418171968311071\n",
      "Epoch 33 | Total train loss: 9.145235684914041\n",
      "Epoch 33 | Val loss: 0.008042112924158573\n",
      "Epoch 34 | Total train loss: 8.301978521626552\n",
      "Epoch 34 | Val loss: 0.010861069895327091\n",
      "Epoch 35 | Total train loss: 8.352265188286765\n",
      "Epoch 35 | Val loss: 0.007437323220074177\n",
      "Epoch 36 | Total train loss: 9.475443610670482\n",
      "Epoch 36 | Val loss: 0.008500643074512482\n",
      "Epoch 37 | Total train loss: 7.998736016624207\n",
      "Epoch 37 | Val loss: 0.008740881457924843\n",
      "Epoch 38 | Total train loss: 8.038150844968413\n",
      "Epoch 38 | Val loss: 0.009000848978757858\n",
      "Epoch 39 | Total train loss: 8.152214038221246\n",
      "Epoch 39 | Val loss: 0.008950481191277504\n",
      "Epoch 40 | Total train loss: 8.373136313492978\n",
      "Epoch 40 | Val loss: 0.007308023050427437\n",
      "Epoch 41 | Total train loss: 8.351053756826332\n",
      "Epoch 41 | Val loss: 0.0070661441422998905\n",
      "Epoch 42 | Total train loss: 7.878070774599109\n",
      "Epoch 42 | Val loss: 0.006173107773065567\n",
      "Epoch 43 | Total train loss: 7.305137996774647\n",
      "Epoch 43 | Val loss: 0.005045254249125719\n",
      "Epoch 44 | Total train loss: 6.1715327962224364\n",
      "Epoch 44 | Val loss: 0.009514996781945229\n",
      "Epoch 45 | Total train loss: 7.941272024482259\n",
      "Epoch 45 | Val loss: 0.0071107167750597\n",
      "Epoch 46 | Total train loss: 7.397695126860526\n",
      "Epoch 46 | Val loss: 0.007534671109169722\n",
      "Epoch 47 | Total train loss: 8.15649280782418\n",
      "Epoch 47 | Val loss: 0.008522894233465195\n",
      "Epoch 48 | Total train loss: 7.199438790684326\n",
      "Epoch 48 | Val loss: 0.009705610573291779\n",
      "Epoch 49 | Total train loss: 6.9129256911105585\n",
      "Epoch 49 | Val loss: 0.009263720363378525\n",
      "Epoch 50 | Total train loss: 6.406705035725622\n",
      "Epoch 50 | Val loss: 0.004343118984252214\n",
      "Epoch 51 | Total train loss: 7.324320296632777\n",
      "Epoch 51 | Val loss: 0.00443253992125392\n",
      "Epoch 52 | Total train loss: 5.110518653175859\n",
      "Epoch 52 | Val loss: 0.00977586954832077\n",
      "Epoch 53 | Total train loss: 6.253956663962072\n",
      "Epoch 53 | Val loss: 0.009315854869782925\n",
      "Epoch 54 | Total train loss: 7.197247545372875\n",
      "Epoch 54 | Val loss: 0.007769802585244179\n",
      "Epoch 55 | Total train loss: 5.59304968786455\n",
      "Epoch 55 | Val loss: 0.008334942162036896\n",
      "Epoch 56 | Total train loss: 5.994707513857065\n",
      "Epoch 56 | Val loss: 0.008264003321528435\n",
      "Epoch 57 | Total train loss: 6.563031569904979\n",
      "Epoch 57 | Val loss: 0.00667038606479764\n",
      "Epoch 58 | Total train loss: 6.515692944976763\n",
      "Epoch 58 | Val loss: 0.006753371562808752\n",
      "Epoch 59 | Total train loss: 6.8050118606554975\n",
      "Epoch 59 | Val loss: 0.004126421175897121\n",
      "Epoch 60 | Total train loss: 6.466980736573532\n",
      "Epoch 60 | Val loss: 0.0047113667242228985\n",
      "Epoch 61 | Total train loss: 6.534014797514374\n",
      "Epoch 61 | Val loss: 0.005257867742329836\n",
      "Epoch 62 | Total train loss: 6.340462850258518\n",
      "Epoch 62 | Val loss: 0.004532396327704191\n",
      "Epoch 63 | Total train loss: 5.97697455719981\n",
      "Epoch 63 | Val loss: 0.00514350738376379\n",
      "Epoch 64 | Total train loss: 6.550869327896862\n",
      "Epoch 64 | Val loss: 0.007543208077549934\n",
      "Epoch 65 | Total train loss: 5.585868045966663\n",
      "Epoch 65 | Val loss: 0.005565681494772434\n",
      "Epoch 66 | Total train loss: 5.4497041540385\n",
      "Epoch 66 | Val loss: 0.004686195403337479\n",
      "Epoch 67 | Total train loss: 4.923374541198541\n",
      "Epoch 67 | Val loss: 0.008602490648627281\n",
      "Epoch 68 | Total train loss: 6.0584377695568605\n",
      "Epoch 68 | Val loss: 0.008392474614083767\n",
      "Epoch 69 | Total train loss: 5.041164349408064\n",
      "Epoch 69 | Val loss: 0.0038395971059799194\n",
      "Epoch 70 | Total train loss: 5.594607785346511\n",
      "Epoch 70 | Val loss: 0.004667713772505522\n",
      "Epoch 71 | Total train loss: 4.645618495992949\n",
      "Epoch 71 | Val loss: 0.006422833539545536\n",
      "Epoch 72 | Total train loss: 5.109850883104059\n",
      "Epoch 72 | Val loss: 0.004139893222600222\n",
      "Epoch 73 | Total train loss: 4.776889212378592\n",
      "Epoch 73 | Val loss: 0.00750831700861454\n",
      "Epoch 74 | Total train loss: 4.758487369284012\n",
      "Epoch 74 | Val loss: 0.004390150774270296\n",
      "Epoch 75 | Total train loss: 4.731603590548502\n",
      "Epoch 75 | Val loss: 0.0046716900542378426\n",
      "Epoch 76 | Total train loss: 4.778467360581544\n",
      "Epoch 76 | Val loss: 0.004458364099264145\n",
      "Epoch 77 | Total train loss: 4.695441187548695\n",
      "Epoch 77 | Val loss: 0.006429010536521673\n",
      "Epoch 78 | Total train loss: 5.4634227254966845\n",
      "Epoch 78 | Val loss: 0.004413985181599855\n",
      "Epoch 79 | Total train loss: 4.607317679267908\n",
      "Epoch 79 | Val loss: 0.003513823729008436\n",
      "Epoch 80 | Total train loss: 5.219180484408753\n",
      "Epoch 80 | Val loss: 0.004238080698996782\n",
      "Epoch 81 | Total train loss: 3.7462287611074885\n",
      "Epoch 81 | Val loss: 0.00423180079087615\n",
      "Epoch 82 | Total train loss: 5.060906551290145\n",
      "Epoch 82 | Val loss: 0.005877612624317408\n",
      "Epoch 83 | Total train loss: 5.9133457186418354\n",
      "Epoch 83 | Val loss: 0.0045746383257210255\n",
      "Epoch 84 | Total train loss: 5.370286767256971\n",
      "Epoch 84 | Val loss: 0.003324618097394705\n",
      "Epoch 85 | Total train loss: 4.1426329522061\n",
      "Epoch 85 | Val loss: 0.00573807442560792\n",
      "Epoch 86 | Total train loss: 4.363098980013547\n",
      "Epoch 86 | Val loss: 0.004938420373946428\n",
      "Epoch 87 | Total train loss: 5.122368458873666\n",
      "Epoch 87 | Val loss: 0.004636506084352732\n",
      "Epoch 88 | Total train loss: 5.0326698090061655\n",
      "Epoch 88 | Val loss: 0.008523968048393726\n",
      "Epoch 89 | Total train loss: 3.380686749697361\n",
      "Epoch 89 | Val loss: 0.005334929563105106\n",
      "Epoch 90 | Total train loss: 5.125544200897565\n",
      "Epoch 90 | Val loss: 0.0037415986880660057\n",
      "Epoch 91 | Total train loss: 5.473490188320184\n",
      "Epoch 91 | Val loss: 0.005215686280280352\n",
      "Epoch 92 | Total train loss: 3.9185217936116317\n",
      "Epoch 92 | Val loss: 0.0041660284623503685\n",
      "Epoch 93 | Total train loss: 3.0708693786651793\n",
      "Epoch 93 | Val loss: 0.0033451328054070473\n",
      "Epoch 94 | Total train loss: 4.353380695752207\n",
      "Epoch 94 | Val loss: 0.003959807101637125\n",
      "Epoch 95 | Total train loss: 3.431268726431199\n",
      "Epoch 95 | Val loss: 0.0028291663620620966\n",
      "Epoch 96 | Total train loss: 4.435670380994338\n",
      "Epoch 96 | Val loss: 0.0034952466376125813\n",
      "Epoch 97 | Total train loss: 4.48224277613194\n",
      "Epoch 97 | Val loss: 0.005922561511397362\n",
      "Epoch 98 | Total train loss: 3.7015125554257224\n",
      "Epoch 98 | Val loss: 0.003548959270119667\n",
      "Epoch 99 | Total train loss: 3.891111920986873\n",
      "Epoch 99 | Val loss: 0.003289660206064582\n",
      "Epoch 100 | Total train loss: 4.355197537476897\n",
      "Epoch 100 | Val loss: 0.004290647804737091\n",
      "Test loss: 0.005042328964918852\n",
      "Model: xxsmall_xsmall | Training complete\n",
      "Model: xxsmall_xsmall | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxsmall_xsmall | Plots made and saved\n",
      "Model: xxsmall_xsmall | Predictions saved\n",
      "Model: xxsmall_xsmall | Model saved\n",
      "Model: xxsmall_xsmall | Garbage collected, states deleted\n",
      "Model: xsmall_small | Started loop\n",
      "Model: xsmall_small | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 262.4090555849034\n",
      "Epoch 1 | Val loss: 0.09968922287225723\n",
      "Epoch 2 | Total train loss: 73.4564594956646\n",
      "Epoch 2 | Val loss: 0.051138121634721756\n",
      "Epoch 3 | Total train loss: 53.031863073047134\n",
      "Epoch 3 | Val loss: 0.04848897084593773\n",
      "Epoch 4 | Total train loss: 48.88165631541051\n",
      "Epoch 4 | Val loss: 0.04112572595477104\n",
      "Epoch 5 | Total train loss: 43.40162906130263\n",
      "Epoch 5 | Val loss: 0.04548988863825798\n",
      "Epoch 6 | Total train loss: 41.37790751145167\n",
      "Epoch 6 | Val loss: 0.04424579069018364\n",
      "Epoch 7 | Total train loss: 36.62523734601564\n",
      "Epoch 7 | Val loss: 0.03336447477340698\n",
      "Epoch 8 | Total train loss: 38.08750678380056\n",
      "Epoch 8 | Val loss: 0.03381558135151863\n",
      "Epoch 9 | Total train loss: 31.430709916612614\n",
      "Epoch 9 | Val loss: 0.03540989011526108\n",
      "Epoch 10 | Total train loss: 28.32370484925559\n",
      "Epoch 10 | Val loss: 0.02314571663737297\n",
      "Epoch 11 | Total train loss: 30.781374562778183\n",
      "Epoch 11 | Val loss: 0.025222616270184517\n",
      "Epoch 12 | Total train loss: 29.555408660922694\n",
      "Epoch 12 | Val loss: 0.021476052701473236\n",
      "Epoch 13 | Total train loss: 24.868605730358468\n",
      "Epoch 13 | Val loss: 0.022593766450881958\n",
      "Epoch 14 | Total train loss: 23.21636161379365\n",
      "Epoch 14 | Val loss: 0.01755394972860813\n",
      "Epoch 15 | Total train loss: 20.678059712335198\n",
      "Epoch 15 | Val loss: 0.02028147131204605\n",
      "Epoch 16 | Total train loss: 21.57266313601758\n",
      "Epoch 16 | Val loss: 0.02110554277896881\n",
      "Epoch 17 | Total train loss: 18.76968036438666\n",
      "Epoch 17 | Val loss: 0.01704578287899494\n",
      "Epoch 18 | Total train loss: 16.802672862437703\n",
      "Epoch 18 | Val loss: 0.017734266817569733\n",
      "Epoch 19 | Total train loss: 15.644091819193818\n",
      "Epoch 19 | Val loss: 0.012878152541816235\n",
      "Epoch 20 | Total train loss: 12.971918348684312\n",
      "Epoch 20 | Val loss: 0.011230630800127983\n",
      "Epoch 21 | Total train loss: 11.475033790865837\n",
      "Epoch 21 | Val loss: 0.01260978914797306\n",
      "Epoch 22 | Total train loss: 11.481345099651207\n",
      "Epoch 22 | Val loss: 0.013994617387652397\n",
      "Epoch 23 | Total train loss: 10.64048338937073\n",
      "Epoch 23 | Val loss: 0.00888723786920309\n",
      "Epoch 24 | Total train loss: 10.209880059545412\n",
      "Epoch 24 | Val loss: 0.006492131855338812\n",
      "Epoch 25 | Total train loss: 9.895900464408442\n",
      "Epoch 25 | Val loss: 0.009097622707486153\n",
      "Epoch 26 | Total train loss: 9.644452973236639\n",
      "Epoch 26 | Val loss: 0.01342281699180603\n",
      "Epoch 27 | Total train loss: 9.236168915235567\n",
      "Epoch 27 | Val loss: 0.010275368578732014\n",
      "Epoch 28 | Total train loss: 9.669407148539904\n",
      "Epoch 28 | Val loss: 0.009820065461099148\n",
      "Epoch 29 | Total train loss: 7.8812810937988615\n",
      "Epoch 29 | Val loss: 0.007597917225211859\n",
      "Epoch 30 | Total train loss: 8.254109253649403\n",
      "Epoch 30 | Val loss: 0.009370601736009121\n",
      "Epoch 31 | Total train loss: 9.469368583956566\n",
      "Epoch 31 | Val loss: 0.010462448000907898\n",
      "Epoch 32 | Total train loss: 9.070847075107167\n",
      "Epoch 32 | Val loss: 0.009096508845686913\n",
      "Epoch 33 | Total train loss: 7.916815636292995\n",
      "Epoch 33 | Val loss: 0.007863352075219154\n",
      "Epoch 34 | Total train loss: 9.55995936018735\n",
      "Epoch 34 | Val loss: 0.009526056237518787\n",
      "Epoch 35 | Total train loss: 7.5893203887098935\n",
      "Epoch 35 | Val loss: 0.006929939612746239\n",
      "Epoch 36 | Total train loss: 7.779227957450985\n",
      "Epoch 36 | Val loss: 0.005815001670271158\n",
      "Epoch 37 | Total train loss: 8.190635890882959\n",
      "Epoch 37 | Val loss: 0.00617772014811635\n",
      "Epoch 38 | Total train loss: 7.291746633190428\n",
      "Epoch 38 | Val loss: 0.009415634907782078\n",
      "Epoch 39 | Total train loss: 9.729576293400442\n",
      "Epoch 39 | Val loss: 0.00663980096578598\n",
      "Epoch 40 | Total train loss: 7.2255347509366175\n",
      "Epoch 40 | Val loss: 0.007190931588411331\n",
      "Epoch 41 | Total train loss: 6.539964939411902\n",
      "Epoch 41 | Val loss: 0.006768433377146721\n",
      "Epoch 42 | Total train loss: 6.5602482032661555\n",
      "Epoch 42 | Val loss: 0.009694580920040607\n",
      "Epoch 43 | Total train loss: 6.0740826228720834\n",
      "Epoch 43 | Val loss: 0.007488541770726442\n",
      "Epoch 44 | Total train loss: 7.033285685289684\n",
      "Epoch 44 | Val loss: 0.00704096257686615\n",
      "Epoch 45 | Total train loss: 6.7495888034472955\n",
      "Epoch 45 | Val loss: 0.007832927629351616\n",
      "Epoch 46 | Total train loss: 7.917476019199398\n",
      "Epoch 46 | Val loss: 0.00714058568701148\n",
      "Epoch 47 | Total train loss: 6.994511789392618\n",
      "Epoch 47 | Val loss: 0.004137746524065733\n",
      "Epoch 48 | Total train loss: 6.859872400340464\n",
      "Epoch 48 | Val loss: 0.007009546272456646\n",
      "Epoch 49 | Total train loss: 7.093246879509479\n",
      "Epoch 49 | Val loss: 0.008999381214380264\n",
      "Epoch 50 | Total train loss: 6.986047539637866\n",
      "Epoch 50 | Val loss: 0.007899273186922073\n",
      "Epoch 51 | Total train loss: 5.746802921623157\n",
      "Epoch 51 | Val loss: 0.004459748975932598\n",
      "Epoch 52 | Total train loss: 6.189613524921242\n",
      "Epoch 52 | Val loss: 0.0066300868056714535\n",
      "Epoch 53 | Total train loss: 6.306235931167976\n",
      "Epoch 53 | Val loss: 0.007441274356096983\n",
      "Epoch 54 | Total train loss: 6.825187497797742\n",
      "Epoch 54 | Val loss: 0.008040858432650566\n",
      "Epoch 55 | Total train loss: 11.767274863572311\n",
      "Epoch 55 | Val loss: 0.005599974188953638\n",
      "Epoch 56 | Total train loss: 6.415759297017871\n",
      "Epoch 56 | Val loss: 0.006666253786534071\n",
      "Epoch 57 | Total train loss: 4.809737273138126\n",
      "Epoch 57 | Val loss: 0.007058541290462017\n",
      "Epoch 58 | Total train loss: 5.407721285763785\n",
      "Epoch 58 | Val loss: 0.006049692630767822\n",
      "Epoch 59 | Total train loss: 6.477972117047784\n",
      "Epoch 59 | Val loss: 0.005833329167217016\n",
      "Epoch 60 | Total train loss: 5.473836628470053\n",
      "Epoch 60 | Val loss: 0.0047889091074466705\n",
      "Epoch 61 | Total train loss: 6.286205591529779\n",
      "Epoch 61 | Val loss: 0.007028648164123297\n",
      "Epoch 62 | Total train loss: 6.354401809308456\n",
      "Epoch 62 | Val loss: 0.007827553898096085\n",
      "Epoch 63 | Total train loss: 6.069731232733091\n",
      "Epoch 63 | Val loss: 0.007760302629321814\n",
      "Epoch 64 | Total train loss: 6.072087458200144\n",
      "Epoch 64 | Val loss: 0.004275972954928875\n",
      "Epoch 65 | Total train loss: 5.390520620777124\n",
      "Epoch 65 | Val loss: 0.0037545235827565193\n",
      "Epoch 66 | Total train loss: 6.045138419277578\n",
      "Epoch 66 | Val loss: 0.00731124822050333\n",
      "Epoch 67 | Total train loss: 6.503061073054482\n",
      "Epoch 67 | Val loss: 0.003940615803003311\n",
      "Epoch 68 | Total train loss: 5.8897266852952725\n",
      "Epoch 68 | Val loss: 0.005202390253543854\n",
      "Epoch 69 | Total train loss: 5.504951040265105\n",
      "Epoch 69 | Val loss: 0.004581778775900602\n",
      "Epoch 70 | Total train loss: 5.786151188931058\n",
      "Epoch 70 | Val loss: 0.005678409244865179\n",
      "Epoch 71 | Total train loss: 5.309762743293049\n",
      "Epoch 71 | Val loss: 0.009183919988572598\n",
      "Epoch 72 | Total train loss: 5.6805216424952505\n",
      "Epoch 72 | Val loss: 0.007956051267683506\n",
      "Epoch 73 | Total train loss: 5.645924847367837\n",
      "Epoch 73 | Val loss: 0.00357731431722641\n",
      "Epoch 74 | Total train loss: 5.295592833530918\n",
      "Epoch 74 | Val loss: 0.003461242653429508\n",
      "Epoch 75 | Total train loss: 5.189548174011236\n",
      "Epoch 75 | Val loss: 0.00534102413803339\n",
      "Epoch 76 | Total train loss: 5.010558460232971\n",
      "Epoch 76 | Val loss: 0.0055655441246926785\n",
      "Epoch 77 | Total train loss: 4.318437547277085\n",
      "Epoch 77 | Val loss: 0.004894900601357222\n",
      "Epoch 78 | Total train loss: 6.3261530006324165\n",
      "Epoch 78 | Val loss: 0.004889765754342079\n",
      "Epoch 79 | Total train loss: 5.102015717413451\n",
      "Epoch 79 | Val loss: 0.00443759560585022\n",
      "Epoch 80 | Total train loss: 6.804873751493005\n",
      "Epoch 80 | Val loss: 0.0063008456490933895\n",
      "Epoch 81 | Total train loss: 4.166159302102926\n",
      "Epoch 81 | Val loss: 0.0045067486353218555\n",
      "Epoch 82 | Total train loss: 5.252626632235092\n",
      "Epoch 82 | Val loss: 0.005330814514309168\n",
      "Epoch 83 | Total train loss: 5.897719079787407\n",
      "Epoch 83 | Val loss: 0.00620634388178587\n",
      "Epoch 84 | Total train loss: 5.043669885071495\n",
      "Epoch 84 | Val loss: 0.004981059581041336\n",
      "Epoch 85 | Total train loss: 5.448819476655615\n",
      "Epoch 85 | Val loss: 0.0033335790503770113\n",
      "Epoch 86 | Total train loss: 5.471998947592738\n",
      "Epoch 86 | Val loss: 0.004987521097064018\n",
      "Epoch 87 | Total train loss: 4.504958750368644\n",
      "Epoch 87 | Val loss: 0.003801864106208086\n",
      "Epoch 88 | Total train loss: 5.2325436975046955\n",
      "Epoch 88 | Val loss: 0.009924238547682762\n",
      "Epoch 89 | Total train loss: 4.539779920571391\n",
      "Epoch 89 | Val loss: 0.0034725854638963938\n",
      "Epoch 90 | Total train loss: 5.098525414827009\n",
      "Epoch 90 | Val loss: 0.008344783447682858\n",
      "Epoch 91 | Total train loss: 4.280244724923932\n",
      "Epoch 91 | Val loss: 0.003717297688126564\n",
      "Epoch 92 | Total train loss: 5.356262834534846\n",
      "Epoch 92 | Val loss: 0.004428566433489323\n",
      "Epoch 93 | Total train loss: 5.98336942418257\n",
      "Epoch 93 | Val loss: 0.004608985967934132\n",
      "Epoch 94 | Total train loss: 6.274903631071538\n",
      "Epoch 94 | Val loss: 0.004585121758282185\n",
      "Epoch 95 | Total train loss: 4.6578331845505545\n",
      "Epoch 95 | Val loss: 0.004116513766348362\n",
      "Epoch 96 | Total train loss: 4.305438732584207\n",
      "Epoch 96 | Val loss: 0.00306898495182395\n",
      "Epoch 97 | Total train loss: 3.873006420497063\n",
      "Epoch 97 | Val loss: 0.0046363272704184055\n",
      "Epoch 98 | Total train loss: 4.492700705045536\n",
      "Epoch 98 | Val loss: 0.004031179938465357\n",
      "Epoch 99 | Total train loss: 4.612035714003355\n",
      "Epoch 99 | Val loss: 0.003110403660684824\n",
      "Epoch 100 | Total train loss: 4.740392850123584\n",
      "Epoch 100 | Val loss: 0.0031848670914769173\n",
      "Test loss: 0.004511552397161722\n",
      "Model: xsmall_small | Training complete\n",
      "Model: xsmall_small | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xsmall_small | Plots made and saved\n",
      "Model: xsmall_small | Predictions saved\n",
      "Model: xsmall_small | Model saved\n",
      "Model: xsmall_small | Garbage collected, states deleted\n",
      "Model: small_medium | Started loop\n",
      "Model: small_medium | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 174.4067918563851\n",
      "Epoch 1 | Val loss: 0.06465815752744675\n",
      "Epoch 2 | Total train loss: 58.691965610600164\n",
      "Epoch 2 | Val loss: 0.06279736757278442\n",
      "Epoch 3 | Total train loss: 50.35666336539498\n",
      "Epoch 3 | Val loss: 0.056135814636945724\n",
      "Epoch 4 | Total train loss: 49.314649161582565\n",
      "Epoch 4 | Val loss: 0.049436815083026886\n",
      "Epoch 5 | Total train loss: 44.2492755991334\n",
      "Epoch 5 | Val loss: 0.043536003679037094\n",
      "Epoch 6 | Total train loss: 43.42816545034111\n",
      "Epoch 6 | Val loss: 0.03250640630722046\n",
      "Epoch 7 | Total train loss: 34.33901625054932\n",
      "Epoch 7 | Val loss: 0.03167717158794403\n",
      "Epoch 8 | Total train loss: 32.00700566584419\n",
      "Epoch 8 | Val loss: 0.028120556846261024\n",
      "Epoch 9 | Total train loss: 26.719150229409934\n",
      "Epoch 9 | Val loss: 0.03330540284514427\n",
      "Epoch 10 | Total train loss: 29.766746845920352\n",
      "Epoch 10 | Val loss: 0.022443994879722595\n",
      "Epoch 11 | Total train loss: 24.13920630532084\n",
      "Epoch 11 | Val loss: 0.022052597254514694\n",
      "Epoch 12 | Total train loss: 23.2579986625542\n",
      "Epoch 12 | Val loss: 0.018192239105701447\n",
      "Epoch 13 | Total train loss: 19.397642373323833\n",
      "Epoch 13 | Val loss: 0.0162020493298769\n",
      "Epoch 14 | Total train loss: 16.503686115503115\n",
      "Epoch 14 | Val loss: 0.017910465598106384\n",
      "Epoch 15 | Total train loss: 13.9909914087674\n",
      "Epoch 15 | Val loss: 0.008644256740808487\n",
      "Epoch 16 | Total train loss: 10.974963878898052\n",
      "Epoch 16 | Val loss: 0.011511514894664288\n",
      "Epoch 17 | Total train loss: 10.473747362277209\n",
      "Epoch 17 | Val loss: 0.010214551351964474\n",
      "Epoch 18 | Total train loss: 9.666505884060939\n",
      "Epoch 18 | Val loss: 0.00929348822683096\n",
      "Epoch 19 | Total train loss: 11.609537262779668\n",
      "Epoch 19 | Val loss: 0.007462206296622753\n",
      "Epoch 20 | Total train loss: 9.725318191722863\n",
      "Epoch 20 | Val loss: 0.009383689612150192\n",
      "Epoch 21 | Total train loss: 7.997116072651579\n",
      "Epoch 21 | Val loss: 0.006502517499029636\n",
      "Epoch 22 | Total train loss: 8.898585396878616\n",
      "Epoch 22 | Val loss: 0.011414493434131145\n",
      "Epoch 23 | Total train loss: 6.315195265292914\n",
      "Epoch 23 | Val loss: 0.006387661676853895\n",
      "Epoch 24 | Total train loss: 8.671211227160029\n",
      "Epoch 24 | Val loss: 0.008068500086665154\n",
      "Epoch 25 | Total train loss: 8.563796301170441\n",
      "Epoch 25 | Val loss: 0.009516761638224125\n",
      "Epoch 26 | Total train loss: 9.197264082610218\n",
      "Epoch 26 | Val loss: 0.006667731795459986\n",
      "Epoch 27 | Total train loss: 6.1114448334569715\n",
      "Epoch 27 | Val loss: 0.00651884451508522\n",
      "Epoch 28 | Total train loss: 7.548056136963623\n",
      "Epoch 28 | Val loss: 0.006748971529304981\n",
      "Epoch 29 | Total train loss: 8.523480289752797\n",
      "Epoch 29 | Val loss: 0.005928836762905121\n",
      "Epoch 30 | Total train loss: 6.636793125182521\n",
      "Epoch 30 | Val loss: 0.007516424171626568\n",
      "Epoch 31 | Total train loss: 6.979564279407441\n",
      "Epoch 31 | Val loss: 0.012595727108418941\n",
      "Epoch 32 | Total train loss: 7.730392132503994\n",
      "Epoch 32 | Val loss: 0.0067309667356312275\n",
      "Epoch 33 | Total train loss: 6.609117637218674\n",
      "Epoch 33 | Val loss: 0.009889374487102032\n",
      "Epoch 34 | Total train loss: 8.36058161399194\n",
      "Epoch 34 | Val loss: 0.008583336137235165\n",
      "Epoch 35 | Total train loss: 6.474895431584173\n",
      "Epoch 35 | Val loss: 0.006065770052373409\n",
      "Epoch 36 | Total train loss: 7.563340254547825\n",
      "Epoch 36 | Val loss: 0.004676502663642168\n",
      "Epoch 37 | Total train loss: 7.564533767586909\n",
      "Epoch 37 | Val loss: 0.0057562473230063915\n",
      "Epoch 38 | Total train loss: 6.092658856502567\n",
      "Epoch 38 | Val loss: 0.006044103763997555\n",
      "Epoch 39 | Total train loss: 6.620233070936706\n",
      "Epoch 39 | Val loss: 0.00685549434274435\n",
      "Epoch 40 | Total train loss: 6.343833718862243\n",
      "Epoch 40 | Val loss: 0.008586777374148369\n",
      "Epoch 41 | Total train loss: 6.481172871782746\n",
      "Epoch 41 | Val loss: 0.005256445147097111\n",
      "Epoch 42 | Total train loss: 6.012853345622716\n",
      "Epoch 42 | Val loss: 0.006256577093154192\n",
      "Epoch 43 | Total train loss: 6.671473660052811\n",
      "Epoch 43 | Val loss: 0.006429233588278294\n",
      "Epoch 44 | Total train loss: 6.689156439557792\n",
      "Epoch 44 | Val loss: 0.004598815925419331\n",
      "Epoch 45 | Total train loss: 5.800330870612015\n",
      "Epoch 45 | Val loss: 0.005496768280863762\n",
      "Epoch 46 | Total train loss: 4.738578345149335\n",
      "Epoch 46 | Val loss: 0.006962897721678019\n",
      "Epoch 47 | Total train loss: 6.07781071745535\n",
      "Epoch 47 | Val loss: 0.005517458077520132\n",
      "Epoch 48 | Total train loss: 5.991917154259681\n",
      "Epoch 48 | Val loss: 0.005969172343611717\n",
      "Epoch 49 | Total train loss: 5.987108298115345\n",
      "Epoch 49 | Val loss: 0.004694151226431131\n",
      "Epoch 50 | Total train loss: 6.2046770808178735\n",
      "Epoch 50 | Val loss: 0.0051668924279510975\n",
      "Epoch 51 | Total train loss: 4.761988750559794\n",
      "Epoch 51 | Val loss: 0.006590493023395538\n",
      "Epoch 52 | Total train loss: 4.400998301072377\n",
      "Epoch 52 | Val loss: 0.006986243184655905\n",
      "Epoch 53 | Total train loss: 6.086311239533686\n",
      "Epoch 53 | Val loss: 0.0044281454756855965\n",
      "Epoch 54 | Total train loss: 5.756307169816978\n",
      "Epoch 54 | Val loss: 0.008112837560474873\n",
      "Epoch 55 | Total train loss: 4.1700885345350684\n",
      "Epoch 55 | Val loss: 0.004147307015955448\n",
      "Epoch 56 | Total train loss: 5.089188440964335\n",
      "Epoch 56 | Val loss: 0.004019679501652718\n",
      "Epoch 57 | Total train loss: 4.785898460412\n",
      "Epoch 57 | Val loss: 0.007141684181988239\n",
      "Epoch 58 | Total train loss: 4.417476421851802\n",
      "Epoch 58 | Val loss: 0.0043800221756100655\n",
      "Epoch 59 | Total train loss: 5.595005426295529\n",
      "Epoch 59 | Val loss: 0.004772971384227276\n",
      "Epoch 60 | Total train loss: 4.476792969146828\n",
      "Epoch 60 | Val loss: 0.004381050355732441\n",
      "Epoch 61 | Total train loss: 4.681679980779336\n",
      "Epoch 61 | Val loss: 0.0052070943638682365\n",
      "Epoch 62 | Total train loss: 5.207322853909545\n",
      "Epoch 62 | Val loss: 0.004123393911868334\n",
      "Epoch 63 | Total train loss: 4.803226710041599\n",
      "Epoch 63 | Val loss: 0.005078637041151524\n",
      "Epoch 64 | Total train loss: 4.3500055420181525\n",
      "Epoch 64 | Val loss: 0.005311131943017244\n",
      "Epoch 65 | Total train loss: 4.967854681126937\n",
      "Epoch 65 | Val loss: 0.005483764223754406\n",
      "Epoch 66 | Total train loss: 5.3388165528782565\n",
      "Epoch 66 | Val loss: 0.0030789796728640795\n",
      "Epoch 67 | Total train loss: 5.937631522812808\n",
      "Epoch 67 | Val loss: 0.004290737211704254\n",
      "Epoch 68 | Total train loss: 4.4568680297306855\n",
      "Epoch 68 | Val loss: 0.003104645060375333\n",
      "Epoch 69 | Total train loss: 3.932038777693947\n",
      "Epoch 69 | Val loss: 0.00397679815068841\n",
      "Epoch 70 | Total train loss: 4.604099932793815\n",
      "Epoch 70 | Val loss: 0.003125127637758851\n",
      "Epoch 71 | Total train loss: 4.294851165771888\n",
      "Epoch 71 | Val loss: 0.003828858956694603\n",
      "Epoch 72 | Total train loss: 4.723414094066726\n",
      "Epoch 72 | Val loss: 0.002645858097821474\n",
      "Epoch 73 | Total train loss: 3.6670674497762548\n",
      "Epoch 73 | Val loss: 0.0031439466401934624\n",
      "Epoch 74 | Total train loss: 4.39919648208911\n",
      "Epoch 74 | Val loss: 0.004241615999490023\n",
      "Epoch 75 | Total train loss: 4.083806280443696\n",
      "Epoch 75 | Val loss: 0.0032436975743621588\n",
      "Epoch 76 | Total train loss: 4.77672649714259\n",
      "Epoch 76 | Val loss: 0.003959245048463345\n",
      "Epoch 77 | Total train loss: 3.654146012702313\n",
      "Epoch 77 | Val loss: 0.005965503863990307\n",
      "Epoch 78 | Total train loss: 4.519610816421903\n",
      "Epoch 78 | Val loss: 0.004511892329901457\n",
      "Epoch 79 | Total train loss: 5.10695079200849\n",
      "Epoch 79 | Val loss: 0.0029122859705239534\n",
      "Epoch 80 | Total train loss: 4.511728284217497\n",
      "Epoch 80 | Val loss: 0.004142985213547945\n",
      "Epoch 81 | Total train loss: 3.0342508559387795\n",
      "Epoch 81 | Val loss: 0.002565946662798524\n",
      "Epoch 82 | Total train loss: 3.7136943830945484\n",
      "Epoch 82 | Val loss: 0.005150299984961748\n",
      "Epoch 83 | Total train loss: 4.8005594223218395\n",
      "Epoch 83 | Val loss: 0.003682320937514305\n",
      "Epoch 84 | Total train loss: 3.3690524809455837\n",
      "Epoch 84 | Val loss: 0.00459221750497818\n",
      "Epoch 85 | Total train loss: 3.262845361941231\n",
      "Epoch 85 | Val loss: 0.003714366815984249\n",
      "Epoch 86 | Total train loss: 4.073346658537389\n",
      "Epoch 86 | Val loss: 0.004785018507391214\n",
      "Epoch 87 | Total train loss: 4.071181519846732\n",
      "Epoch 87 | Val loss: 0.0025983857922255993\n",
      "Epoch 88 | Total train loss: 3.3488974215417784\n",
      "Epoch 88 | Val loss: 0.0029491116292774677\n",
      "Epoch 89 | Total train loss: 3.258988054679321\n",
      "Epoch 89 | Val loss: 0.004976197145879269\n",
      "Epoch 90 | Total train loss: 2.846509132416884\n",
      "Epoch 90 | Val loss: 0.002464359626173973\n",
      "Epoch 91 | Total train loss: 4.165166909975369\n",
      "Epoch 91 | Val loss: 0.0023639057762920856\n",
      "Epoch 92 | Total train loss: 3.8230283195251076\n",
      "Epoch 92 | Val loss: 0.0039518061093986034\n",
      "Epoch 93 | Total train loss: 4.013402624557557\n",
      "Epoch 93 | Val loss: 0.0036285631358623505\n",
      "Epoch 94 | Total train loss: 4.115218405630685\n",
      "Epoch 94 | Val loss: 0.0029887654818594456\n",
      "Epoch 95 | Total train loss: 3.8407722234233006\n",
      "Epoch 95 | Val loss: 0.003441775217652321\n",
      "Epoch 96 | Total train loss: 3.8157289997645876\n",
      "Epoch 96 | Val loss: 0.005249196197837591\n",
      "Epoch 97 | Total train loss: 3.2611638677553856\n",
      "Epoch 97 | Val loss: 0.0027158798184245825\n",
      "Epoch 98 | Total train loss: 3.7787869156724128\n",
      "Epoch 98 | Val loss: 0.0018235183088108897\n",
      "Epoch 99 | Total train loss: 3.5675998667001068\n",
      "Epoch 99 | Val loss: 0.004207565914839506\n",
      "Epoch 100 | Total train loss: 2.651469532146251\n",
      "Epoch 100 | Val loss: 0.0037980072665959597\n",
      "Test loss: 0.005723519250750542\n",
      "Model: small_medium | Training complete\n",
      "Model: small_medium | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: small_medium | Plots made and saved\n",
      "Model: small_medium | Predictions saved\n",
      "Model: small_medium | Model saved\n",
      "Model: small_medium | Garbage collected, states deleted\n",
      "Model: medium_large | Started loop\n",
      "Model: medium_large | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 149.27743168476445\n",
      "Epoch 1 | Val loss: 0.07099466025829315\n",
      "Epoch 2 | Total train loss: 59.703053746077785\n",
      "Epoch 2 | Val loss: 0.05271631479263306\n",
      "Epoch 3 | Total train loss: 50.3023998086137\n",
      "Epoch 3 | Val loss: 0.045246902853250504\n",
      "Epoch 4 | Total train loss: 43.0702445664665\n",
      "Epoch 4 | Val loss: 0.04814727231860161\n",
      "Epoch 5 | Total train loss: 35.22893737656341\n",
      "Epoch 5 | Val loss: 0.02711251936852932\n",
      "Epoch 6 | Total train loss: 28.92442383504067\n",
      "Epoch 6 | Val loss: 0.03557807579636574\n",
      "Epoch 7 | Total train loss: 27.091543986623947\n",
      "Epoch 7 | Val loss: 0.025098677724599838\n",
      "Epoch 8 | Total train loss: 23.453021447257015\n",
      "Epoch 8 | Val loss: 0.020558426156640053\n",
      "Epoch 9 | Total train loss: 18.730482828183142\n",
      "Epoch 9 | Val loss: 0.018261337652802467\n",
      "Epoch 10 | Total train loss: 14.052016918408981\n",
      "Epoch 10 | Val loss: 0.01789696328341961\n",
      "Epoch 11 | Total train loss: 11.72725145681079\n",
      "Epoch 11 | Val loss: 0.009095290675759315\n",
      "Epoch 12 | Total train loss: 11.559081381841679\n",
      "Epoch 12 | Val loss: 0.00915983971208334\n",
      "Epoch 13 | Total train loss: 11.451300309202907\n",
      "Epoch 13 | Val loss: 0.009869844652712345\n",
      "Epoch 14 | Total train loss: 10.203544932057412\n",
      "Epoch 14 | Val loss: 0.016230246052145958\n",
      "Epoch 15 | Total train loss: 10.22955241567458\n",
      "Epoch 15 | Val loss: 0.010603772476315498\n",
      "Epoch 16 | Total train loss: 7.445479955440078\n",
      "Epoch 16 | Val loss: 0.007900900207459927\n",
      "Epoch 17 | Total train loss: 9.462696931689152\n",
      "Epoch 17 | Val loss: 0.005710178520530462\n",
      "Epoch 18 | Total train loss: 9.413443643817118\n",
      "Epoch 18 | Val loss: 0.009699706919491291\n",
      "Epoch 19 | Total train loss: 7.504417103161359\n",
      "Epoch 19 | Val loss: 0.0065141478553414345\n",
      "Epoch 20 | Total train loss: 9.468274431177633\n",
      "Epoch 20 | Val loss: 0.012555214576423168\n",
      "Epoch 21 | Total train loss: 7.643952120477479\n",
      "Epoch 21 | Val loss: 0.008123804815113544\n",
      "Epoch 22 | Total train loss: 8.454111418301522\n",
      "Epoch 22 | Val loss: 0.0055619035847485065\n",
      "Epoch 23 | Total train loss: 6.927105438636772\n",
      "Epoch 23 | Val loss: 0.00818732287734747\n",
      "Epoch 24 | Total train loss: 6.331492510649241\n",
      "Epoch 24 | Val loss: 0.00779309868812561\n",
      "Epoch 25 | Total train loss: 8.188887861175772\n",
      "Epoch 25 | Val loss: 0.008856450207531452\n",
      "Epoch 26 | Total train loss: 7.1701345170013155\n",
      "Epoch 26 | Val loss: 0.008671481162309647\n",
      "Epoch 27 | Total train loss: 8.566734850866851\n",
      "Epoch 27 | Val loss: 0.005701704882085323\n",
      "Epoch 28 | Total train loss: 7.518518887984783\n",
      "Epoch 28 | Val loss: 0.007009546272456646\n",
      "Epoch 29 | Total train loss: 7.215537347186\n",
      "Epoch 29 | Val loss: 0.0060387710109353065\n",
      "Epoch 30 | Total train loss: 6.559679022006549\n",
      "Epoch 30 | Val loss: 0.008128409273922443\n",
      "Epoch 31 | Total train loss: 5.374789281479593\n",
      "Epoch 31 | Val loss: 0.006871935911476612\n",
      "Epoch 32 | Total train loss: 6.8587050505273055\n",
      "Epoch 32 | Val loss: 0.004287893883883953\n",
      "Epoch 33 | Total train loss: 6.3359833929487195\n",
      "Epoch 33 | Val loss: 0.0038714082911610603\n",
      "Epoch 34 | Total train loss: 6.099714256147763\n",
      "Epoch 34 | Val loss: 0.005137230735272169\n",
      "Epoch 35 | Total train loss: 5.796550718377603\n",
      "Epoch 35 | Val loss: 0.003576819086447358\n",
      "Epoch 36 | Total train loss: 5.66429546674965\n",
      "Epoch 36 | Val loss: 0.00404656957834959\n",
      "Epoch 37 | Total train loss: 6.312790926375726\n",
      "Epoch 37 | Val loss: 0.006301108282059431\n",
      "Epoch 38 | Total train loss: 5.402966440097998\n",
      "Epoch 38 | Val loss: 0.0066034262999892235\n",
      "Epoch 39 | Total train loss: 6.914482917209625\n",
      "Epoch 39 | Val loss: 0.004826834890991449\n",
      "Epoch 40 | Total train loss: 4.938322879974521\n",
      "Epoch 40 | Val loss: 0.004240943118929863\n",
      "Epoch 41 | Total train loss: 5.949194545547016\n",
      "Epoch 41 | Val loss: 0.008738565258681774\n",
      "Epoch 42 | Total train loss: 5.857855895989815\n",
      "Epoch 42 | Val loss: 0.005353886634111404\n",
      "Epoch 43 | Total train loss: 4.104954829085216\n",
      "Epoch 43 | Val loss: 0.006195917259901762\n",
      "Epoch 44 | Total train loss: 5.33424226003217\n",
      "Epoch 44 | Val loss: 0.0065852077677845955\n",
      "Epoch 45 | Total train loss: 5.4937135377317645\n",
      "Epoch 45 | Val loss: 0.0031459054443985224\n",
      "Epoch 46 | Total train loss: 4.733254276363823\n",
      "Epoch 46 | Val loss: 0.005644273944199085\n",
      "Epoch 47 | Total train loss: 5.457107356279266\n",
      "Epoch 47 | Val loss: 0.006969207897782326\n",
      "Epoch 48 | Total train loss: 4.916106432640845\n",
      "Epoch 48 | Val loss: 0.005326090846210718\n",
      "Epoch 49 | Total train loss: 4.0608356376551455\n",
      "Epoch 49 | Val loss: 0.004995209630578756\n",
      "Epoch 50 | Total train loss: 5.589813173183586\n",
      "Epoch 50 | Val loss: 0.003782880725339055\n",
      "Epoch 51 | Total train loss: 5.145386526269704\n",
      "Epoch 51 | Val loss: 0.0034982780925929546\n",
      "Epoch 52 | Total train loss: 3.712278076879727\n",
      "Epoch 52 | Val loss: 0.004023748449981213\n",
      "Epoch 53 | Total train loss: 4.378848383343325\n",
      "Epoch 53 | Val loss: 0.006525288335978985\n",
      "Epoch 54 | Total train loss: 4.252318136956774\n",
      "Epoch 54 | Val loss: 0.0036242580972611904\n",
      "Epoch 55 | Total train loss: 3.015404842191515\n",
      "Epoch 55 | Val loss: 0.003616146743297577\n",
      "Epoch 56 | Total train loss: 4.116504333618536\n",
      "Epoch 56 | Val loss: 0.0045649162493646145\n",
      "Epoch 57 | Total train loss: 3.425860434515812\n",
      "Epoch 57 | Val loss: 0.004129224456846714\n",
      "Epoch 58 | Total train loss: 3.8068949462124237\n",
      "Epoch 58 | Val loss: 0.002532288199290633\n",
      "Epoch 59 | Total train loss: 4.650894277362113\n",
      "Epoch 59 | Val loss: 0.00305509683676064\n",
      "Epoch 60 | Total train loss: 3.720938452246571\n",
      "Epoch 60 | Val loss: 0.0037065783981233835\n",
      "Epoch 61 | Total train loss: 4.428932318532077\n",
      "Epoch 61 | Val loss: 0.004139486234635115\n",
      "Epoch 62 | Total train loss: 5.037688126902367\n",
      "Epoch 62 | Val loss: 0.002827824093401432\n",
      "Epoch 63 | Total train loss: 4.3672167979378855\n",
      "Epoch 63 | Val loss: 0.005610164720565081\n",
      "Epoch 64 | Total train loss: 4.006048676441878\n",
      "Epoch 64 | Val loss: 0.0036000157706439495\n",
      "Epoch 65 | Total train loss: 3.457795093939467\n",
      "Epoch 65 | Val loss: 0.006207657046616077\n",
      "Epoch 66 | Total train loss: 4.083950582518753\n",
      "Epoch 66 | Val loss: 0.003915856592357159\n",
      "Epoch 67 | Total train loss: 3.7059878430546576\n",
      "Epoch 67 | Val loss: 0.00375003507360816\n",
      "Epoch 68 | Total train loss: 3.5790292574266687\n",
      "Epoch 68 | Val loss: 0.003589968429878354\n",
      "Epoch 69 | Total train loss: 3.445164440606959\n",
      "Epoch 69 | Val loss: 0.003787085646763444\n",
      "Epoch 70 | Total train loss: 3.60422579333283\n",
      "Epoch 70 | Val loss: 0.0029898989014327526\n",
      "Epoch 71 | Total train loss: 3.0715256304454783\n",
      "Epoch 71 | Val loss: 0.002669496927410364\n",
      "Epoch 72 | Total train loss: 3.424651491430609\n",
      "Epoch 72 | Val loss: 0.002797609893605113\n",
      "Epoch 73 | Total train loss: 3.9086305499467358\n",
      "Epoch 73 | Val loss: 0.003046497469767928\n",
      "Epoch 74 | Total train loss: 3.5772890691517887\n",
      "Epoch 74 | Val loss: 0.004300124943256378\n",
      "Epoch 75 | Total train loss: 3.26261895485419\n",
      "Epoch 75 | Val loss: 0.006657241377979517\n",
      "Epoch 76 | Total train loss: 3.1877722741824073\n",
      "Epoch 76 | Val loss: 0.003841903991997242\n",
      "Epoch 77 | Total train loss: 3.777510132941643\n",
      "Epoch 77 | Val loss: 0.002381658647209406\n",
      "Epoch 78 | Total train loss: 3.799352568674408\n",
      "Epoch 78 | Val loss: 0.003067765152081847\n",
      "Epoch 79 | Total train loss: 3.0368043430104876\n",
      "Epoch 79 | Val loss: 0.0035995175130665302\n",
      "Epoch 80 | Total train loss: 3.706912156224007\n",
      "Epoch 80 | Val loss: 0.0029016488697379827\n",
      "Epoch 81 | Total train loss: 2.9993309872893974\n",
      "Epoch 81 | Val loss: 0.0031715547665953636\n",
      "Epoch 82 | Total train loss: 3.1626917927563056\n",
      "Epoch 82 | Val loss: 0.0020354995504021645\n",
      "Epoch 83 | Total train loss: 2.853600321244244\n",
      "Epoch 83 | Val loss: 0.003381600370630622\n",
      "Epoch 84 | Total train loss: 4.074283172078481\n",
      "Epoch 84 | Val loss: 0.004425700753927231\n",
      "Epoch 85 | Total train loss: 3.454847339372236\n",
      "Epoch 85 | Val loss: 0.0023665125481784344\n",
      "Epoch 86 | Total train loss: 4.252699042465338\n",
      "Epoch 86 | Val loss: 0.002294942969456315\n",
      "Epoch 87 | Total train loss: 3.233882803322601\n",
      "Epoch 87 | Val loss: 0.003986885771155357\n",
      "Epoch 88 | Total train loss: 3.11306947241917\n",
      "Epoch 88 | Val loss: 0.002835619729012251\n",
      "Epoch 89 | Total train loss: 3.8935443678083175\n",
      "Epoch 89 | Val loss: 0.004989759996533394\n",
      "Epoch 90 | Total train loss: 4.196842560126356\n",
      "Epoch 90 | Val loss: 0.0027467156760394573\n",
      "Epoch 91 | Total train loss: 3.13770506174194\n",
      "Epoch 91 | Val loss: 0.0020515210926532745\n",
      "Epoch 92 | Total train loss: 3.3913594347990283\n",
      "Epoch 92 | Val loss: 0.0038051053415983915\n",
      "Epoch 93 | Total train loss: 3.2622789695478787\n",
      "Epoch 93 | Val loss: 0.002980057382956147\n",
      "Epoch 94 | Total train loss: 3.462308130772101\n",
      "Epoch 94 | Val loss: 0.002399286488071084\n",
      "Epoch 95 | Total train loss: 2.9883523803021035\n",
      "Epoch 95 | Val loss: 0.0025278462562710047\n",
      "Epoch 96 | Total train loss: 3.882374756574279\n",
      "Epoch 96 | Val loss: 0.0014326063683256507\n",
      "Epoch 97 | Total train loss: 3.2192128316754633\n",
      "Epoch 97 | Val loss: 0.004124833270907402\n",
      "Epoch 98 | Total train loss: 3.126244122507387\n",
      "Epoch 98 | Val loss: 0.00402730330824852\n",
      "Epoch 99 | Total train loss: 3.1676891977770083\n",
      "Epoch 99 | Val loss: 0.002842967864125967\n",
      "Epoch 100 | Total train loss: 3.749141724820902\n",
      "Epoch 100 | Val loss: 0.00360873038880527\n",
      "Test loss: 0.0031791108194738626\n",
      "Model: medium_large | Training complete\n",
      "Model: medium_large | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: medium_large | Plots made and saved\n",
      "Model: medium_large | Predictions saved\n",
      "Model: medium_large | Model saved\n",
      "Model: medium_large | Garbage collected, states deleted\n",
      "Model: large_xlarge | Started loop\n",
      "Model: large_xlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 110.98425444605891\n",
      "Epoch 1 | Val loss: 0.052569981664419174\n",
      "Epoch 2 | Total train loss: 44.69586639404588\n",
      "Epoch 2 | Val loss: 0.04003478214144707\n",
      "Epoch 3 | Total train loss: 33.10511817470342\n",
      "Epoch 3 | Val loss: 0.03708915784955025\n",
      "Epoch 4 | Total train loss: 30.572354708483545\n",
      "Epoch 4 | Val loss: 0.027845917269587517\n",
      "Epoch 5 | Total train loss: 21.36259579044463\n",
      "Epoch 5 | Val loss: 0.01822696067392826\n",
      "Epoch 6 | Total train loss: 18.81923647449912\n",
      "Epoch 6 | Val loss: 0.01269418653100729\n",
      "Epoch 7 | Total train loss: 11.964618710880131\n",
      "Epoch 7 | Val loss: 0.010814853943884373\n",
      "Epoch 8 | Total train loss: 11.005176490680242\n",
      "Epoch 8 | Val loss: 0.01133502647280693\n",
      "Epoch 9 | Total train loss: 11.38655174578821\n",
      "Epoch 9 | Val loss: 0.00742409098893404\n",
      "Epoch 10 | Total train loss: 9.595611350983972\n",
      "Epoch 10 | Val loss: 0.009553677402436733\n",
      "Epoch 11 | Total train loss: 11.262743870158829\n",
      "Epoch 11 | Val loss: 0.007321934215724468\n",
      "Epoch 12 | Total train loss: 9.046476838746912\n",
      "Epoch 12 | Val loss: 0.010735159739851952\n",
      "Epoch 13 | Total train loss: 10.778648293336005\n",
      "Epoch 13 | Val loss: 0.011885599233210087\n",
      "Epoch 14 | Total train loss: 8.185479328209112\n",
      "Epoch 14 | Val loss: 0.009784414432942867\n",
      "Epoch 15 | Total train loss: 8.664767897730371\n",
      "Epoch 15 | Val loss: 0.01007523201406002\n",
      "Epoch 16 | Total train loss: 10.273861715161672\n",
      "Epoch 16 | Val loss: 0.007636425085365772\n",
      "Epoch 17 | Total train loss: 8.96072584145486\n",
      "Epoch 17 | Val loss: 0.0040516615845263\n",
      "Epoch 18 | Total train loss: 8.04907992325002\n",
      "Epoch 18 | Val loss: 0.016513437032699585\n",
      "Epoch 19 | Total train loss: 7.340851173884857\n",
      "Epoch 19 | Val loss: 0.015738585963845253\n",
      "Epoch 20 | Total train loss: 9.795818172670465\n",
      "Epoch 20 | Val loss: 0.007618882227689028\n",
      "Epoch 21 | Total train loss: 8.060737598002106\n",
      "Epoch 21 | Val loss: 0.0076926774345338345\n",
      "Epoch 22 | Total train loss: 7.183077851093913\n",
      "Epoch 22 | Val loss: 0.007232257630676031\n",
      "Epoch 23 | Total train loss: 8.123142818220686\n",
      "Epoch 23 | Val loss: 0.008022593334317207\n",
      "Epoch 24 | Total train loss: 6.391036586255268\n",
      "Epoch 24 | Val loss: 0.0056410073302686214\n",
      "Epoch 25 | Total train loss: 7.20519821070809\n",
      "Epoch 25 | Val loss: 0.0074359942227602005\n",
      "Epoch 26 | Total train loss: 5.9535177910329935\n",
      "Epoch 26 | Val loss: 0.0069047436118125916\n",
      "Epoch 27 | Total train loss: 4.6398915848381534\n",
      "Epoch 27 | Val loss: 0.006886323448270559\n",
      "Epoch 28 | Total train loss: 7.817697459841952\n",
      "Epoch 28 | Val loss: 0.007287319749593735\n",
      "Epoch 29 | Total train loss: 7.669181296974557\n",
      "Epoch 29 | Val loss: 0.004801234696060419\n",
      "Epoch 30 | Total train loss: 5.8464472663017375\n",
      "Epoch 30 | Val loss: 0.006685983389616013\n",
      "Epoch 31 | Total train loss: 7.812081266148681\n",
      "Epoch 31 | Val loss: 0.009421422146260738\n",
      "Epoch 32 | Total train loss: 7.388757076925003\n",
      "Epoch 32 | Val loss: 0.0065597291104495525\n",
      "Epoch 33 | Total train loss: 6.184669498413314\n",
      "Epoch 33 | Val loss: 0.005834326148033142\n",
      "Epoch 34 | Total train loss: 5.770553481319439\n",
      "Epoch 34 | Val loss: 0.007554373703896999\n",
      "Epoch 35 | Total train loss: 5.446950116069502\n",
      "Epoch 35 | Val loss: 0.006242041010409594\n",
      "Epoch 36 | Total train loss: 4.94869600820914\n",
      "Epoch 36 | Val loss: 0.0044902595691382885\n",
      "Epoch 37 | Total train loss: 6.622435108136415\n",
      "Epoch 37 | Val loss: 0.0037242674734443426\n",
      "Epoch 38 | Total train loss: 4.774022447068035\n",
      "Epoch 38 | Val loss: 0.003503162879496813\n",
      "Epoch 39 | Total train loss: 6.606696169871384\n",
      "Epoch 39 | Val loss: 0.004269205499440432\n",
      "Epoch 40 | Total train loss: 5.749914696986821\n",
      "Epoch 40 | Val loss: 0.00606013648211956\n",
      "Epoch 41 | Total train loss: 5.497961775808335\n",
      "Epoch 41 | Val loss: 0.005032978951931\n",
      "Epoch 42 | Total train loss: 5.513727401686879\n",
      "Epoch 42 | Val loss: 0.006795779336243868\n",
      "Epoch 43 | Total train loss: 5.561854995947442\n",
      "Epoch 43 | Val loss: 0.004093165509402752\n",
      "Epoch 44 | Total train loss: 4.6007988444671355\n",
      "Epoch 44 | Val loss: 0.005784942768514156\n",
      "Epoch 45 | Total train loss: 5.696471536350373\n",
      "Epoch 45 | Val loss: 0.004025523085147142\n",
      "Epoch 46 | Total train loss: 4.183352364401969\n",
      "Epoch 46 | Val loss: 0.0041368999518454075\n",
      "Epoch 47 | Total train loss: 5.09486250665816\n",
      "Epoch 47 | Val loss: 0.004452360328286886\n",
      "Epoch 48 | Total train loss: 4.3497809936563385\n",
      "Epoch 48 | Val loss: 0.00709239486604929\n",
      "Epoch 49 | Total train loss: 5.277940130447803\n",
      "Epoch 49 | Val loss: 0.004040160682052374\n",
      "Epoch 50 | Total train loss: 3.758125703096141\n",
      "Epoch 50 | Val loss: 0.003976043313741684\n",
      "Epoch 51 | Total train loss: 5.112254754540231\n",
      "Epoch 51 | Val loss: 0.004704262595623732\n",
      "Epoch 52 | Total train loss: 3.5590966508206634\n",
      "Epoch 52 | Val loss: 0.005086133722215891\n",
      "Epoch 53 | Total train loss: 4.840573159432296\n",
      "Epoch 53 | Val loss: 0.00616912916302681\n",
      "Epoch 54 | Total train loss: 4.115087689120514\n",
      "Epoch 54 | Val loss: 0.004232072737067938\n",
      "Epoch 55 | Total train loss: 4.059903603126941\n",
      "Epoch 55 | Val loss: 0.0049073100090026855\n",
      "Epoch 56 | Total train loss: 3.463803610418381\n",
      "Epoch 56 | Val loss: 0.007855595089495182\n",
      "Epoch 57 | Total train loss: 4.992825391581334\n",
      "Epoch 57 | Val loss: 0.0049117677845060825\n",
      "Epoch 58 | Total train loss: 5.195662602314314\n",
      "Epoch 58 | Val loss: 0.006203823257237673\n",
      "Epoch 59 | Total train loss: 4.166034686206103\n",
      "Epoch 59 | Val loss: 0.002393310656771064\n",
      "Epoch 60 | Total train loss: 3.864267571311757\n",
      "Epoch 60 | Val loss: 0.003365251235663891\n",
      "Epoch 61 | Total train loss: 3.0935111789384706\n",
      "Epoch 61 | Val loss: 0.005534494295716286\n",
      "Epoch 62 | Total train loss: 4.700244783468406\n",
      "Epoch 62 | Val loss: 0.002838938729837537\n",
      "Epoch 63 | Total train loss: 4.980928505637621\n",
      "Epoch 63 | Val loss: 0.0031965160742402077\n",
      "Epoch 64 | Total train loss: 3.5812666900542354\n",
      "Epoch 64 | Val loss: 0.00315117253921926\n",
      "Epoch 65 | Total train loss: 4.687509730167619\n",
      "Epoch 65 | Val loss: 0.0040414463728666306\n",
      "Epoch 66 | Total train loss: 3.946371959465978\n",
      "Epoch 66 | Val loss: 0.004403682425618172\n",
      "Epoch 67 | Total train loss: 4.176568349543686\n",
      "Epoch 67 | Val loss: 0.002666811691597104\n",
      "Epoch 68 | Total train loss: 2.8604192033241134\n",
      "Epoch 68 | Val loss: 0.0033374219201505184\n",
      "Epoch 69 | Total train loss: 3.052825034958687\n",
      "Epoch 69 | Val loss: 0.0020790547132492065\n",
      "Epoch 70 | Total train loss: 3.6348055983798417\n",
      "Epoch 70 | Val loss: 0.00418916717171669\n",
      "Epoch 71 | Total train loss: 3.6981431897149832\n",
      "Epoch 71 | Val loss: 0.004702077247202396\n",
      "Epoch 72 | Total train loss: 3.5701491671690064\n",
      "Epoch 72 | Val loss: 0.002595813013613224\n",
      "Epoch 73 | Total train loss: 2.918452650181223\n",
      "Epoch 73 | Val loss: 0.00273807137273252\n",
      "Epoch 74 | Total train loss: 2.9726350454383237\n",
      "Epoch 74 | Val loss: 0.004618688486516476\n",
      "Epoch 75 | Total train loss: 3.153920650833129\n",
      "Epoch 75 | Val loss: 0.004192826803773642\n",
      "Epoch 76 | Total train loss: 3.7995511969218967\n",
      "Epoch 76 | Val loss: 0.002436694223433733\n",
      "Epoch 77 | Total train loss: 3.8123678367416005\n",
      "Epoch 77 | Val loss: 0.002034625969827175\n",
      "Epoch 78 | Total train loss: 2.8012742254626346\n",
      "Epoch 78 | Val loss: 0.0021767818834632635\n",
      "Epoch 79 | Total train loss: 3.800876938379588\n",
      "Epoch 79 | Val loss: 0.004101828206330538\n",
      "Epoch 80 | Total train loss: 3.8227780022986053\n",
      "Epoch 80 | Val loss: 0.0033900151029229164\n",
      "Epoch 81 | Total train loss: 3.5237276799008725\n",
      "Epoch 81 | Val loss: 0.0034944741055369377\n",
      "Epoch 82 | Total train loss: 3.063162553134134\n",
      "Epoch 82 | Val loss: 0.0031493643764406443\n",
      "Epoch 83 | Total train loss: 3.1588271903483474\n",
      "Epoch 83 | Val loss: 0.002192286541685462\n",
      "Epoch 84 | Total train loss: 2.398749629926556\n",
      "Epoch 84 | Val loss: 0.0032156563829630613\n",
      "Epoch 85 | Total train loss: 3.0481573725297153\n",
      "Epoch 85 | Val loss: 0.003310380969196558\n",
      "Epoch 86 | Total train loss: 3.3575580261110076\n",
      "Epoch 86 | Val loss: 0.002422502264380455\n",
      "Epoch 87 | Total train loss: 3.2143416065820816\n",
      "Epoch 87 | Val loss: 0.002700148383155465\n",
      "Epoch 88 | Total train loss: 4.822919344270758\n",
      "Epoch 88 | Val loss: 0.0023897418286651373\n",
      "Epoch 89 | Total train loss: 3.002028009166736\n",
      "Epoch 89 | Val loss: 0.00290310219861567\n",
      "Epoch 90 | Total train loss: 3.609409826210481\n",
      "Epoch 90 | Val loss: 0.0012681977823376656\n",
      "Epoch 91 | Total train loss: 2.86207659298006\n",
      "Epoch 91 | Val loss: 0.0014036912471055984\n",
      "Epoch 92 | Total train loss: 2.904628050389192\n",
      "Epoch 92 | Val loss: 0.00231893640011549\n",
      "Epoch 93 | Total train loss: 3.244871135370545\n",
      "Epoch 93 | Val loss: 0.004628430586308241\n",
      "Epoch 94 | Total train loss: 2.6730810521343358\n",
      "Epoch 94 | Val loss: 0.003329963656142354\n",
      "Epoch 95 | Total train loss: 2.967334979836778\n",
      "Epoch 95 | Val loss: 0.0030210160184651613\n",
      "Epoch 96 | Total train loss: 2.6598670571902403\n",
      "Epoch 96 | Val loss: 0.0023148071486502886\n",
      "Epoch 97 | Total train loss: 2.5944010074019843\n",
      "Epoch 97 | Val loss: 0.0019494738662615418\n",
      "Epoch 98 | Total train loss: 2.8801209104251484\n",
      "Epoch 98 | Val loss: 0.0016028456157073379\n",
      "Epoch 99 | Total train loss: 2.6537462468306785\n",
      "Epoch 99 | Val loss: 0.0014041127869859338\n",
      "Epoch 100 | Total train loss: 2.7762936828480065\n",
      "Epoch 100 | Val loss: 0.003927926998585463\n",
      "Test loss: 0.0030745172407478094\n",
      "Model: large_xlarge | Training complete\n",
      "Model: large_xlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: large_xlarge | Plots made and saved\n",
      "Model: large_xlarge | Predictions saved\n",
      "Model: large_xlarge | Model saved\n",
      "Model: large_xlarge | Garbage collected, states deleted\n",
      "Model: xlarge_xxlarge | Started loop\n",
      "Model: xlarge_xxlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 102.9705334758255\n",
      "Epoch 1 | Val loss: 0.05552215129137039\n",
      "Epoch 2 | Total train loss: 51.57587034171593\n",
      "Epoch 2 | Val loss: 0.051272954791784286\n",
      "Epoch 3 | Total train loss: 39.72787620249437\n",
      "Epoch 3 | Val loss: 0.0322176031768322\n",
      "Epoch 4 | Total train loss: 28.674369621025107\n",
      "Epoch 4 | Val loss: 0.027280744165182114\n",
      "Epoch 5 | Total train loss: 21.56710041687529\n",
      "Epoch 5 | Val loss: 0.01642719656229019\n",
      "Epoch 6 | Total train loss: 16.505922762295086\n",
      "Epoch 6 | Val loss: 0.010380675084888935\n",
      "Epoch 7 | Total train loss: 16.06738140350899\n",
      "Epoch 7 | Val loss: 0.011317827738821507\n",
      "Epoch 8 | Total train loss: 12.906741729676924\n",
      "Epoch 8 | Val loss: 0.0482964925467968\n",
      "Epoch 9 | Total train loss: 16.40078122344994\n",
      "Epoch 9 | Val loss: 0.010286347009241581\n",
      "Epoch 10 | Total train loss: 11.769083380142547\n",
      "Epoch 10 | Val loss: 0.014158745296299458\n",
      "Epoch 11 | Total train loss: 10.792123724904513\n",
      "Epoch 11 | Val loss: 0.010724830441176891\n",
      "Epoch 12 | Total train loss: 11.047797874982962\n",
      "Epoch 12 | Val loss: 0.017980575561523438\n",
      "Epoch 13 | Total train loss: 9.220948118504111\n",
      "Epoch 13 | Val loss: 0.01057844702154398\n",
      "Epoch 14 | Total train loss: 8.215226687034829\n",
      "Epoch 14 | Val loss: 0.006354069337248802\n",
      "Epoch 15 | Total train loss: 10.832923281387707\n",
      "Epoch 15 | Val loss: 0.014105753041803837\n",
      "Epoch 16 | Total train loss: 8.176163374799216\n",
      "Epoch 16 | Val loss: 0.007425660267472267\n",
      "Epoch 17 | Total train loss: 9.523296610060243\n",
      "Epoch 17 | Val loss: 0.010730856098234653\n",
      "Epoch 18 | Total train loss: 9.113351796899678\n",
      "Epoch 18 | Val loss: 0.008404611609876156\n",
      "Epoch 19 | Total train loss: 8.021617807437906\n",
      "Epoch 19 | Val loss: 0.009735377505421638\n",
      "Epoch 20 | Total train loss: 9.771476060142504\n",
      "Epoch 20 | Val loss: 0.006499238777905703\n",
      "Epoch 21 | Total train loss: 8.068459026347682\n",
      "Epoch 21 | Val loss: 0.005812985822558403\n",
      "Epoch 22 | Total train loss: 7.217280311424247\n",
      "Epoch 22 | Val loss: 0.008382182568311691\n",
      "Epoch 23 | Total train loss: 8.284800780068508\n",
      "Epoch 23 | Val loss: 0.006396670825779438\n",
      "Epoch 24 | Total train loss: 7.1119314380100604\n",
      "Epoch 24 | Val loss: 0.004326953552663326\n",
      "Epoch 25 | Total train loss: 6.9965627924329965\n",
      "Epoch 25 | Val loss: 0.010082581080496311\n",
      "Epoch 26 | Total train loss: 7.558620038455047\n",
      "Epoch 26 | Val loss: 0.006394986994564533\n",
      "Epoch 27 | Total train loss: 5.9198553017574795\n",
      "Epoch 27 | Val loss: 0.006508309859782457\n",
      "Epoch 28 | Total train loss: 7.421469674318587\n",
      "Epoch 28 | Val loss: 0.006007075775414705\n",
      "Epoch 29 | Total train loss: 5.333126885470506\n",
      "Epoch 29 | Val loss: 0.010069032199680805\n",
      "Epoch 30 | Total train loss: 6.828630547834109\n",
      "Epoch 30 | Val loss: 0.004327088128775358\n",
      "Epoch 31 | Total train loss: 6.547827436297894\n",
      "Epoch 31 | Val loss: 0.011544229462742805\n",
      "Epoch 32 | Total train loss: 5.503135403196097\n",
      "Epoch 32 | Val loss: 0.005848465953022242\n",
      "Epoch 33 | Total train loss: 5.35828918416297\n",
      "Epoch 33 | Val loss: 0.009877636097371578\n",
      "Epoch 34 | Total train loss: 7.3913857689533415\n",
      "Epoch 34 | Val loss: 0.005138337146490812\n",
      "Epoch 35 | Total train loss: 6.110895796143382\n",
      "Epoch 35 | Val loss: 0.0030291012953966856\n",
      "Epoch 36 | Total train loss: 5.680130082536948\n",
      "Epoch 36 | Val loss: 0.005323891062289476\n",
      "Epoch 37 | Total train loss: 3.8806400977269107\n",
      "Epoch 37 | Val loss: 0.005842804443091154\n",
      "Epoch 38 | Total train loss: 5.443266284105903\n",
      "Epoch 38 | Val loss: 0.005053883418440819\n",
      "Epoch 39 | Total train loss: 4.920777128086229\n",
      "Epoch 39 | Val loss: 0.004090148024260998\n",
      "Epoch 40 | Total train loss: 4.15786786238283\n",
      "Epoch 40 | Val loss: 0.007739997934550047\n",
      "Epoch 41 | Total train loss: 6.911164952589445\n",
      "Epoch 41 | Val loss: 0.006708096247166395\n",
      "Epoch 42 | Total train loss: 4.5771578095206\n",
      "Epoch 42 | Val loss: 0.003669850993901491\n",
      "Epoch 43 | Total train loss: 5.984589729069967\n",
      "Epoch 43 | Val loss: 0.0036995757836848497\n",
      "Epoch 44 | Total train loss: 5.446862958546944\n",
      "Epoch 44 | Val loss: 0.005948775913566351\n",
      "Epoch 45 | Total train loss: 4.782653313080118\n",
      "Epoch 45 | Val loss: 0.0028684029821306467\n",
      "Epoch 46 | Total train loss: 4.103562823095665\n",
      "Epoch 46 | Val loss: 0.0032876301556825638\n",
      "Epoch 47 | Total train loss: 4.026354709872976\n",
      "Epoch 47 | Val loss: 0.004399399273097515\n",
      "Epoch 48 | Total train loss: 5.350843679062872\n",
      "Epoch 48 | Val loss: 0.01039179041981697\n",
      "Epoch 49 | Total train loss: 4.302553138372616\n",
      "Epoch 49 | Val loss: 0.0022611506283283234\n",
      "Epoch 50 | Total train loss: 4.22551467748724\n",
      "Epoch 50 | Val loss: 0.004543154034763575\n",
      "Epoch 51 | Total train loss: 5.481036017044062\n",
      "Epoch 51 | Val loss: 0.0034581951331347227\n",
      "Epoch 52 | Total train loss: 4.838978699056412\n",
      "Epoch 52 | Val loss: 0.0038635036908090115\n",
      "Epoch 53 | Total train loss: 5.030671039852791\n",
      "Epoch 53 | Val loss: 0.003276916453614831\n",
      "Epoch 54 | Total train loss: 5.225724911952\n",
      "Epoch 54 | Val loss: 0.0031327379401773214\n",
      "Epoch 55 | Total train loss: 4.3910280986501675\n",
      "Epoch 55 | Val loss: 0.004422750789672136\n",
      "Epoch 56 | Total train loss: 4.588291873727144\n",
      "Epoch 56 | Val loss: 0.006421638652682304\n",
      "Epoch 57 | Total train loss: 4.008988183586496\n",
      "Epoch 57 | Val loss: 0.00537098990753293\n",
      "Epoch 58 | Total train loss: 4.388275709721597\n",
      "Epoch 58 | Val loss: 0.0041823917999863625\n",
      "Epoch 59 | Total train loss: 3.506649901854871\n",
      "Epoch 59 | Val loss: 0.003965272102504969\n",
      "Epoch 60 | Total train loss: 4.080545252826255\n",
      "Epoch 60 | Val loss: 0.0025023918133229017\n",
      "Epoch 61 | Total train loss: 4.472275143743332\n",
      "Epoch 61 | Val loss: 0.0033566057682037354\n",
      "Epoch 62 | Total train loss: 3.335473728826514\n",
      "Epoch 62 | Val loss: 0.003935317508876324\n",
      "Epoch 63 | Total train loss: 3.815716145925336\n",
      "Epoch 63 | Val loss: 0.003944661933928728\n",
      "Epoch 64 | Total train loss: 3.7515402172994072\n",
      "Epoch 64 | Val loss: 0.00286356289871037\n",
      "Epoch 65 | Total train loss: 3.893396823244302\n",
      "Epoch 65 | Val loss: 0.0048947446048259735\n",
      "Epoch 66 | Total train loss: 3.5890683561356695\n",
      "Epoch 66 | Val loss: 0.005094015505164862\n",
      "Epoch 67 | Total train loss: 4.276947020219723\n",
      "Epoch 67 | Val loss: 0.002397683449089527\n",
      "Epoch 68 | Total train loss: 3.403190717238715\n",
      "Epoch 68 | Val loss: 0.002376020886003971\n",
      "Epoch 69 | Total train loss: 3.5643273097377914\n",
      "Epoch 69 | Val loss: 0.0032041443046182394\n",
      "Epoch 70 | Total train loss: 3.0727644247847365\n",
      "Epoch 70 | Val loss: 0.0036793136969208717\n",
      "Epoch 71 | Total train loss: 3.194697250976759\n",
      "Epoch 71 | Val loss: 0.00216768984682858\n",
      "Epoch 72 | Total train loss: 3.6021564720684864\n",
      "Epoch 72 | Val loss: 0.003924235701560974\n",
      "Epoch 73 | Total train loss: 2.643432130633755\n",
      "Epoch 73 | Val loss: 0.005477030295878649\n",
      "Epoch 74 | Total train loss: 3.556736042765806\n",
      "Epoch 74 | Val loss: 0.0069762845523655415\n",
      "Epoch 75 | Total train loss: 3.4583640250253893\n",
      "Epoch 75 | Val loss: 0.0030166669748723507\n",
      "Epoch 76 | Total train loss: 3.894128944521867\n",
      "Epoch 76 | Val loss: 0.004037756007164717\n",
      "Epoch 77 | Total train loss: 3.549719358223797\n",
      "Epoch 77 | Val loss: 0.002797182882204652\n",
      "Epoch 78 | Total train loss: 3.634579022909932\n",
      "Epoch 78 | Val loss: 0.005102119874209166\n",
      "Epoch 79 | Total train loss: 2.8321898718099874\n",
      "Epoch 79 | Val loss: 0.0035575206857174635\n",
      "Epoch 80 | Total train loss: 3.3516081678800447\n",
      "Epoch 80 | Val loss: 0.00223570060916245\n",
      "Epoch 81 | Total train loss: 3.569494723229923\n",
      "Epoch 81 | Val loss: 0.0011230065720155835\n",
      "Epoch 82 | Total train loss: 3.270712735275424\n",
      "Epoch 82 | Val loss: 0.0031719745602458715\n",
      "Epoch 83 | Total train loss: 3.7318926431912587\n",
      "Epoch 83 | Val loss: 0.002722973469644785\n",
      "Epoch 84 | Total train loss: 2.986417431519044\n",
      "Epoch 84 | Val loss: 0.0017369425622746348\n",
      "Epoch 85 | Total train loss: 2.9627945864196477\n",
      "Epoch 85 | Val loss: 0.005250608082860708\n",
      "Epoch 86 | Total train loss: 3.3058533202892133\n",
      "Epoch 86 | Val loss: 0.004180250223726034\n",
      "Epoch 87 | Total train loss: 2.8135630403488676\n",
      "Epoch 87 | Val loss: 0.0038466318510472775\n",
      "Epoch 88 | Total train loss: 2.7828623397255114\n",
      "Epoch 88 | Val loss: 0.006727277766913176\n",
      "Epoch 89 | Total train loss: 3.4074649085721376\n",
      "Epoch 89 | Val loss: 0.004488768521696329\n",
      "Epoch 90 | Total train loss: 4.172445204051243\n",
      "Epoch 90 | Val loss: 0.0021713953465223312\n",
      "Epoch 91 | Total train loss: 3.495800540309574\n",
      "Epoch 91 | Val loss: 0.0019886272493749857\n",
      "Epoch 92 | Total train loss: 3.174137987190875\n",
      "Epoch 92 | Val loss: 0.002352484269067645\n",
      "Epoch 93 | Total train loss: 3.104929337448766\n",
      "Epoch 93 | Val loss: 0.002868076553568244\n",
      "Epoch 94 | Total train loss: 3.019513912840864\n",
      "Epoch 94 | Val loss: 0.0038059393409639597\n",
      "Epoch 95 | Total train loss: 2.2156555992374933\n",
      "Epoch 95 | Val loss: 0.003097989596426487\n",
      "Epoch 96 | Total train loss: 3.2706297247907514\n",
      "Epoch 96 | Val loss: 0.0034858707804232836\n",
      "Epoch 97 | Total train loss: 2.9066691444729713\n",
      "Epoch 97 | Val loss: 0.0033741227816790342\n",
      "Epoch 98 | Total train loss: 3.3018956466906957\n",
      "Epoch 98 | Val loss: 0.0027477184776216745\n",
      "Epoch 99 | Total train loss: 2.6717062586288023\n",
      "Epoch 99 | Val loss: 0.0025310141500085592\n",
      "Epoch 100 | Total train loss: 2.7482669762324576\n",
      "Epoch 100 | Val loss: 0.0049123335629701614\n",
      "Test loss: 0.004301073960959911\n",
      "Model: xlarge_xxlarge | Training complete\n",
      "Model: xlarge_xxlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xlarge_xxlarge | Plots made and saved\n",
      "Model: xlarge_xxlarge | Predictions saved\n",
      "Model: xlarge_xxlarge | Model saved\n",
      "Model: xlarge_xxlarge | Garbage collected, states deleted\n",
      "Model: xxlarge_xxxlarge | Started loop\n",
      "Model: xxlarge_xxxlarge | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 105.78590279682976\n",
      "Epoch 1 | Val loss: 0.055434539914131165\n",
      "Epoch 2 | Total train loss: 47.13117021398284\n",
      "Epoch 2 | Val loss: 0.03875301778316498\n",
      "Epoch 3 | Total train loss: 39.11540219048038\n",
      "Epoch 3 | Val loss: 0.030596435070037842\n",
      "Epoch 4 | Total train loss: 29.906229310421622\n",
      "Epoch 4 | Val loss: 0.022430438548326492\n",
      "Epoch 5 | Total train loss: 20.177523127388667\n",
      "Epoch 5 | Val loss: 0.02269711159169674\n",
      "Epoch 6 | Total train loss: 16.60554934480251\n",
      "Epoch 6 | Val loss: 0.008897418156266212\n",
      "Epoch 7 | Total train loss: 12.66520776867992\n",
      "Epoch 7 | Val loss: 0.021900396794080734\n",
      "Epoch 8 | Total train loss: 15.562676001596628\n",
      "Epoch 8 | Val loss: 0.009164465591311455\n",
      "Epoch 9 | Total train loss: 10.4537395253069\n",
      "Epoch 9 | Val loss: 0.009237485937774181\n",
      "Epoch 10 | Total train loss: 9.826359021008557\n",
      "Epoch 10 | Val loss: 0.005500118713825941\n",
      "Epoch 11 | Total train loss: 11.195588313553799\n",
      "Epoch 11 | Val loss: 0.010956507176160812\n",
      "Epoch 12 | Total train loss: 12.52524599773733\n",
      "Epoch 12 | Val loss: 0.012918536551296711\n",
      "Epoch 13 | Total train loss: 8.525122455503606\n",
      "Epoch 13 | Val loss: 0.009636851027607918\n",
      "Epoch 14 | Total train loss: 10.910673712460266\n",
      "Epoch 14 | Val loss: 0.007223053369671106\n",
      "Epoch 15 | Total train loss: 9.30487989772655\n",
      "Epoch 15 | Val loss: 0.010198712348937988\n",
      "Epoch 16 | Total train loss: 8.266456914341688\n",
      "Epoch 16 | Val loss: 0.008012838661670685\n",
      "Epoch 17 | Total train loss: 9.887839574303143\n",
      "Epoch 17 | Val loss: 0.00993251707404852\n",
      "Epoch 18 | Total train loss: 7.572514711626354\n",
      "Epoch 18 | Val loss: 0.008078102022409439\n",
      "Epoch 19 | Total train loss: 8.953457520870415\n",
      "Epoch 19 | Val loss: 0.006566587369889021\n",
      "Epoch 20 | Total train loss: 8.333669800916596\n",
      "Epoch 20 | Val loss: 0.006683587562292814\n",
      "Epoch 21 | Total train loss: 8.945003615893256\n",
      "Epoch 21 | Val loss: 0.00657248729839921\n",
      "Epoch 22 | Total train loss: 7.698026157199308\n",
      "Epoch 22 | Val loss: 0.005622202064841986\n",
      "Epoch 23 | Total train loss: 6.15478982492391\n",
      "Epoch 23 | Val loss: 0.009758186526596546\n",
      "Epoch 24 | Total train loss: 7.456977992084603\n",
      "Epoch 24 | Val loss: 0.003778136568143964\n",
      "Epoch 25 | Total train loss: 6.550799272482891\n",
      "Epoch 25 | Val loss: 0.007498667575418949\n",
      "Epoch 26 | Total train loss: 6.295939809567358\n",
      "Epoch 26 | Val loss: 0.005780341569334269\n",
      "Epoch 27 | Total train loss: 6.407129025120412\n",
      "Epoch 27 | Val loss: 0.004876493941992521\n",
      "Epoch 28 | Total train loss: 6.90870943474556\n",
      "Epoch 28 | Val loss: 0.006676591467112303\n",
      "Epoch 29 | Total train loss: 6.247013661418464\n",
      "Epoch 29 | Val loss: 0.004812085069715977\n",
      "Epoch 30 | Total train loss: 6.486366990373938\n",
      "Epoch 30 | Val loss: 0.004885093308985233\n",
      "Epoch 31 | Total train loss: 6.235855525608827\n",
      "Epoch 31 | Val loss: 0.005564253311604261\n",
      "Epoch 32 | Total train loss: 5.660147624264027\n",
      "Epoch 32 | Val loss: 0.0071700322441756725\n",
      "Epoch 33 | Total train loss: 4.295748443277489\n",
      "Epoch 33 | Val loss: 0.0055410233326256275\n",
      "Epoch 34 | Total train loss: 7.034711825423301\n",
      "Epoch 34 | Val loss: 0.005346895195543766\n",
      "Epoch 35 | Total train loss: 5.424229397271461\n",
      "Epoch 35 | Val loss: 0.005697693210095167\n",
      "Epoch 36 | Total train loss: 5.940484481718158\n",
      "Epoch 36 | Val loss: 0.005435741972178221\n",
      "Epoch 37 | Total train loss: 6.039697580470829\n",
      "Epoch 37 | Val loss: 0.0036239251494407654\n",
      "Epoch 38 | Total train loss: 5.501121158961261\n",
      "Epoch 38 | Val loss: 0.004924721084535122\n",
      "Epoch 39 | Total train loss: 4.389545669813259\n",
      "Epoch 39 | Val loss: 0.00850702915340662\n",
      "Epoch 40 | Total train loss: 6.028508593040215\n",
      "Epoch 40 | Val loss: 0.0039061030838638544\n",
      "Epoch 41 | Total train loss: 4.548566327866979\n",
      "Epoch 41 | Val loss: 0.006172581110149622\n",
      "Epoch 42 | Total train loss: 4.42612743873724\n",
      "Epoch 42 | Val loss: 0.007672952953726053\n",
      "Epoch 43 | Total train loss: 5.035454988255424\n",
      "Epoch 43 | Val loss: 0.004904329776763916\n",
      "Epoch 44 | Total train loss: 4.5276185369746145\n",
      "Epoch 44 | Val loss: 0.004544938914477825\n",
      "Epoch 45 | Total train loss: 4.38895762537652\n",
      "Epoch 45 | Val loss: 0.004220874048769474\n",
      "Epoch 46 | Total train loss: 4.7535204230828185\n",
      "Epoch 46 | Val loss: 0.002305708359926939\n",
      "Epoch 47 | Total train loss: 5.241358791184439\n",
      "Epoch 47 | Val loss: 0.00718883378431201\n",
      "Epoch 48 | Total train loss: 4.8820683201665815\n",
      "Epoch 48 | Val loss: 0.010155857540667057\n",
      "Epoch 49 | Total train loss: 4.269253026178092\n",
      "Epoch 49 | Val loss: 0.004074892029166222\n",
      "Epoch 50 | Total train loss: 4.208906489992614\n",
      "Epoch 50 | Val loss: 0.0020388790871948004\n",
      "Epoch 51 | Total train loss: 4.404301410571378\n",
      "Epoch 51 | Val loss: 0.0033364836126565933\n",
      "Epoch 52 | Total train loss: 4.641405390858608\n",
      "Epoch 52 | Val loss: 0.004245880525559187\n",
      "Epoch 53 | Total train loss: 4.239291471965515\n",
      "Epoch 53 | Val loss: 0.004595565143972635\n",
      "Epoch 54 | Total train loss: 4.7713811125642565\n",
      "Epoch 54 | Val loss: 0.003738459898158908\n",
      "Epoch 55 | Total train loss: 3.3024163888014986\n",
      "Epoch 55 | Val loss: 0.0028895651921629906\n",
      "Epoch 56 | Total train loss: 5.3393322365644735\n",
      "Epoch 56 | Val loss: 0.006458925548940897\n",
      "Epoch 57 | Total train loss: 3.424860684376199\n",
      "Epoch 57 | Val loss: 0.003218758851289749\n",
      "Epoch 58 | Total train loss: 4.765238523982145\n",
      "Epoch 58 | Val loss: 0.002230788813903928\n",
      "Epoch 59 | Total train loss: 4.017620652126851\n",
      "Epoch 59 | Val loss: 0.006004377268254757\n",
      "Epoch 60 | Total train loss: 4.364385269597278\n",
      "Epoch 60 | Val loss: 0.005888568237423897\n",
      "Epoch 61 | Total train loss: 4.096110719581361\n",
      "Epoch 61 | Val loss: 0.0029109891038388014\n",
      "Epoch 62 | Total train loss: 3.885308255155678\n",
      "Epoch 62 | Val loss: 0.0029559030663222075\n",
      "Epoch 63 | Total train loss: 4.633981481760856\n",
      "Epoch 63 | Val loss: 0.004618954844772816\n",
      "Epoch 64 | Total train loss: 3.994726620471141\n",
      "Epoch 64 | Val loss: 0.002297158818691969\n",
      "Epoch 65 | Total train loss: 26736474.488735527\n",
      "Epoch 65 | Val loss: 10.918848037719727\n",
      "Epoch 66 | Total train loss: 4.296719452728894\n",
      "Epoch 66 | Val loss: 0.0031641842797398567\n",
      "Epoch 67 | Total train loss: 3.7990051883997467\n",
      "Epoch 67 | Val loss: 0.0017809983110055327\n",
      "Epoch 68 | Total train loss: 2.8066611742189025\n",
      "Epoch 68 | Val loss: 0.0029337499290704727\n",
      "Epoch 69 | Total train loss: 3.113315336491212\n",
      "Epoch 69 | Val loss: 0.00273254350759089\n",
      "Epoch 70 | Total train loss: 2.7581821018381447\n",
      "Epoch 70 | Val loss: 0.0020316799636930227\n",
      "Epoch 71 | Total train loss: 3.0284400039695925\n",
      "Epoch 71 | Val loss: 0.0024993536062538624\n",
      "Epoch 72 | Total train loss: 3.3084408779095043\n",
      "Epoch 72 | Val loss: 0.0020728418603539467\n",
      "Epoch 73 | Total train loss: 2.6055362286581953\n",
      "Epoch 73 | Val loss: 0.002387220971286297\n",
      "Epoch 74 | Total train loss: 2.396322210227538\n",
      "Epoch 74 | Val loss: 0.0008562565199099481\n",
      "Epoch 75 | Total train loss: 2.613815946685861\n",
      "Epoch 75 | Val loss: 0.0020373968873173\n",
      "Epoch 76 | Total train loss: 2.3715383456798804\n",
      "Epoch 76 | Val loss: 0.001511194510385394\n",
      "Epoch 77 | Total train loss: 2.2021954196736715\n",
      "Epoch 77 | Val loss: 0.0021671801805496216\n",
      "Epoch 78 | Total train loss: 2.0698224916772574\n",
      "Epoch 78 | Val loss: 0.0020830470602959394\n",
      "Epoch 79 | Total train loss: 2.7641862213336594\n",
      "Epoch 79 | Val loss: 0.0018628275720402598\n",
      "Epoch 80 | Total train loss: 2.4382400896168974\n",
      "Epoch 80 | Val loss: 0.003276778617873788\n",
      "Epoch 81 | Total train loss: 2.296387895882077\n",
      "Epoch 81 | Val loss: 0.002098244149237871\n",
      "Epoch 82 | Total train loss: 2.2413929684058616\n",
      "Epoch 82 | Val loss: 0.0014861265663057566\n",
      "Epoch 83 | Total train loss: 2.158386638750585\n",
      "Epoch 83 | Val loss: 0.0023965395521372557\n",
      "Epoch 84 | Total train loss: 1.7299281093631294\n",
      "Epoch 84 | Val loss: 0.0016297256806865335\n",
      "Epoch 85 | Total train loss: 2.0202759203190794\n",
      "Epoch 85 | Val loss: 0.002292933873832226\n",
      "Epoch 86 | Total train loss: 1.9349119785948972\n",
      "Epoch 86 | Val loss: 0.002209938131272793\n",
      "Epoch 87 | Total train loss: 1.9914423863042003\n",
      "Epoch 87 | Val loss: 0.0017182446317747235\n",
      "Epoch 88 | Total train loss: 2.233941425973825\n",
      "Epoch 88 | Val loss: 0.001975273247808218\n",
      "Epoch 89 | Total train loss: 2.511849800414579\n",
      "Epoch 89 | Val loss: 0.001741552958264947\n",
      "Epoch 90 | Total train loss: 1.9404507255756585\n",
      "Epoch 90 | Val loss: 0.0031460695900022984\n",
      "Epoch 91 | Total train loss: 2.06026084263317\n",
      "Epoch 91 | Val loss: 0.002778528956696391\n",
      "Epoch 92 | Total train loss: 2.4810677880330445\n",
      "Epoch 92 | Val loss: 0.0021825931034982204\n",
      "Epoch 93 | Total train loss: 2.060082465270966\n",
      "Epoch 93 | Val loss: 0.001548222848214209\n",
      "Epoch 94 | Total train loss: 2.6171179254093886\n",
      "Epoch 94 | Val loss: 0.0025437616277486086\n",
      "Epoch 95 | Total train loss: 4.5366784278332375\n",
      "Epoch 95 | Val loss: 0.0019321305444464087\n",
      "Epoch 96 | Total train loss: 2.627145001954432\n",
      "Epoch 96 | Val loss: 0.004478733520954847\n",
      "Epoch 97 | Total train loss: 3.916240918058037\n",
      "Epoch 97 | Val loss: 0.0033049487974494696\n",
      "Epoch 98 | Total train loss: 3.58876345889297\n",
      "Epoch 98 | Val loss: 0.004768785089254379\n",
      "Epoch 99 | Total train loss: 3.617646385498375\n",
      "Epoch 99 | Val loss: 0.002135510090738535\n",
      "Epoch 100 | Total train loss: 2.8736748923140567\n",
      "Epoch 100 | Val loss: 0.0031287209130823612\n",
      "Test loss: 0.002997736679390073\n",
      "Model: xxlarge_xxxlarge | Training complete\n",
      "Model: xxlarge_xxxlarge | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxlarge_xxxlarge | Plots made and saved\n",
      "Model: xxlarge_xxxlarge | Predictions saved\n",
      "Model: xxlarge_xxxlarge | Model saved\n",
      "Model: xxlarge_xxxlarge | Garbage collected, states deleted\n",
      "Model: xxxlarge_sub_enormous | Started loop\n",
      "Model: xxxlarge_sub_enormous | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 86.33960689878586\n",
      "Epoch 1 | Val loss: 0.0536770261824131\n",
      "Epoch 2 | Total train loss: 58.29356859510881\n",
      "Epoch 2 | Val loss: 0.06157128885388374\n",
      "Epoch 3 | Total train loss: 45.91871267890383\n",
      "Epoch 3 | Val loss: 0.03823765739798546\n",
      "Epoch 4 | Total train loss: 29.859686585919917\n",
      "Epoch 4 | Val loss: 0.020770495757460594\n",
      "Epoch 5 | Total train loss: 18.32704366080725\n",
      "Epoch 5 | Val loss: 0.015587194822728634\n",
      "Epoch 6 | Total train loss: 14.742211624494018\n",
      "Epoch 6 | Val loss: 0.010582644492387772\n",
      "Epoch 7 | Total train loss: 14.915451755016875\n",
      "Epoch 7 | Val loss: 0.008408441208302975\n",
      "Epoch 8 | Total train loss: 12.190720802941087\n",
      "Epoch 8 | Val loss: 0.010627643205225468\n",
      "Epoch 9 | Total train loss: 13.768650610543318\n",
      "Epoch 9 | Val loss: 0.009123729541897774\n",
      "Epoch 10 | Total train loss: 13.309594718104563\n",
      "Epoch 10 | Val loss: 0.00881970301270485\n",
      "Epoch 11 | Total train loss: 11.04034267145289\n",
      "Epoch 11 | Val loss: 0.01115593034774065\n",
      "Epoch 12 | Total train loss: 10.780751697110645\n",
      "Epoch 12 | Val loss: 0.0057646743953228\n",
      "Epoch 13 | Total train loss: 10.672809796026286\n",
      "Epoch 13 | Val loss: 0.00590122863650322\n",
      "Epoch 14 | Total train loss: 10.598815916991953\n",
      "Epoch 14 | Val loss: 0.00942208431661129\n",
      "Epoch 15 | Total train loss: 9.376788472112821\n",
      "Epoch 15 | Val loss: 0.01094103418290615\n",
      "Epoch 16 | Total train loss: 8.74291821829729\n",
      "Epoch 16 | Val loss: 0.011206327006220818\n",
      "Epoch 17 | Total train loss: 7.907496823368092\n",
      "Epoch 17 | Val loss: 0.005991047248244286\n",
      "Epoch 18 | Total train loss: 7.507404526369896\n",
      "Epoch 18 | Val loss: 0.007051188964396715\n",
      "Epoch 19 | Total train loss: 7.043723795871756\n",
      "Epoch 19 | Val loss: 0.012825895100831985\n",
      "Epoch 20 | Total train loss: 6.951124775000039\n",
      "Epoch 20 | Val loss: 0.0074720256961882114\n",
      "Epoch 21 | Total train loss: 7.664716705955357\n",
      "Epoch 21 | Val loss: 0.00768384849652648\n",
      "Epoch 22 | Total train loss: 7.92402605860093\n",
      "Epoch 22 | Val loss: 0.005358572117984295\n",
      "Epoch 23 | Total train loss: 6.742499954605137\n",
      "Epoch 23 | Val loss: 0.006622368469834328\n",
      "Epoch 24 | Total train loss: 175831678985.5467\n",
      "Epoch 24 | Val loss: 0.00867569912225008\n",
      "Epoch 25 | Total train loss: 6.069354324807819\n",
      "Epoch 25 | Val loss: 0.005566112231463194\n",
      "Epoch 26 | Total train loss: 6.655880006564928\n",
      "Epoch 26 | Val loss: 0.010685637593269348\n",
      "Epoch 27 | Total train loss: 6.452893677590055\n",
      "Epoch 27 | Val loss: 0.006681090220808983\n",
      "Epoch 28 | Total train loss: 6.341891500919246\n",
      "Epoch 28 | Val loss: 0.013733809813857079\n",
      "Epoch 29 | Total train loss: 4.996549972313289\n",
      "Epoch 29 | Val loss: 0.008024632930755615\n",
      "Epoch 30 | Total train loss: 5.331290939989458\n",
      "Epoch 30 | Val loss: 0.005476111080497503\n",
      "Epoch 31 | Total train loss: 5.8090820489333055\n",
      "Epoch 31 | Val loss: 0.0052052647806704044\n",
      "Epoch 32 | Total train loss: 5.321892545882292\n",
      "Epoch 32 | Val loss: 0.00567593052983284\n",
      "Epoch 33 | Total train loss: 5.468020441674753\n",
      "Epoch 33 | Val loss: 0.005617732182145119\n",
      "Epoch 34 | Total train loss: 5.777880820482551\n",
      "Epoch 34 | Val loss: 0.005242354702204466\n",
      "Epoch 35 | Total train loss: 5.001478079747244\n",
      "Epoch 35 | Val loss: 0.0035138106904923916\n",
      "Epoch 36 | Total train loss: 5.16195391637558\n",
      "Epoch 36 | Val loss: 0.0047911545261740685\n",
      "Epoch 37 | Total train loss: 5.3730957452044095\n",
      "Epoch 37 | Val loss: 0.004870837554335594\n",
      "Epoch 38 | Total train loss: 5.714471427283911\n",
      "Epoch 38 | Val loss: 0.0058487169444561005\n",
      "Epoch 39 | Total train loss: 5.517077257084992\n",
      "Epoch 39 | Val loss: 0.003845569444820285\n",
      "Epoch 40 | Total train loss: 4.842347330753455\n",
      "Epoch 40 | Val loss: 0.004677025135606527\n",
      "Epoch 41 | Total train loss: 4.482866303404535\n",
      "Epoch 41 | Val loss: 0.0048698983155190945\n",
      "Epoch 42 | Total train loss: 4.820299381074165\n",
      "Epoch 42 | Val loss: 0.005694383755326271\n",
      "Epoch 43 | Total train loss: 6.226250386602487\n",
      "Epoch 43 | Val loss: 0.0045541515573859215\n",
      "Epoch 44 | Total train loss: 5.250406939039294\n",
      "Epoch 44 | Val loss: 0.005611984990537167\n",
      "Epoch 45 | Total train loss: 4.234569357559394\n",
      "Epoch 45 | Val loss: 0.004135699477046728\n",
      "Epoch 46 | Total train loss: 6.044976850448734\n",
      "Epoch 46 | Val loss: 0.004738652613013983\n",
      "Epoch 47 | Total train loss: 5.433718989184399\n",
      "Epoch 47 | Val loss: 0.004156023263931274\n",
      "Epoch 48 | Total train loss: 4.265233121928816\n",
      "Epoch 48 | Val loss: 0.005833928938955069\n",
      "Epoch 49 | Total train loss: 5.136410365263089\n",
      "Epoch 49 | Val loss: 0.003424982773140073\n",
      "Epoch 50 | Total train loss: 3.980577918649942\n",
      "Epoch 50 | Val loss: 0.002743575256317854\n",
      "Epoch 51 | Total train loss: 4.08655747977302\n",
      "Epoch 51 | Val loss: 0.004154655151069164\n",
      "Epoch 52 | Total train loss: 4.159738193576885\n",
      "Epoch 52 | Val loss: 0.0033393355552107096\n",
      "Epoch 53 | Total train loss: 4.8093557089898695\n",
      "Epoch 53 | Val loss: 0.00644315592944622\n",
      "Epoch 54 | Total train loss: 4.800261168146562\n",
      "Epoch 54 | Val loss: 0.004198324866592884\n",
      "Epoch 55 | Total train loss: 4.636866171217207\n",
      "Epoch 55 | Val loss: 0.005070851184427738\n",
      "Epoch 56 | Total train loss: 4.497631548866707\n",
      "Epoch 56 | Val loss: 0.004511588718742132\n",
      "Epoch 57 | Total train loss: 5.222323652060709\n",
      "Epoch 57 | Val loss: 0.005332237109541893\n",
      "Epoch 58 | Total train loss: 4.834153245694097\n",
      "Epoch 58 | Val loss: 0.004413465969264507\n",
      "Epoch 59 | Total train loss: 4.498715486992637\n",
      "Epoch 59 | Val loss: 0.0067879376001656055\n",
      "Epoch 60 | Total train loss: 5.032889124844473\n",
      "Epoch 60 | Val loss: 0.005240846890956163\n",
      "Epoch 61 | Total train loss: 4.596572595034331\n",
      "Epoch 61 | Val loss: 0.00478243175894022\n",
      "Epoch 62 | Total train loss: 4.16120583978568\n",
      "Epoch 62 | Val loss: 0.004209716804325581\n",
      "Epoch 63 | Total train loss: 5.037338316865089\n",
      "Epoch 63 | Val loss: 0.004586084745824337\n",
      "Epoch 64 | Total train loss: 3.2254218711314024\n",
      "Epoch 64 | Val loss: 0.003306165337562561\n",
      "Epoch 65 | Total train loss: 5.1653299908206805\n",
      "Epoch 65 | Val loss: 0.004082351457327604\n",
      "Epoch 66 | Total train loss: 4.266716759984092\n",
      "Epoch 66 | Val loss: 0.007188623771071434\n",
      "Epoch 67 | Total train loss: 5.869454066157118\n",
      "Epoch 67 | Val loss: 0.00500290235504508\n",
      "Epoch 68 | Total train loss: 4.688404098635999\n",
      "Epoch 68 | Val loss: 0.0040880292654037476\n",
      "Epoch 69 | Total train loss: 5.313489927711657\n",
      "Epoch 69 | Val loss: 0.004275307059288025\n",
      "Epoch 70 | Total train loss: 4.334303966692119\n",
      "Epoch 70 | Val loss: 0.004073434043675661\n",
      "Epoch 71 | Total train loss: 4.701525183565991\n",
      "Epoch 71 | Val loss: 0.004477930720895529\n",
      "Epoch 72 | Total train loss: 6.051914604701608\n",
      "Epoch 72 | Val loss: 0.00448249327018857\n",
      "Epoch 73 | Total train loss: 6.41010813323885\n",
      "Epoch 73 | Val loss: 0.010810348205268383\n",
      "Epoch 74 | Total train loss: 6.4648429171768385\n",
      "Epoch 74 | Val loss: 0.004224020522087812\n",
      "Epoch 75 | Total train loss: 6.383982546997231\n",
      "Epoch 75 | Val loss: 0.018920863047242165\n",
      "Epoch 76 | Total train loss: 5.524459998218219\n",
      "Epoch 76 | Val loss: 0.0031875409185886383\n",
      "Epoch 77 | Total train loss: 5.18608583631476\n",
      "Epoch 77 | Val loss: 0.00487600639462471\n",
      "Epoch 78 | Total train loss: 5.579722191860128\n",
      "Epoch 78 | Val loss: 0.003768661990761757\n",
      "Epoch 79 | Total train loss: 5.541432142048734\n",
      "Epoch 79 | Val loss: 0.02487686276435852\n",
      "Epoch 80 | Total train loss: 6.73217142458634\n",
      "Epoch 80 | Val loss: 0.005568129941821098\n",
      "Epoch 81 | Total train loss: 5.071659768965219\n",
      "Epoch 81 | Val loss: 0.0028251963667571545\n",
      "Epoch 82 | Total train loss: 4.609527302012111\n",
      "Epoch 82 | Val loss: 0.0028246205765753984\n",
      "Epoch 83 | Total train loss: 5.040919998798245\n",
      "Epoch 83 | Val loss: 0.0030976415146142244\n",
      "Epoch 84 | Total train loss: 4.659407635952903\n",
      "Epoch 84 | Val loss: 0.005372884217649698\n",
      "Epoch 85 | Total train loss: 5.032479134267334\n",
      "Epoch 85 | Val loss: 0.009666339494287968\n",
      "Epoch 86 | Total train loss: 135641366.93484262\n",
      "Epoch 86 | Val loss: 0.004953040741384029\n",
      "Epoch 87 | Total train loss: 4.2099325003771355\n",
      "Epoch 87 | Val loss: 0.004758918192237616\n",
      "Epoch 88 | Total train loss: 3.4140963047344712\n",
      "Epoch 88 | Val loss: 0.00565479788929224\n",
      "Epoch 89 | Total train loss: 4.312267253786445\n",
      "Epoch 89 | Val loss: 0.0032513109035789967\n",
      "Epoch 90 | Total train loss: 4.734733803264817\n",
      "Epoch 90 | Val loss: 0.0033141756430268288\n",
      "Epoch 91 | Total train loss: 3.639940939881825\n",
      "Epoch 91 | Val loss: 0.003315690206363797\n",
      "Epoch 92 | Total train loss: 3.5194841281757476\n",
      "Epoch 92 | Val loss: 0.0020375654567033052\n",
      "Epoch 93 | Total train loss: 3.2497023766295285\n",
      "Epoch 93 | Val loss: 0.002607817994430661\n",
      "Epoch 94 | Total train loss: 3.970236142701083\n",
      "Epoch 94 | Val loss: 0.002717811381444335\n",
      "Epoch 95 | Total train loss: 3.263555761050725\n",
      "Epoch 95 | Val loss: 0.0033665618393570185\n",
      "Epoch 96 | Total train loss: 4.331695217527113\n",
      "Epoch 96 | Val loss: 0.00292833405546844\n",
      "Epoch 97 | Total train loss: 3.410543609725323\n",
      "Epoch 97 | Val loss: 0.004160459153354168\n",
      "Epoch 98 | Total train loss: 2.721433982162125\n",
      "Epoch 98 | Val loss: 0.003309606807306409\n",
      "Epoch 99 | Total train loss: 2.4032065823954305\n",
      "Epoch 99 | Val loss: 0.003334788605570793\n",
      "Epoch 100 | Total train loss: 2.3577330242228527\n",
      "Epoch 100 | Val loss: 0.0031421263702213764\n",
      "Test loss: 0.0026630754582583904\n",
      "Model: xxxlarge_sub_enormous | Training complete\n",
      "Model: xxxlarge_sub_enormous | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: xxxlarge_sub_enormous | Plots made and saved\n",
      "Model: xxxlarge_sub_enormous | Predictions saved\n",
      "Model: xxxlarge_sub_enormous | Model saved\n",
      "Model: xxxlarge_sub_enormous | Garbage collected, states deleted\n",
      "Model: sub_enormous_enormous | Started loop\n",
      "Model: sub_enormous_enormous | Initialization done for model and optimizer\n",
      "Epoch 1 | Total train loss: 145.99686276947614\n",
      "Epoch 1 | Val loss: 0.05042839050292969\n",
      "Epoch 2 | Total train loss: 46.07217156886509\n",
      "Epoch 2 | Val loss: 0.04168049618601799\n",
      "Epoch 3 | Total train loss: 25.676468170458975\n",
      "Epoch 3 | Val loss: 0.014483262784779072\n",
      "Epoch 4 | Total train loss: 26.770050191142218\n",
      "Epoch 4 | Val loss: 0.030408402904868126\n",
      "Epoch 5 | Total train loss: 17.27579540988927\n",
      "Epoch 5 | Val loss: 0.017069974914193153\n",
      "Epoch 6 | Total train loss: 18.473265007956797\n",
      "Epoch 6 | Val loss: 0.014415767043828964\n",
      "Epoch 7 | Total train loss: 19.003298958830783\n",
      "Epoch 7 | Val loss: 0.02369006723165512\n",
      "Epoch 8 | Total train loss: 13.07735382849205\n",
      "Epoch 8 | Val loss: 0.02557607740163803\n",
      "Epoch 9 | Total train loss: 14.357489376225203\n",
      "Epoch 9 | Val loss: 0.011033552698791027\n",
      "Epoch 10 | Total train loss: 13.31938922956624\n",
      "Epoch 10 | Val loss: 0.00861064251512289\n",
      "Epoch 11 | Total train loss: 15.58835536278093\n",
      "Epoch 11 | Val loss: 0.014077174477279186\n",
      "Epoch 12 | Total train loss: 13.251424710909532\n",
      "Epoch 12 | Val loss: 0.008120609447360039\n",
      "Epoch 13 | Total train loss: 12.548656987602953\n",
      "Epoch 13 | Val loss: 0.010152827948331833\n",
      "Epoch 14 | Total train loss: 10.928213929490994\n",
      "Epoch 14 | Val loss: 0.01245321985334158\n",
      "Epoch 15 | Total train loss: 11.998191568037328\n",
      "Epoch 15 | Val loss: 0.008157606236636639\n",
      "Epoch 16 | Total train loss: 9.079762155711705\n",
      "Epoch 16 | Val loss: 0.006995924282819033\n",
      "Epoch 17 | Total train loss: 9.682049808469628\n",
      "Epoch 17 | Val loss: 0.01382956188172102\n",
      "Epoch 18 | Total train loss: 10.669704775865284\n",
      "Epoch 18 | Val loss: 0.007406262680888176\n",
      "Epoch 19 | Total train loss: 8.031018097171454\n",
      "Epoch 19 | Val loss: 0.00602783914655447\n",
      "Epoch 20 | Total train loss: 9.829075440518977\n",
      "Epoch 20 | Val loss: 0.007593558635562658\n",
      "Epoch 21 | Total train loss: 7.944369773685594\n",
      "Epoch 21 | Val loss: 0.01786373183131218\n",
      "Epoch 22 | Total train loss: 7.30822095601161\n",
      "Epoch 22 | Val loss: 0.007554695010185242\n",
      "Epoch 23 | Total train loss: 6.500651600871379\n",
      "Epoch 23 | Val loss: 0.008721248246729374\n",
      "Epoch 24 | Total train loss: 27937058.599030487\n",
      "Epoch 24 | Val loss: 0.012571150436997414\n",
      "Epoch 25 | Total train loss: 8.901760100921592\n",
      "Epoch 25 | Val loss: 0.010727682150900364\n",
      "Epoch 26 | Total train loss: 5.254831749454297\n",
      "Epoch 26 | Val loss: 0.007543486077338457\n",
      "Epoch 27 | Total train loss: 5.6437653147736455\n",
      "Epoch 27 | Val loss: 0.006196528673171997\n",
      "Epoch 28 | Total train loss: 5.1037764854154375\n",
      "Epoch 28 | Val loss: 0.004402278456836939\n",
      "Epoch 29 | Total train loss: 5.42551278267365\n",
      "Epoch 29 | Val loss: 0.004681820049881935\n",
      "Epoch 30 | Total train loss: 5.225403959957021\n",
      "Epoch 30 | Val loss: 0.0031104215886443853\n",
      "Epoch 31 | Total train loss: 4.097588364881403\n",
      "Epoch 31 | Val loss: 0.0031706634908914566\n",
      "Epoch 32 | Total train loss: 6.013711536022697\n",
      "Epoch 32 | Val loss: 0.006230780389159918\n",
      "Epoch 33 | Total train loss: 4.753434188704432\n",
      "Epoch 33 | Val loss: 0.004050191957503557\n",
      "Epoch 34 | Total train loss: 4.1914388565043055\n",
      "Epoch 34 | Val loss: 0.005579627119004726\n",
      "Epoch 35 | Total train loss: 3.9675044318488517\n",
      "Epoch 35 | Val loss: 0.0051378607749938965\n",
      "Epoch 36 | Total train loss: 4.789767753188542\n",
      "Epoch 36 | Val loss: 0.00430320855230093\n",
      "Epoch 37 | Total train loss: 3.7666032578243858\n",
      "Epoch 37 | Val loss: 0.004014760721474886\n",
      "Epoch 38 | Total train loss: 3.42859185293446\n",
      "Epoch 38 | Val loss: 0.004336906131356955\n",
      "Epoch 39 | Total train loss: 4.397122726333009\n",
      "Epoch 39 | Val loss: 0.0027355332858860493\n",
      "Epoch 40 | Total train loss: 2.99331155002551\n",
      "Epoch 40 | Val loss: 0.0049109626561403275\n",
      "Epoch 41 | Total train loss: 4.065851588471162\n",
      "Epoch 41 | Val loss: 0.0036529735662043095\n",
      "Epoch 42 | Total train loss: 4.400449496125816\n",
      "Epoch 42 | Val loss: 0.011215265840291977\n",
      "Epoch 43 | Total train loss: 4.483902278743074\n",
      "Epoch 43 | Val loss: 0.006636302452534437\n",
      "Epoch 44 | Total train loss: 1.3565368334768873e+28\n",
      "Epoch 44 | Val loss: 2.152174021797763e+25\n",
      "Epoch 45 | Total train loss: 2.375139290445539e+28\n",
      "Epoch 45 | Val loss: 3.0003358299335003e+24\n",
      "Epoch 46 | Total train loss: 1.6753832999469081e+28\n",
      "Epoch 46 | Val loss: 2.672123634799979e+25\n",
      "Epoch 47 | Total train loss: 1.7296175194885477e+28\n",
      "Epoch 47 | Val loss: 1.682496167498123e+25\n",
      "Epoch 48 | Total train loss: 1.1397575093848207e+28\n",
      "Epoch 48 | Val loss: 6.248205636288348e+24\n",
      "Epoch 49 | Total train loss: 1.206624703429741e+28\n",
      "Epoch 49 | Val loss: 2.330058972496456e+25\n",
      "Epoch 50 | Total train loss: 2.7108125415304216e+28\n",
      "Epoch 50 | Val loss: 4.698504697073098e+25\n",
      "Epoch 51 | Total train loss: 1.8968390895203884e+28\n",
      "Epoch 51 | Val loss: 6.625751112019695e+24\n",
      "Epoch 52 | Total train loss: 1.9951685399197503e+28\n",
      "Epoch 52 | Val loss: 1.5937046095737293e+25\n",
      "Epoch 53 | Total train loss: 2.677156520059455e+28\n",
      "Epoch 53 | Val loss: 1.104687649955814e+25\n",
      "Epoch 54 | Total train loss: 2.2518707250025017e+28\n",
      "Epoch 54 | Val loss: 1.2252585668174916e+25\n",
      "Epoch 55 | Total train loss: 8.41790320312042e+27\n",
      "Epoch 55 | Val loss: 5.543299628538932e+24\n",
      "Epoch 56 | Total train loss: 1.3440682111546584e+28\n",
      "Epoch 56 | Val loss: 5.864998163047876e+24\n",
      "Epoch 57 | Total train loss: 8.247110455222229e+27\n",
      "Epoch 57 | Val loss: 9.495875410762146e+24\n",
      "Epoch 58 | Total train loss: 1.0623131453698154e+28\n",
      "Epoch 58 | Val loss: 3.477417630843575e+24\n",
      "Epoch 59 | Total train loss: 9.019464798532788e+27\n",
      "Epoch 59 | Val loss: 6.098364463559862e+24\n",
      "Epoch 60 | Total train loss: 1.3855203024647628e+28\n",
      "Epoch 60 | Val loss: 2.5046144588280705e+24\n",
      "Epoch 61 | Total train loss: 6.81392256507838e+27\n",
      "Epoch 61 | Val loss: 6.080391570224546e+24\n",
      "Epoch 62 | Total train loss: 1.154372947513646e+28\n",
      "Epoch 62 | Val loss: 5.779296319463678e+24\n",
      "Epoch 63 | Total train loss: 7.803070940060239e+27\n",
      "Epoch 63 | Val loss: 4.7705528371546884e+24\n",
      "Epoch 64 | Total train loss: 1.2114219487561933e+28\n",
      "Epoch 64 | Val loss: 1.873289919115297e+24\n",
      "Epoch 65 | Total train loss: 4.890154976070983e+27\n",
      "Epoch 65 | Val loss: 4.0726032695336945e+24\n",
      "Epoch 66 | Total train loss: 1.6171537189596468e+28\n",
      "Epoch 66 | Val loss: 2.4289580923711063e+23\n",
      "Epoch 67 | Total train loss: 2.2706841713917801e+27\n",
      "Epoch 67 | Val loss: 1.967824438461476e+24\n",
      "Epoch 68 | Total train loss: 6.583145434995996e+27\n",
      "Epoch 68 | Val loss: 5.812109041945542e+24\n",
      "Epoch 69 | Total train loss: 2.15040552967593e+27\n",
      "Epoch 69 | Val loss: 3.8599526049704094e+25\n",
      "Epoch 70 | Total train loss: 2.984494079924192e+27\n",
      "Epoch 70 | Val loss: 7.706748185022396e+24\n",
      "Epoch 71 | Total train loss: 3.7333415664051317e+27\n",
      "Epoch 71 | Val loss: 1.2919867523773368e+24\n",
      "Epoch 72 | Total train loss: 2.85390587359334e+27\n",
      "Epoch 72 | Val loss: 2.65558405347919e+24\n",
      "Epoch 73 | Total train loss: 9.163954223096319e+26\n",
      "Epoch 73 | Val loss: 3.330461283892398e+23\n",
      "Epoch 74 | Total train loss: 1.7553749737425937e+27\n",
      "Epoch 74 | Val loss: 7.925016690201179e+23\n",
      "Epoch 75 | Total train loss: 1.1263202860672035e+27\n",
      "Epoch 75 | Val loss: 4.422207980831271e+24\n",
      "Epoch 76 | Total train loss: 1.9619303881867164e+27\n",
      "Epoch 76 | Val loss: 7.522831552534126e+23\n",
      "Epoch 77 | Total train loss: 1.0720077469205074e+27\n",
      "Epoch 77 | Val loss: 3.3459767970982385e+24\n",
      "Epoch 78 | Total train loss: 5.4676856911157674e+26\n",
      "Epoch 78 | Val loss: 1.0543069401710526e+24\n",
      "Epoch 79 | Total train loss: 1.9498064260661103e+27\n",
      "Epoch 79 | Val loss: 8.065448294069816e+23\n",
      "Epoch 80 | Total train loss: 1.470746558609122e+27\n",
      "Epoch 80 | Val loss: 3.7708617161975026e+22\n",
      "Epoch 81 | Total train loss: 4.800577900027434e+26\n",
      "Epoch 81 | Val loss: 9.689323821789752e+23\n",
      "Epoch 82 | Total train loss: 1.0531004943500064e+27\n",
      "Epoch 82 | Val loss: 5.66251848552912e+23\n",
      "Epoch 83 | Total train loss: 1.104818984277263e+27\n",
      "Epoch 83 | Val loss: 2.5810133787915955e+24\n",
      "Epoch 84 | Total train loss: 4.266600980456316e+26\n",
      "Epoch 84 | Val loss: 2.7648788864168952e+23\n",
      "Epoch 85 | Total train loss: 4.5464411308545976e+26\n",
      "Epoch 85 | Val loss: 1.0238265778930091e+24\n",
      "Epoch 86 | Total train loss: 3.575106531678559e+26\n",
      "Epoch 86 | Val loss: 6.382600831389239e+23\n",
      "Epoch 87 | Total train loss: 4.9537767491405385e+26\n",
      "Epoch 87 | Val loss: 1.3687947072734181e+23\n",
      "Epoch 88 | Total train loss: 1.0935383942880649e+27\n",
      "Epoch 88 | Val loss: 6.347244511866634e+22\n",
      "Epoch 89 | Total train loss: 2.53223616431371e+27\n",
      "Epoch 89 | Val loss: 3.644987998124341e+23\n",
      "Epoch 90 | Total train loss: 3.0654804510885804e+27\n",
      "Epoch 90 | Val loss: 1.7900536702262952e+23\n",
      "Epoch 91 | Total train loss: 7.527064530609452e+26\n",
      "Epoch 91 | Val loss: 1.9025881765631484e+23\n",
      "Epoch 92 | Total train loss: 2.571865134794459e+26\n",
      "Epoch 92 | Val loss: 2.5167665757457998e+23\n",
      "Epoch 93 | Total train loss: 2.954636397243921e+26\n",
      "Epoch 93 | Val loss: 2.8746566493498423e+23\n",
      "Epoch 94 | Total train loss: 7.52254481506768e+26\n",
      "Epoch 94 | Val loss: 3.823674978667714e+23\n",
      "Epoch 95 | Total train loss: 2.1728701734135194e+26\n",
      "Epoch 95 | Val loss: 1.2017259329047505e+23\n",
      "Epoch 96 | Total train loss: 4.4062102565168664e+26\n",
      "Epoch 96 | Val loss: 4.979718378536254e+23\n",
      "Epoch 97 | Total train loss: 2.7338405706143573e+26\n",
      "Epoch 97 | Val loss: 6.029838556320921e+23\n",
      "Epoch 98 | Total train loss: 9.496992235572874e+25\n",
      "Epoch 98 | Val loss: 3.968712503947255e+23\n",
      "Epoch 99 | Total train loss: 4.9690821359525915e+25\n",
      "Epoch 99 | Val loss: 2.941416749218087e+23\n",
      "Epoch 100 | Total train loss: 1.2309820716149946e+26\n",
      "Epoch 100 | Val loss: 5.6444449997925266e+23\n",
      "Test loss: 1.6068771412863892e+23\n",
      "Model: sub_enormous_enormous | Training complete\n",
      "Model: sub_enormous_enormous | Metrics saved: R^2, MAE, Flops, Param#\n",
      "Model: sub_enormous_enormous | Plots made and saved\n",
      "Model: sub_enormous_enormous | Predictions saved\n",
      "Model: sub_enormous_enormous | Model saved\n",
      "Model: sub_enormous_enormous | Garbage collected, states deleted\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and store all the Pairnif models\n",
    "\n",
    "for i in exploration_dict['cfg_models']['cfg_pairnifs'].keys():\n",
    "    \n",
    "    print(f\"Model: {i} | Started loop\")\n",
    "    model = NIF_Pointwise(cfg_shape_net=exploration_dict['cfg_models']['cfg_pairnifs'][i]['cfg_shape_net'], cfg_param_net=exploration_dict['cfg_models']['cfg_pairnifs'][i]['cfg_param_net'])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "    model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    print(f\"Model: {i} | Initialization done for model and optimizer\")\n",
    "\n",
    "    collector_dict, model = train(cfg_train=cfg_train, model=model, optimizer=optimizer, loss_fn=loss_fn, dataloaders=dataloaders)\n",
    "    print(f\"Model: {i} | Training complete\")\n",
    "\n",
    "    # Calculate and export all the R^2 and MAE results\n",
    "    r2 = r2_score(collector_dict['actual']['test'].cpu(), collector_dict['preds']['test'].cpu(), multioutput='raw_values')\n",
    "    mae = mean_absolute_error(collector_dict['actual']['test'].cpu(), collector_dict['preds']['test'].cpu(), multioutput='raw_values')\n",
    "    exploration_dict['perf_models']['r2s']['pairnif'][i] = r2\n",
    "    exploration_dict['perf_models']['maes']['pairnif'][i] = mae\n",
    "\n",
    "    #Get the flops of the model, save them\n",
    "    show_tensor_1 = torch.ones(1, 20).to(device)\n",
    "    show_tensor_2 = torch.ones(1, 3).to(device)\n",
    "    flop_counter = FlopCounterMode(display=False)\n",
    "    with flop_counter:\n",
    "        model(show_tensor_1, show_tensor_2)\n",
    "    flops_per_sample = flop_counter.get_total_flops()\n",
    "    exploration_dict['perf_models']['flops']['pairnif'][i] = flops_per_sample\n",
    "\n",
    "    # Save the param amount\n",
    "    exploration_dict['perf_models']['params']['pairnif'][i] = get_n_params(model=model)\n",
    "    print(f\"Model: {i} | Metrics saved: R^2, MAE, Flops, Param#\")\n",
    "\n",
    "    # Export the graphs necessary\n",
    "    all_plots_ex_analysis(model_name=i, model_type='pairnif', collector_dict=collector_dict, index=index)\n",
    "    print(f\"Model: {i} | Plots made and saved\")\n",
    "\n",
    "    # Export the predictions for future reference\n",
    "    exploration_dict['perf_models']['preds']['pairnif'][i] = collector_dict['preds']['test']\n",
    "    print(f\"Model: {i} | Predictions saved\")\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = rf\"trained_models\\pairnif\\{i}\"\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model: {i} | Model saved\")\n",
    "\n",
    "    # Delete unnecessary stuff, free up mem\n",
    "    del collector_dict\n",
    "    del model\n",
    "    del optimizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"Model: {i} | Garbage collected, states deleted\")\n",
    "\n",
    "    # Now we are ready for the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc64ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('exploration_dict_pairnif.pkl', 'wb') as f:\n",
    "    pickle.dump(exploration_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c5e17",
   "metadata": {},
   "source": [
    "# Merging Dicts, Cleaning Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exploration_dict_pairnif.pkl', 'rb') as f:\n",
    "    exploration_dict_pairnif = pickle.load(f)\n",
    "\n",
    "with open('exploration_dict_mlp.pkl', 'rb') as f:\n",
    "    exploration_dict_mlp = pickle.load(f)\n",
    "\n",
    "exploration_dict_full = {\n",
    "    'cfg_models':  {\n",
    "        'cfg_mlps': {},\n",
    "        'cfg_pairnifs': {}\n",
    "    },\n",
    "    'perf_models': {\n",
    "        'r2s': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'maes': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'preds': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'flops': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        },\n",
    "        'params': {\n",
    "            'mlp': {},\n",
    "            'pairnif': {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "exploration_dict_full['cfg_models']['cfg_mlps'] = exploration_dict_mlp['cfg_models']['cfg_mlps']\n",
    "exploration_dict_full['cfg_models']['cfg_pairnifs'] = exploration_dict_pairnif['cfg_models']['cfg_pairnifs']\n",
    "\n",
    "exploration_dict_full['perf_models']['r2s']['mlp'] = exploration_dict_mlp['perf_models']['r2s']['mlp']\n",
    "exploration_dict_full['perf_models']['r2s']['pairnif'] = exploration_dict_pairnif['perf_models']['r2s']['pairnif']\n",
    "\n",
    "exploration_dict_full['perf_models']['maes']['mlp'] = exploration_dict_mlp['perf_models']['maes']['mlp']\n",
    "exploration_dict_full['perf_models']['maes']['pairnif'] = exploration_dict_pairnif['perf_models']['maes']['pairnif']\n",
    "\n",
    "exploration_dict_full['perf_models']['preds']['mlp'] = exploration_dict_mlp['perf_models']['preds']['mlp']\n",
    "exploration_dict_full['perf_models']['preds']['pairnif'] = exploration_dict_pairnif['perf_models']['preds']['pairnif']\n",
    "\n",
    "exploration_dict_full['perf_models']['flops']['mlp'] = exploration_dict_mlp['perf_models']['flops']['mlp']\n",
    "exploration_dict_full['perf_models']['flops']['pairnif'] = exploration_dict_pairnif['perf_models']['flops']['pairnif']\n",
    "\n",
    "exploration_dict_full['perf_models']['params']['mlp'] = exploration_dict_mlp['perf_models']['params']['mlp']\n",
    "exploration_dict_full['perf_models']['params']['pairnif'] = exploration_dict_pairnif['perf_models']['params']['pairnif']\n",
    "\n",
    "exploration_dict = exploration_dict_full\n",
    "\n",
    "# Add the true test data to\n",
    "test_data = next(iter(dl_test))\n",
    "target = test_data['perf_coeffs']\n",
    "\n",
    "\n",
    "with open('exploration_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(exploration_dict, f)\n",
    "\n",
    "del exploration_dict_full\n",
    "del exploration_dict_mlp\n",
    "del exploration_dict_pairnif\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e17d65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the true test data to\n",
    "test_data = next(iter(dl_test))\n",
    "target = test_data['perf_coeffs']\n",
    "exploration_dict['perf_models']['target'] = target\n",
    "\n",
    "\n",
    "with open('exploration_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(exploration_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6072dd39",
   "metadata": {},
   "source": [
    "# Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba6b41",
   "metadata": {},
   "source": [
    "The analyses of the training are as follows:\n",
    "\n",
    "- Predicting Cd: is the hardest for some reason, the R2 and MAE values for R2 are the lowest, this is kind of expected from NeuralFoil documentation\n",
    "    - Weighted loss?\n",
    "    - SmoothL1 loss (Huber loss) must be implemented either to Cd or entirely\n",
    "- Numerical Instability: We have numerical unstability on the >20m parameter models. To fix this (and to make overall training better), the following changes will be made the next exploration:\n",
    "    - Gradient clipping using pytorch\n",
    "    - Layer normalization\n",
    "- Model plateu: A lot of the models hit plateu of around 0.005 loss, and increasing model size only seems to change when this convergence happens. However, it seems that the plateu are varying depending on the model size (as expected), very minimally.\n",
    "    - New models must be tried out, since NeuralFoil has much better predictions given less parameters:\n",
    "        - Different input data\n",
    "        - Possibly residual connections since the models are relatively deep\n",
    "        - Any other architectural improvements\n",
    "    - Learning rate schedulers must be implemented\n",
    "- Improvements on Workflow:\n",
    "    - Instead of logging the results on exploration_dict, everything must be logged either on TensorBoard or Weights and Biases\n",
    "    - Use of tmux + .py for serialized exploration\n",
    "        - The MLP runs took on the upwards of 100mins and PairNIF on the upwards of 200mins on a 3050ti laptop, jupyter + vscode must be causing a lot of overhead\n",
    "        - The script must take inputs from .yaml\n",
    "    - The plots should not be output from the model, the user must do them on their own as well, that capability must be removed\n",
    "    - Maybe checkpointing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (pip)",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
